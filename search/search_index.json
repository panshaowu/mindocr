{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindocr","title":"MindOCR","text":""},{"location":"#introduction","title":"Introduction","text":"<p>MindOCR is an open-source toolbox for OCR development and application based on MindSpore, which integrates series of mainstream text detection and recognition algorihtms/models, provides easy-to-use training and inference tools. It can accelerate the process of developing and deploying SoTA text detection and recognition models in real-world applications, such as DBNet/DBNet++ and CRNN/SVTR, and help fulfill the need of image-text understanding.</p>  Major Features  <ul> <li>Modular design: We decoupled the OCR task into several configurable modules. Users can setup the training and evaluation pipelines, customize the data processing pipeline and model architectures easily by modifying just few lines of code.</li> <li>High-performance: MindOCR provides a series of pretrained weights trained with optimized configurations that reach competitive performance on OCR tasks.</li> <li>Low-cost-to-apply: Easy-to-use inference tools are provided in MindOCR to perform text detection and recognition tasks.</li> </ul>"},{"location":"#installation","title":"Installation","text":"Details"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>MindOCR is built on MindSpore AI framework, which supports CPU/GPU/NPU devices. MindOCR is compatible with the following framework versions. For details and installation guideline, please refer to the installation links shown below.</p> <ul> <li>mindspore &gt;= 2.2.0 [install]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (for distributed training/evaluation)  [install]</li> <li>mindspore lite (for offline inference) &gt;= 2.2.0  [install]</li> </ul>"},{"location":"#dependency","title":"Dependency","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#install-from-source-recommend","title":"Install from Source (recommend)","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>Using <code>-e</code> for \"editable\" mode can help resolve potential module import issues.</p>"},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install mindocr\n</code></pre> <p>As this project is under active development, the version installed from PyPI is out-of-date currently. (will update soon).</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-text-detection-and-recognition-demo","title":"1. Text Detection and Recognition Demo","text":"<p>After installing MindOCR, we can run text detection and recognition on an arbitrary image easily as follows.</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN\n</code></pre> <p>After running, the results will be saved in <code>./inference_results</code> by default. Here is an example result.</p> <p> </p> <p>  Visualization of text detection and recognition result  </p> <p>We can see that all texts on the image are detected and recognized accurately. For more usage, please refer to the inference section in tutorials.</p>"},{"location":"#2-model-training-and-evaluation-quick-guideline","title":"2. Model Training and Evaluation - Quick Guideline","text":"<p>It is easy to train your OCR model with the <code>tools/train.py</code> script, which supports both text detection and recognition model training.</p> <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <p>The <code>--config</code> arg specifies the path to a yaml file that defines the model to be trained and the training strategy including data process pipeline, optimizer, lr scheduler, etc.</p> <p>MindOCR provides SoTA OCR models with their training strategies in <code>configs</code> folder. You may adapt it to your task/dataset, for example, by running</p> <pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/dbpp_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre> <p>Similarly, it is easy to evaluate the trained model with the <code>tools/eval.py</code> script.</p> <pre><code>python tools/eval.py \\\n    --config {path/to/model_config.yaml} \\\n    --opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre> <p>For more illustration and usage, please refer to the model training section in Tutorials.</p>"},{"location":"#3-model-offline-inference-quick-guideline","title":"3. Model Offline Inference - Quick Guideline","text":"<p>You can do MindSpore Lite inference in MindOCR using MindOCR models or Third-party models (PaddleOCR, MMOCR, etc.). Please refer to the following documents  - Python/C++ Inference on Ascend 310  - MindOCR Models Offline Inference - Quick Start  - Third-party Models Offline Inference - Quick Start.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Datasets<ul> <li>Dataset Preparation</li> <li>Data Transformation Mechanism</li> </ul> </li> <li>Model Training<ul> <li>Yaml Configuration</li> <li>Text Detection</li> <li>Text Recognition</li> <li>Distributed Training</li> <li>Advance: Gradient Accumulation, EMA, Resume Training, etc</li> </ul> </li> <li>Inference with MindSpore<ul> <li>Python Online Inference</li> </ul> </li> <li>Inference with MindSpore Lite<ul> <li>Python/C++ Inference on Ascend 310</li> <li>MindOCR Models Offline Inference - Quick Start</li> <li>Third-party Models Offline Inference - Quick Start</li> <li>Model Conversion</li> </ul> </li> <li>Developer Guides<ul> <li>Customize Dataset</li> <li>Customize Data Transformation</li> <li>Customize a New Model</li> <li>Customize Postprocessing Method</li> </ul> </li> </ul>"},{"location":"#model-list","title":"Model List","text":"Text Detection <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021)</li> </ul> Text Recognition <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> MASTER (PR'2019)</li> <li> VISIONLAN (ICCV'2021)</li> <li> RobustScanner (ECCV'2020)</li> <li> ABINet (CVPR'2021)</li> </ul> Layout Analysis <ul> <li> YOLOv8 (Ultralytics Inc.)</li> </ul> Key Information Extraction <ul> <li> LayoutXLM SER (arXiv'2016)</li> </ul> Table Recognition <ul> <li> TableMaster (arXiv'2021)</li> </ul> <p>For the detailed performance of the trained models, please refer to configs.</p> <p>For details of MindSpore Lite and ACL inference models support, please refer to MindOCR Models Support List and Third-party Models Support List (PaddleOCR, MMOCR, etc.).</p>"},{"location":"#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li>Born-Digital Images [download]</li> <li>CASIA-10K [download]</li> <li>CCPD [download]</li> <li>Chinese Text Recognition Benchmark paper(datasets/chinese_text_recognition.md)]</li> <li>COCO-Text [download]</li> <li>CTW [download]</li> <li>ICDAR2015 paper(datasets/icdar2015.md)]</li> <li>ICDAR2019 ArT [download]</li> <li>LSVT [download]</li> <li>MLT2017 paper(datasets/mlt2017.md)]</li> <li>MSRA-TD500 paper(datasets/td500.md)]</li> <li>MTWI-2018 [download]</li> <li>RCTW-17 [download]</li> <li>ReCTS [download]</li> <li>SCUT-CTW1500 paper(datasets/ctw1500.md)]</li> <li>SROIE [download]</li> <li>SVT [download]</li> <li>SynText150k paper(datasets/syntext150k.md)]</li> <li>SynthText paper(datasets/synthtext.md)]</li> <li>TextOCR [download]</li> <li>Total-Text paper(datasets/totaltext.md)]</li> </ul> Layout Analysis Datasets <ul> <li>PublayNet paper(https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/publaynet.tar.gz)]</li> </ul> Key Information Extraction Datasets <ul> <li>XFUND paper(https://github.com/doc-analysis/XFUND/releases/tag/v1.0)]</li> </ul> Table Recognition Datasets <ul> <li>PubTabNet paper(https://dax-cdn.cdn.appdomain.cloud/dax-pubtabnet/2.0.0/pubtabnet.tar.gz)]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Frequently asked questions about configuring environment and mindocr, please refer to FAQ.</p>"},{"location":"#notes","title":"Notes","text":""},{"location":"#what-is-new","title":"What is New","text":"News <ul> <li> <p>2023/12/25 1. Add new trained models</p> <ul> <li>TableMaster for table recognition 2. Add more benchmark datasets and their results</li> <li>PubTabNet</li> </ul> </li> <li> <p>2023/12/14 1. Add new trained models</p> <ul> <li>LayoutXLM SER for key information extraction</li> <li>VI-LayoutXLM SER for key information extraction</li> <li>PP-OCRv3 DBNet for text detection and PP-OCRv3 SVTR for recognition, supporting online inferece and finetuning 2. Add more benchmark datasets and their results</li> <li>XFUND 3. Multiple specifications support for Ascend 910: DBNet ResNet-50, DBNet++ ResNet-50, CRNN VGG7, SVTR-Tiny, FCENet, ABINet</li> </ul> </li> <li>2023/11/28 1. Add offline inference support for PP-OCRv4<ul> <li>PP-OCRv4 DBNet for text detection and PP-OCRv4 CRNN for text recognition, supporting offline inferece 2. Fix bugs of third-party models offline inference</li> </ul> </li> <li>2023/11/17 1. Add new trained models<ul> <li>YOLOv8 for layout analysis 2. Add more benchmark datasets and their results</li> <li>PublayNet</li> </ul> </li> <li>2023/07/06 1. Add new trained models<ul> <li>RobustScanner for text recognition</li> </ul> </li> <li>2023/07/05 1. Add new trained models<ul> <li>VISIONLAN for text recognition</li> </ul> </li> <li>2023/06/29 1. Add new trained models<ul> <li>FCENet for text detection</li> <li>MASTER for text recognition</li> </ul> </li> <li> <p>2023/06/07 1. Add new trained models</p> <ul> <li>PSENet for text detection</li> <li>EAST for text detection</li> <li>SVTR for text recognition 2. Add more benchmark datasets and their results</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. Add resume training function, which can be used in case of unexpected interruption in training. Usage: add the <code>resume</code> parameter under the <code>model</code> field in the yaml config, e.g.,<code>resume: True</code>, load and resume training from {ckpt_save_dir}/train_resume.ckpt or <code>resume: /path/to/train_resume.ckpt</code>, load and resume training from the given path. 4. Improve postprocessing for detection: re-scale detected text polygons to original image space by default, which can be enabled by add \"shape_list\" to the <code>eval.dataset.output_columns</code> list. 5. Refactor online inference to support more models, see README.md for details.</li> </ul> </li> <li> <p>2023/05/15 1. Add new trained models</p> <ul> <li>DBNet++ for text detection</li> <li>CRNN-Seq2Seq for text recognition</li> <li>DBNet pretrained on SynthText is now available: checkpoint url 2. Add more benchmark datasets and their results</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>More benchmark results for DBNet are reported here. 3. Add checkpoint manager for saving top-k checkpoints and improve log. 4. Python inference code refactored. 5. Bug fix: use Meter to average loss for large datasets, disable <code>pred_cast_fp32</code> for ctcloss in AMP training, fix error when invalid polygons exist.</li> </ul> </li> <li> <p>2023/05/04 1. Support loading self-defined pretrained checkpoints via setting <code>model-pretrained</code> with checkpoint url or local path in yaml. 2. Support setting probability for executing augmentation including rotation and flip. 3. Add Exponential Moving Average(EMA) for model training, which can be enabled by setting <code>train-ema</code> (default: False) and <code>train-ema_decay</code> in the yaml config. 4. Arg parameter changed\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: change the column number feeding into the network to the column index. 5. Arg parameter changed\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: change the column number corresponds to the label to the column index.</p> </li> <li> <p>2023/04/21 1. Add parameter grouping to support flexible regularization in training. Usage: add <code>grouping_strategy</code> argument in yaml config to select a predefined grouping strategy, or use <code>no_weight_decay_params</code> argument to pick layers to exclude from weight decay (e.g., bias, norm). Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 2. Add gradient accumulation to support large batch size training. Usage: add <code>gradient_accumulation_steps</code> in yaml config, the global batch size = batch_size * devices * gradient_accumulation_steps. Example can be referred in <code>configs/rec/crnn/crnn_icdar15.yaml</code> 3. Add gradient clip to support training stablization. Enable it by setting <code>grad_clip</code> as True in yaml config.</p> </li> <li> <p>2023/03/23 1. Add dynamic loss scaler support, compatible with drop overflow update. To enable dynamic loss scaler, please set <code>type</code> of <code>loss_scale</code> as <code>dynamic</code>. A YAML example can be viewed in <code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. Arg names changed: <code>output_keys</code> -&gt; <code>output_columns</code>, <code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code> 2. Data pipeline updated</p> </li> <li> <p>2023/03/13 1. Add system test and CI workflow. 2. Add modelarts adapter to allow training on OpenI platform. To train on OpenI:     <pre><code>    i)   Create a new training task on the openi cloud platform.\n    ii)  Link the dataset (e.g., ic15_mindocr) on the webpage.\n    iii) Add run parameter `config` and write the yaml file path on the website UI interface, e.g., '/home/work/user-job-dir/V0001/configs/rec/test.yaml'\n    iv)  Add run parameter `enable_modelarts` and set True on the website UI interface.\n    v)   Fill in other blanks and launch.\n</code></pre></p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindOCR better.</p> <p>Please refer to CONTRIBUTING.md for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"datasets/borndigital/","title":"Born-Digital Images Dataset","text":""},{"location":"datasets/borndigital/#data-downloading","title":"Data Downloading","text":"<p>The Born-Digital dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>This dataset is divided into 4 tasks: (1.1) Text Localization, (1.2) Text Segmentation, (1.3) Word Recognition, and  (1.4) End To End.  For now, we consider and download only the dataset for Task 1.1.</p> <p>After downloading the images and annotations, unzip the files and rename as appropriate e.g. <code>train_images</code> for the images and <code>train_labels</code> for the ground truths, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>Born-Digital\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_labels\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/borndigital/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/borndigital/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name borndigital --task det \\\n    --image_dir path/to/Born-Digital/train_images/ \\\n    --label_dir path/to/Born-Digital/train_labels \\\n    --output_path path/to/Born-Digital/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>Born-Digital/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/casia10k/","title":"CASIA-10K Dataset","text":""},{"location":"datasets/casia10k/#data-downloading","title":"Data Downloading","text":"<p>The CASIA-10K dataset can be downloaded from this link.</p> <p>After downloading the file as above, unzip it, after which the directory structure should be like as follows (ignoring the archive file):</p> <pre><code>CASIA-10K\n  |--- test\n  |    |--- PAL00001.jpg\n  |    |--- PAL00001.txt\n  |    |--- PAL00005.jpg\n  |    |--- PAL00005.txt\n  |    |--- ...\n  |--- train\n  |    |--- PAL00003.jpg\n  |    |--- PAL00003.txt\n  |    |--- PAL00006.jpg\n  |    |--- PAL00006.txt\n  |    |--- ...\n  |--- CASIA-10K_test.txt\n  |--- CASIA-10K_train.txt\n</code></pre>"},{"location":"datasets/casia10k/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/casia10k/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name casia10k --task det \\\n    --image_dir path/to/CASIA-10K/train/ \\\n    --label_dir path/to/CASIA-10K/train \\\n    --output_path path/to/CASIA-10K/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CASIA-10K/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ccpd/","title":"Chinese City Parking Dataset (CCPD) 2019","text":""},{"location":"datasets/ccpd/#data-downloading","title":"Data Downloading","text":"<p>The CCPD can be downloaded from this link using either the Google or BaiduYun drive links. This dataset is divided into 3 sets: train, val, test. Labels for each set can be found under the <code>splits</code> directory of the dataset. CCPD-Green dataset is already separated into different folders and thus does not require labels.</p> <p>The annotations for each image are embedded into the filename of the image. The format is described on their official website here.</p> <p>After downloading the dataset, the directory structure should be like as follows: <pre><code>CCPD2019\n  |--- ccpd_base\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ccpd_blur\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ...\n  |--- ...\n  |--- ...\n  |--- splits\n</code></pre></p>"},{"location":"datasets/ccpd/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ccpd/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ccpd --task det \\\n    --image_dir path/to/CCPD2019/ccpd_base \\\n    --label_dir path/to/CCPD2019/splits/train.txt \\\n    --output_path path/to/CCPD2019/det_gt.txt\n</code></pre> <p><code>label_dir</code> is not required for CCPD-Green dataset.</p> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CCPD2019/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/chinese_text_recognition/","title":"Chinese-Text-Recognition","text":""},{"location":"datasets/chinese_text_recognition/#data-downloading","title":"Data Downloading","text":"<p>Following the setup in Benchmarking-Chinese-Text-Recognition, we use the same training, validation and evaliation data as described in Datasets section.</p> <p>Please download the following LMDB files as introduced in Downloads section:</p> <ul> <li>scene datasets: The union dataset contains RCTW, ReCTS, LSVT, ArT, CTW</li> <li>web: MTWI</li> <li>document: generated with Text Render</li> <li>handwriting dataset: SCUT-HCCDoc</li> </ul>"},{"location":"datasets/chinese_text_recognition/#data-structure","title":"Data Structure","text":"<p>After downloading the files, please put all training files under the same folder <code>training</code>, all validation data under <code>validation</code> folder, and all evaluation data under <code>evaluation</code>.</p> <p>The data structure should be like:</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#data-configuration","title":"Data Configuration","text":"<p>To use the datasets, you can specify the datasets as follow in configuration file.</p>"},{"location":"datasets/chinese_text_recognition/#model-training","title":"Model Training","text":"<pre><code>...\ntrain:\n  ...\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\n    data_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\n    data_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n  ...\n</code></pre>"},{"location":"datasets/chinese_text_recognition/#model-evaluation","title":"Model Evaluation","text":"<pre><code>...\ntrain:\n  # NO NEED TO CHANGE ANYTHING IN TRAIN SINCE IT IS NOT USED\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\n    data_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n  ...\n</code></pre> <p>Back to dataset converters</p>"},{"location":"datasets/cocotext/","title":"COCO-Text Dataset","text":""},{"location":"datasets/cocotext/#data-downloading","title":"Data Downloading","text":"<p>Official Website</p> <p>The COCO-Text images dataset and the annotations (in JSON format) <code>annotations v1.4 JSON</code> can be downloaded from this link.</p> <p>Note: Please register an account to download this dataset.</p> <p>After downloading the images and annotations, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>COCO-Text\n  |--- train_images\n  |    |--- COCO_train2014_000000000036.jpg\n  |    |--- COCO_train2014_000000000064.jpg\n  |    |--- ...\n  |--- COCO_Text.json\n</code></pre></p>"},{"location":"datasets/cocotext/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/cocotext/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name cocotext --task det \\\n    --image_dir path/to/COCO-Text/train_images/ \\\n    --label_dir path/to/COCO-Text/COCO_Text.json \\\n    --output_path path/to/COCO-Text/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>COCO-Text/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/converters/","title":"Dataset Preparation","text":"<p>This document shows how to convert ocr annotation to the general format (not including LMDB) for model training.</p> <p>You may also refer to <code>convert_datasets.sh</code> which is a quick solution for converting annotation files of all datasets under a given directory.</p> To download and convert OCR datasets to the required data format, please refer to these instructions. <ul> <li>Born-Digital Images</li> <li>CASIA-10K</li> <li>CCPD</li> <li>Chinese text recognition</li> <li>COCO-Text</li> <li>CTW</li> <li>ICDAR2015</li> <li>ICDAR2019 ArT</li> <li>LSVT</li> <li>MLT2017</li> <li>MSRA-TD500</li> <li>MTWI-2018</li> <li>RCTW-17</li> <li>ReCTS</li> <li>SCUT-CTW1500</li> <li>SROIE</li> <li>SVT</li> <li>SynText150k</li> <li>SynthText</li> <li>TextOCR</li> <li>Total-Text</li> </ul>"},{"location":"datasets/converters/#text-detectionspotting-annotation","title":"Text Detection/Spotting Annotation","text":"<p>The format of the converted annotation file should follow: <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>Taking ICDAR2015 (ic15) dataset as an example, to convert the ic15 dataset to the required format, please run</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/train/ch4_training_images \\\n        --label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/test/ch4_test_images \\\n        --label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"datasets/converters/#text-recognition-annotation","title":"Text Recognition Annotation","text":""},{"location":"datasets/converters/#common-dataset-format","title":"Common Dataset Format","text":"<p>The annotation format for text recognition dataset follows <pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> Note that image name and text label are seperated by \\t.</p> <p>To convert, please run: <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"datasets/converters/#lmdb-dataset-format","title":"LMDB Dataset Format","text":"<p>Some of the dataset can be converted to LMDB format. Currently, this is only supported for the <code>SynthText</code> and <code>SynthAdd</code> datasets.</p> <p>To convert to LMDB format, please run</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name synthtext \\\n    --task rec_lmdb \\\n    --image_dir /path/to/SynthText \\\n    --label_dir /path/to/SynthText_gt.mat \\\n    --output_path ST_full\n</code></pre>"},{"location":"datasets/ctw/","title":"CTW Dataset","text":""},{"location":"datasets/ctw/#data-downloading","title":"Data Downloading","text":"<p>The CTW images dataset Official Website | Download Link</p> <p>Note: Please fill in a form to download this dataset.</p> <p>The images are in 26 batches, i.e. 26 different .tar archived files of the format <code>images-trainval/ctw-trainval*.tar</code>. All 26 batches need to be downloaded.</p> <p>The CTW annotations (in JSON Lines format i.e. <code>.jsonl</code>) can be downloaded from this download link. The annotations archived file is named <code>ctw-annotations.tar.gz</code>.</p> <p>After downloading the zipped images, unzip the batches and collect all the images into a single folder e.g. <code>train_val/</code>. After downloading the zipped annotations, unzip them. Finally, the directory structure should look like this (ignoring the archive files):</p> <pre><code>CTW\n  |--- train_val\n  |    |--- 0000172.jpg\n  |    |--- 0000174.jpg\n  |    |--- ...\n  |--- train.jsonl\n  |--- val.jsonl\n  |--- test_cls.jsonl\n  |--- info.json\n</code></pre>"},{"location":"datasets/ctw/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw --task det \\\n    --image_dir path/to/CTW/train_val/ \\\n    --label_dir path/to/CTW/train.jsonl \\\n    --output_path path/to/CTW/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>CTW/</code>.</p> <p>Note that the <code>label_dir</code> flag may be altered to prepare the validation data.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ctw1500/","title":"SCUT-CTW1500 Datasets","text":""},{"location":"datasets/ctw1500/#data-downloading","title":"Data Downloading","text":"<p>SCUT-CTW1500 Datasets download link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/ctw1500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ctw1500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/train_images/ \\\n    --label_dir path/to/ctw1500/ctw_1500_train_labels \\\n    --output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/test_images/ \\\n    --label_dir path/to/ctw1500/gt_ctw_1500 \\\n    --output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>ctw1500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/ic19_art/","title":"ICDAR2019 Dataset","text":""},{"location":"datasets/ic19_art/#data-downloading","title":"Data Downloading","text":"<p>The ICDAR2019 ArT images and annotations Official Website | Download Link</p> <p>Note: Please register an account to download this dataset</p> <p>For the images, the archived file <code>train_images.tar.gz</code> from the section \"Task 1 and Task 3\" needs to be downloaded. For the annotations, the .JSON file <code>train_labels.json</code> from the same section needs to be downloaded.</p> <p>After downloading the dataset, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>ICDAR2019-ArT\n  |--- train_images\n  |    |--- train_images\n  |    |    |--- gt_0.jpg\n  |    |    |--- gt_1.jpg\n  |    |    |--- ...\n  |--- train_labels.json\n</code></pre></p>"},{"location":"datasets/ic19_art/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/ic19_art/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ic19_art --task det \\\n    --image_dir path/to/ICDAR2019-ArT/train_images/train_images/ \\\n    --label_dir path/to/ICDAR2019-ArT/train_labels.json \\\n    --output_path path/to/ICDAR2019-ArT/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>ICDAR2019-ArT/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/icdar2015/","title":"ICDAR 2015 Datasets","text":""},{"location":"datasets/icdar2015/#data-downloading","title":"Data Downloading","text":"<p>ICDAR 2015 Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>ICDAR 2015 Challenge has three tasks. Task 1 is Text Localization. Task 3 is Word Recognition. Task 4 is End-to-end Text Spotting. Task 2 Text Segmentation is not available.</p>"},{"location":"datasets/icdar2015/#text-localization","title":"Text Localization","text":"<p>The four files downloaded for task 1 are <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre></p>"},{"location":"datasets/icdar2015/#word-recognition","title":"Word Recognition","text":"<p>The three files downloaded for task 3 are <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre> The three files are only needed for training word recognition models. Training text detection models does not require the three files.</p>"},{"location":"datasets/icdar2015/#e2e","title":"E2E","text":"<p>The nine files downloaded for task 4 are the union of the four files in the text localization task (task 1) and five vocabulary files <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre> If you download a file named <code>Challenge4_Test_Task4_GT.zip</code>, please note that it is the same file as <code>Challenge4_Test_Task1_GT.zip</code>, except for its name. In this repository, we will use <code>Challenge4_Test_Task4_GT.zip</code> for ICDAR2015 dataset.</p> <p>After downloading the icdar2015 dataset, place all the files under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/lsvt/","title":"LSVT Dataset","text":""},{"location":"datasets/lsvt/#data-downloading","title":"Data Downloading","text":"<p>The LSVT dataset Official Website |  Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>The images are split into two zipped files <code>train_full_images_0.tar.gz</code> and <code>train_full_images_1.tar.gz</code>. Both are to be downloaded. After downloading the images as above, unzip them and collect them into a single folder e.g. <code>train_images</code>.</p> <p>The LSVT annotations (in JSON format) can be downloaded from this download link. The file <code>train_full_labels.json</code> needs to be downloaded.</p> <p>After downloading the images and annotations as above, the directory structure should be like as follows (ignoring the archive files): <pre><code>LSVT\n  |--- train_images\n  |    |--- gt_0.jpg\n  |    |--- gt_1.jpg\n  |    |--- ...\n  |--- train_full_labels.json\n</code></pre></p>"},{"location":"datasets/lsvt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/lsvt/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name lsvt --task det \\\n    --image_dir path/to/LSVT/train_images/ \\\n    --label_dir path/to/LSVT/train_full_labels.json \\\n    --output_path path/to/LSVT/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>LSVT/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/mlt2017/","title":"MLT 2017 Datasets","text":""},{"location":"datasets/mlt2017/#data-downloading","title":"Data Downloading","text":"<p>MLT (Multi-Lingual) 2017 dataset Paper | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>MLT 2017 dataset consists of two tasks. Task 1 is Text detection (Multi-Language Script) and Task 2 is Word Recognition.</p>"},{"location":"datasets/mlt2017/#text-detectionmulti-script","title":"Text Detection(Multi-script)","text":"<p>The 11 files downloaded for task 1 are</p> <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre> <p>No need to download the Test Set.</p>"},{"location":"datasets/mlt2017/#word-identification","title":"Word Identification","text":"<p>The 6 files downloaded for task 2 are <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n\n&lt;/details&gt;\n\nAfter downloading the files, place them under `[path-to-data-dir]` folder:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip</p> <p>```</p> <p>Back to dataset converters</p>"},{"location":"datasets/mtwi2018/","title":"ICPR MTWI-2018 Dataset","text":""},{"location":"datasets/mtwi2018/#data-downloading","title":"Data Downloading","text":"<p>The ICPR MTWI-2018 dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>The ICPR MTWI dataset has derived three tasks: Text Line(column) Recognition of Web Images, Text Detection of Web Images, and End to End Text Detection and Recognition of Web Images. The three tasks share the same training data: <code>mtwi_train.zip</code>; For test data, task1 has test data: <code>mtwi_task1.zip</code>, and task\u2154 share the same test data: <code>mtwi_task2_3.zip</code>. For now, we will consider and download only the training data <code>mtw_train.zip</code>.</p> <p>After downloading the dataset, unzip the file, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>MTWI-2018\n  |--- image_train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- txt_train\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/mtwi2018/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/mtwi2018/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name mtwi2018 --task det \\\n    --image_dir path/to/MTWI-2018/image_train/ \\\n    --label_dir path/to/MTWI-2018/txt_train.json \\\n    --output_path path/to/MTWI-2018/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>MTWI-2018/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/pubtabnet/","title":"PubTabNet Dataset","text":""},{"location":"datasets/pubtabnet/#data-downloading","title":"Data Downloading","text":"<p>PubTabNet dataset Official Website | Download Link</p> <p>Download the images and annotations, unzip the files. The directory structure should be:</p> <pre><code>pubtabnet\n  |--- train\n  |    |--- PMC1064074_007_00.png\n  |    |--- PMC1064076_003_00.png\n  |    |--- ...\n  |--- test\n  |    |--- PMC1064127_003_00.png\n  |    |--- PMC1065052_003_00.png\n  |    |--- ...\n  |--- val\n  |    |--- PMC1064865_002_00.png\n  |    |--- PMC1079806_002_00.png\n  |    |--- ...\n  |--- PubTabNet_2.0.0.jsonl\n</code></pre>"},{"location":"datasets/pubtabnet/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/pubtabnet/#for-table-recognition-task","title":"For Table Recognition Task","text":"<p>To prepare the annotation for Table Recognition, run the following commands:</p> <ul> <li>Split the annotation for training set:</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/train/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_train.jsonl \\\n    --split train\n</code></pre> <ul> <li>Split the annotation for validation set:</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/val/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_val.jsonl \\\n    --split val\n</code></pre> <ul> <li>Note: the annotation for testing set is not provided</li> </ul> <p>Then, the generated standard annotation file <code>pubtab_train.jsonl</code> and <code>pubtab_val.jsonl</code> will be placed under the folder <code>pubtabnet/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/rctw17/","title":"RCTW-17 Dataset","text":""},{"location":"datasets/rctw17/#data-downloading","title":"Data Downloading","text":"<p>The RCTW dataset Official Website | Download Link</p> <p>The training set is split into two zip files <code>train_images.zip.001</code> and <code>train_images.zip.002</code>. The annotations are <code>*_gts.zip</code> files.</p> <p>After downloading and unzipping the images and annotations, collect the images into a single folder e.g. <code>train_images/</code>, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>RCTW-17\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_gts\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/rctw17/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/rctw17/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rctw17 --task det \\\n    --image_dir path/to/RCTW-17/train_images/ \\\n    --label_dir path/to/RCTW-17/train_gts \\\n    --output_path path/to/RCTW-17/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>RCTW-17/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/rects/","title":"ReCTS Dataset","text":""},{"location":"datasets/rects/#data-downloading","title":"Data Downloading","text":"<p>The ReCTS images and annotations dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>After downloading the images and annotations, unzip the file, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>ReCTS\n  |--- img\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- gt\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n  |--- gt_unicode\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/rects/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/rects/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rects --task det \\\n    --image_dir path/to/ReCTS/img/ \\\n    --label_dir path/to/ReCTS/gt_unicode.json \\\n    --output_path path/to/ReCTS/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>ReCTS/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/sroie/","title":"SROIE Dataset","text":""},{"location":"datasets/sroie/#data-downloading","title":"Data Downloading","text":"<p>The SROIE dataset Official Website | Download Link</p> <p>Note: Please register an account to download this dataset.</p> <p>This dataset is divided into 3 tasks: (1) Text Localisation, (2) OCR, and (3) Key Information Extraction. For now, we consider and download the updated dataset only for Task 1.</p> <p>After downloading and unzipping the dataset as above and renaming the extracted folder as appropriate e.g. <code>train</code>, the directory structure should be like as follows (ignoring the archive files): <pre><code>SROIE\n  |--- train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"datasets/sroie/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/sroie/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name sroie --task det \\\n    --image_dir path/to/SROIE/train/ \\\n    --label_dir path/to/SROIE/train \\\n    --output_path path/to/SROIE/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>SROIE/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/svt/","title":"The Street View Text Dataset (SVT)","text":""},{"location":"datasets/svt/#data-downloading","title":"Data Downloading","text":"<p>The Street View Text Dataset (SVT) Download Link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"datasets/svt/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/svt/#for-recognition-task","title":"For Recognition task","text":"<p>To prepare the data for text recognition, you can run the following command:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name  svt --task rec \\\n    --image_dir path/to/svt1/ \\\n    --label_dir path/to/svt1/train.xml \\\n    --output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>Then you can have a folder <code>cropped_images/</code> and an annotation file <code>rec_train_gt.txt</code> under the folder <code>svt1/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/syntext150k/","title":"SynText150k Datasets","text":""},{"location":"datasets/syntext150k/#data-downloading","title":"Data Downloading","text":"<p>SynText150k paper</p> <p>Download Syntext-150k - Part1: 54,327 [images][annotations] - Part2: 94,723 [images][annotations]</p> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"datasets/synthtext/","title":"SynthText Datasets","text":""},{"location":"datasets/synthtext/#data-downloading","title":"Data Downloading","text":"<p>SynthText is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout.</p> <p>Paper | Download Link</p> <p>Download the <code>SynthText.zip</code> file and unzip in <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: Additionally, It is strongly recommended to pre-process the <code>SynthText</code> dataset before using it as it contains some faulty data: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat --image_dir=/path-to-data-dir/SynthText\n</code></pre> This operation will generate a filtered output in the same format as the original <code>SynthText</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/td500/","title":"MSRA Text Detection 500 Database (MSRA-TD500)","text":""},{"location":"datasets/td500/#data-downloading","title":"Data Downloading","text":"<p>MSRA Text Detection 500 Database\uff08MSRA-TD500\uff09Download Link</p> <p>Please download the data from the website above and unzip the file. After unzipping the file, the data structure should be like:</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"datasets/td500/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/td500/#for-detection-task","title":"For Detection task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/train/ \\\n    --label_dir path/to/MSRA-TD500/train \\\n    --output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/test/ \\\n    --label_dir path/to/MSRA-TD500/test \\\n    --output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>Then you can have two annotation files <code>train_det_gt.txt</code> and <code>test_det_gt.txt</code> under the folder <code>MSRA-TD500/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/textocr/","title":"TextOCR Dataset","text":""},{"location":"datasets/textocr/#data-downloading","title":"Data Downloading","text":"<p>The TextOCR dataset Official Website | Download Link</p> <p>After downloading the images and annotations, unzip the files, after which the directory structure should be like as follows (ignoring the archive files): <pre><code>TextOCR\n  |--- train_val_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- TextOCR_0.1_train.json\n  |--- TextOCR_0.1_val.json\n</code></pre></p>"},{"location":"datasets/textocr/#data-preparation","title":"Data Preparation","text":""},{"location":"datasets/textocr/#for-detection-task","title":"For Detection Task","text":"<p>To prepare the data for text detection, you can run the following commands:</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name textocr --task det \\\n    --image_dir path/to/TextOCR/train_val_images/ \\\n    --label_dir path/to/TextOCR/TextOCR_0.1_train.json \\\n    --output_path path/to/TextOCR/det_gt.txt\n</code></pre> <p>The generated standard annotation file <code>det_gt.txt</code> will now be placed under the folder <code>TextOCR/</code>.</p> <p>Back to dataset converters</p>"},{"location":"datasets/totaltext/","title":"Total-Text Datasets","text":""},{"location":"datasets/totaltext/#data-downloading","title":"Data Downloading","text":"<ul> <li> <p>Total-Text paper</p> </li> <li> <p>github repo\uff1a</p> </li> <li>images</li> <li> <p>annotations</p> </li> <li> <p>Dataset download link\uff1a</p> </li> <li>images (size = 441Mb)</li> <li>annotations (.txt format)</li> </ul> <p>After downloading the two files, place them under <code>[path-to-data-dir]</code> folder: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>Back to dataset converters</p>"},{"location":"inference/convert_dynamic/","title":"Convert dynamic.md","text":""},{"location":"inference/convert_dynamic/#inference-dynamic-shape-scaling","title":"Inference - Dynamic Shape Scaling","text":""},{"location":"inference/convert_dynamic/#1-introduction","title":"1. Introduction","text":"<p>In some inference scenarios, such as object recognition after detection, the input batch size and image size of the recognition network are not fixed because the number of object and the size of the object are not fixed. If each inference is calculated according to the maximum batch size or maximum image size, it will cause a waste of computing resources.</p> <p>Therefore, you can set some candidate values during model conversion, and resize to the best matching candidate value during inference, thereby improving performance. Users can manually select these candidate values empirically, or they can be statistically derived from the dataset.</p> <p>This tool integrates the function of dataset statistics, can count the appropriate combination of <code>batch size</code>, <code>height</code> and <code>width</code> as candidate values, and encapsulates the model conversion tool, thus realizing the automatic model shape scaling.</p>"},{"location":"inference/convert_dynamic/#2-environment","title":"2. Environment","text":"<p>Please refer to Environment Installation to install ACL or MindSpore Lite environment.</p>"},{"location":"inference/convert_dynamic/#3-model","title":"3. Model","text":"<p>Currently, ONNX model files are supported, and by selecting different backends, they are automatically shape scaling and converted to OM or MIndIR model files.</p> <p>Please make sure that the input model is the dynamic shape version. For example, if the text detection model needs to shape scaling for H and W, make sure that at least the H and W axes are dynamic, and the shape can be <code>(1,3,-1,-1)</code> and <code>(-1,3,- 1,-1)</code>etc.</p>"},{"location":"inference/convert_dynamic/#4-dataset","title":"4. Dataset","text":"<p>Two types of data are supported:</p> <ol> <li>Image folder</li> </ol> <ul> <li> <p>This tool will read all the images in the folder, record <code>height</code> and <code>width</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text detection and text recognition models</p> </li> </ul> <ol> <li>Annotation file for text detection</li> </ol> <ul> <li> <p>Refer to converter, which is the annotation file output when the      parameter <code>task</code> is <code>det</code></p> </li> <li> <p>This tool will read the coordinates of the text box marked under each image, record <code>height</code> and <code>width</code>, and the      number of boxes as <code>batch size</code>, and count suitable candidate values</p> </li> <li> <p>Suitable for text recognition models</p> </li> </ul>"},{"location":"inference/convert_dynamic/#5-usages","title":"5. Usages","text":"<p><code>cd deploy/models_utils/auto_scaling</code></p>"},{"location":"inference/convert_dynamic/#51-command-example","title":"5.1 Command example","text":"<ul> <li>auto shape scaling for batch size</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/det_gt.txt\n    --input_shape=-1,3,48,192 \\\n    --output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_dynamic_bs.om</code></p> <ul> <li>auto shape scaling for height and width</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_dynamic_hw.om</code></p> <ul> <li>auto shape scaling for batch szie, height and width</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=-1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>The output result is multiple OM models, combining multiple different batch sizes, and each model uses the same dynamic image size\uff1a<code>model_dynamic_bs1_hw.om</code>, <code>model_dynamic_bs4_hw.om</code>, ......</p> <ul> <li>no shape scaling</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --input_shape=4,3,48,192 \\\n    --output_path=output\n</code></pre> <p>The output is a single OM model: <code>model_static.om</code></p>"},{"location":"inference/convert_dynamic/#52-parameter-details","title":"5.2 Parameter Details","text":"Name Default Required Description model_path None Y Path to model file input_shape None Y model input shape, NCHW format data_path None N Path to image folder or annotation file input_name x N model input name backend atc N converter backend\uff0catc or lite output_path ./output N Path to output model soc_version Ascend310P3 N soc_version for Ascend\uff0cAscend310P3 or Ascend310"},{"location":"inference/convert_dynamic/#53-configuration-file","title":"5.3 Configuration file","text":"<p>In addition to the above command line parameters, there are some parameters in auto_scaling.yaml to describe the statistics of the dataset, You can modify it yourself if necessary:</p> <ul> <li>limit_side_len</li> </ul> <p>The size limit of <code>height</code> and <code>width</code> of the original input data, if it exceeds the range, it will be compressed   according to the ratio, and the degree of discreteness of the data can be adjusted.</p> <ul> <li>strategy</li> </ul> <p>Data statistics algorithm strategy, supports <code>mean_std</code> and <code>max_min</code> two algorithms, default: <code>mean_std</code>.</p> <ul> <li>mean_std</li> </ul> <pre><code>mean_std = [mean - n_std * sigma\uff0cmean + n_std * sigma]\n</code></pre> <ul> <li>max_min</li> </ul> <pre><code>max_min = [min - (max - min) * expand_ratio / 2\uff0cmax + (max - min) * expand_ratio / 2]\n</code></pre> <ul> <li>width_range/height_range</li> </ul> <p>For the width/height size limit after discrete statistics, exceeding will be filtered.</p> <ul> <li>interval</li> </ul> <p>Interval size, such as some networks may require that the input size must be a multiple of 32.</p> <ul> <li>max_scaling_num</li> </ul> <p>The maximum number of discrete values for shape scaling .</p> <ul> <li>batch_choices</li> </ul> <p>The default batch size value, if the data_path uses an image folder, the batch size information cannot be counted, and   the default value will be used.</p> <ul> <li>default_scaling</li> </ul> <p>If user does not use data_path, provide default <code>height</code> and <code>width</code> discrete values for shape scaling .</p>"},{"location":"inference/convert_tutorial/","title":"Model Conversion","text":""},{"location":"inference/convert_tutorial/#inference-model-conversion-tutorial","title":"Inference - Model Conversion Tutorial","text":"<p>MindOCR supports model inference for models trained using MindOCR and converted from third-party toolboxes(PaddelOCR and mmOCR).</p>"},{"location":"inference/convert_tutorial/#1-mindocr-models","title":"1. MindOCR models","text":"<p>The inference of MindOCR models supports MindSpore Lite backend.</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];</code></pre>"},{"location":"inference/convert_tutorial/#11-model-export","title":"1.1 Model Export","text":"<p>Before inference, it is necessary to export the trained ckpt to a MindIR file. Please run <code>tools/export.py</code>.</p> <pre><code># Export mindir of model `dbnet_resnet50` by downloading online ckpt\npython tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280\n\n# Export mindir of model `dbnet_resnet50` by loading local ckpt\npython tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt\n\n# Export mindir of model whose architecture is defined by crnn_resnet34.yaml with local checkpoint\npython tools/export.py --model_name_or_config configs/rec/crnn/crnn_resnet34.yaml --local_ckpt_path ~/.mindspore/models/crnn_resnet34-83f37f07.ckpt --data_shape 32 100\n\nFor more usage, run `python tools/export.py -h`.\n</code></pre> <p>Some models provide download links for MIndIR export files, as shown in Model List. You can jump to the corresponding model introduction page for download.</p>"},{"location":"inference/convert_tutorial/#12-model-conversion","title":"1.2 Model Conversion","text":"<p>You need to use the <code>converter_lite</code> tool to convert the above exported MindIR file offline so that it can be used for MindSpore Lite inference.</p> <p>The tutorial for the <code>converter_lite</code> command can be referred to Offline Conversion of Inference Models.</p> <p>Assuming the input model is input.mindir and the output model after <code>converter_lite</code> conversion is output.mindir, the conversion command is as follows:</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=MINDIR \\\n    --optimize=ascend_oriented \\\n    --modelFile=input.mindir \\\n    --outputFile=output \\\n    --configFile=config.txt\n</code></pre> <p>Among them, <code>config.txt</code> can be used to set the shape and inference precision of the conversion model.</p>"},{"location":"inference/convert_tutorial/#121-model-shape-setting","title":"1.2.1 Model Shape Setting","text":"<ul> <li> <p>Static Shape</p> <p>If the input name of the exported model is <code>x</code>, and the input shape is <code>(1,3,736,1280)</code>, then the <code>config.txt</code> is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> <p>The generated output.mindir is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> </li> <li> <p>Dynamic Shape(scaling)</p> <p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number and size of targets is not fixed resulting. If each inference is computed at the maximum Batch Size or maximum Image Size, it will result in wasted computational resources.</p> <p>Assuming the exported model input shape is (-1, 3, -1, -1), and the NHW axes are dynamic. Therefore, some optional values can be set during model conversion to adapt to input images of various size during inference.</p> <p><code>converter_lite</code> achieves this by setting the <code>dynamic_dims</code> parameter in <code>[ascend_context]</code> through <code>--configFile</code>. Please refer to the Dynamic Shape Configuration for details. We will refer to it as Model Shape Scaling for short.</p> <p>So, there are two options for conversion, by setting different config.txt:</p> <ul> <li> <p>Dynamic Image Size</p> <p>N uses fixed values, HW uses multiple optional values, the config.txt is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> </li> <li> <p>Dynamic Batch Size</p> <p>N uses multiple optional values, HW uses fixed values, the config.txt is as follows:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> </li> </ul> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> </li> <li> <p>Dynamic Shape</p> <p>The diffenece between <code>Dynamic Shape</code> and <code>Dynamic Shape(scaling)</code> is that <code>Dynamic Shape</code> mode can adapt diverse batch size and input shape, the config.txt is as follows: <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></p> </li> </ul> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial/#122-model-precision-mode-setting","title":"1.2.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>converter_lite</code> when converting the model. Please refer to the Ascend Conversion Tool Description, the usage of <code>precision_mode</code> parameter is described in the table of the configuration file, you can choose <code>enforce_fp16</code>, <code>enforce_fp32</code>, <code>preferred_fp32</code> and <code>enforce_origin</code> etc. So, you can add the <code>precision_mode</code> parameter in the <code>[Ascend_context]</code> of the above config.txt file to set the precision mode:</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>If not set, defaults to <code>enforce_fp16</code>.</p>"},{"location":"inference/convert_tutorial/#2-paddleocr-models","title":"2. PaddleOCR models","text":"<p>The PaddleOCR models support two inference backends: MindSpore Lite, corresponding to the MindSpore Lite MindIR model and OM model, respectively.</p> <pre><code>graph LR;\n    in1[PaddleOCR trained model] -- export --&gt; in2[PaddleOCR inference model] -- paddle2onnx --&gt; ONNX;\n    ONNX -- converter_lite --&gt; o2(MindSpore Lite MindIR);\n    ONNX -- atc --&gt; o1(OM);</code></pre> <p>Two formats of Paddle models are used here, the training model and the inference model. The differences are as follows:</p> type format description trained model .pdparams\u3001.pdopt\u3001.states PaddlePaddle trained model, which can store information such as model structure, weights, optimizer status, etc inference model inference.pdmodel\u3001inference.pdiparams PaddlePaddle inference model, which can be derived from its trained model, saving the network structure and weights. <p>After downloading the model file and decompressing it, please distinguish between the trained model and inference model according to the model format.</p>"},{"location":"inference/convert_tutorial/#21-trained-inference-model","title":"2.1 Trained -&gt; Inference model","text":"<p>In the download link of PaddleOCR model, there are two formats: trained model and inference model. If a training model is provided, it needs to be converted to the format of inference model.</p> <p>On the original PaddleOCR introduction page of each trained model, there are usually conversion script samples that only need to input the config file, model file, and save path of the trained model. The example is as follows:</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n    -c configs/det/det_r50_vd_db.yml \\\n    -o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\n    Global.save_inference_dir=./det_db\n</code></pre>"},{"location":"inference/convert_tutorial/#22-inference-model-onnx","title":"2.2 Inference model -&gt; ONNX","text":"<p>Install model conversion tool paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>For detailed usage tutorials, please refer to Paddle2ONNX model transformation and prediction\u3002</p> <p>Run the conversion command to generate the onnx model:</p> <pre><code>paddle2onnx \\\n    --model_dir det_db \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file det_db.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>The <code>input_shape_dict</code> in the parameter can generally be viewed by opening the inference model using the Netron, or found in the code in tools/export_model. py above.</p>"},{"location":"inference/convert_tutorial/#23-onnx-mindir","title":"2.3 ONNX -&gt; MindIR","text":"<p>The <code>converter_lite</code> script can be used to convert the ONNX into a MindSpore Lite MindIR. For detailed usage tutorials, please refer to Offline Conversion of Inference Models\u3002</p> <p>The conversion command is as follows:</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=det_db.onnx \\\n    --outputFile=det_db_output \\\n    --configFile=config.txt\n</code></pre> <p>The conversion process is same as for MindOCR models, except that <code>--fmk</code> needs to specify that the input is the ONNX, which will not be repeated here.</p>"},{"location":"inference/convert_tutorial/#24-onnx-om","title":"2.4 ONNX -&gt; OM","text":"<p>The ONNX model can be converted into an OM model by ATC tools.</p> <p>Ascend Tensor Compiler (ATC) is a model conversion tool built upon the heterogeneous computing architecture CANN. It is designed to convert models of open-source frameworks into .om offline models supported by Ascend AI Processor. A detailed tutorial on the tool can be found in ATC Instructions.</p>"},{"location":"inference/convert_tutorial/#241-model-shape-setting","title":"2.4.1 Model Shape Setting","text":"<p>The exported ONNX in the example has an input shape of (-1, 3, -1, -1).</p> <ul> <li> <p>Static Shape</p> <p>It can be converted to a static shape version by fixed values for NHW, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>The generated file is a static shape version, and the input image during inference needs to be resized to this input_shape to meet the input requirements.</p> </li> <li> <p>Dynamic Shape(Scaling)</p> <p>The ATC tool also supports Model Shape Scaling by parameter dynamic_dims, and some optional values can be set during model conversion to adapt to input images of various shape during inference.</p> <p>So, there are two options for conversion, by setting different command line parameters:</p> <ul> <li> <p>Dynamic Image Size</p> <p>N uses fixed values, HW uses multiple optional values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,-1,-1\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_dynamic_bs \\\n    --log=error\n</code></pre> </li> <li> <p>Dynamic Batch Size</p> <p>N uses multiple optional values, HW uses fixed values, the command is as follows:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:-1,3,736,1280\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"1;4;8;16;32\" \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_dynamic_bs \\\n    --log=error\n</code></pre> </li> </ul> <p>When converting the dynamic batch size/image size model, the option of NHW values can be set by the user based on empirical values or calculated from the dataset.</p> <p>If your model needs to support both dynamic batch size and dynamic image size togather, you can combine multiple models with different batch size, each using the same dynamic image size.</p> <p>In order to simplify the model conversion process, we have developed an automatic tool that can complete the dynamic value selection and model conversion. For detailed tutorials, please refer to Model Shape Scaling.</p> </li> </ul> <p>Notes:</p> <p>If the exported model is a static shape version, it cannot support dynamic image size and batch size conversion. It is necessary to ensure that the exported model is a dynamic shape version.</p>"},{"location":"inference/convert_tutorial/#242-model-precision-mode-setting","title":"2.4.2 Model Precision Mode Setting","text":"<p>For the precision of model inference, it is necessary to set it in <code>atc</code> when converting the model. Please refer to the Command-Line Options. Optional values include <code>force_fp16</code>, <code>force_fp32</code>, <code>allow_fp32_to_fp16</code>, <code>must_keep_origin_dtype</code>, <code>allow_mix_precision</code>, etc. So, you can add the <code>precision_mode</code> parameter in the <code>atc</code> command line to set the precision:</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>If not set, defaults to <code>force_fp16</code>.</p>"},{"location":"inference/convert_tutorial/#3-mmocr-models","title":"3. MMOCR models","text":"<p>MMOCR uses Pytorch, and its model files typically have a pth format suffix. You need to first export it to ONNX format and then convert to an OM/MindIR format file supported by ACL/MindSpore Lite.</p> <pre><code>graph LR;\n    MMOCR_pth -- export --&gt;  ONNX;\n    ONNX -- converter_lite --&gt; o2(MindSpore Lite MindIR);\n    ONNX -- atc --&gt; o1(OM);</code></pre>"},{"location":"inference/convert_tutorial/#31-mmocr-model-onnx","title":"3.1 MMOCR model -&gt; ONNX","text":"<p>MMDeploy provides the command to export MMOCR models to ONNX. For detailed tutorials, please refer to How to convert model.</p> <p>For parameter <code>deploy_cfg</code>, you need to select the <code>*_onnxruntime_dynamic.py</code> file in directory mmdeploy/configs/mmocr to export as a dynamic shape ONNX model.</p>"},{"location":"inference/convert_tutorial/#32-onnx-mindir","title":"3.2 ONNX -&gt; MindIR","text":"<p>Please refer to ONNX -&gt; MindSpore Lite MindIR in the PaddleOCR section above.</p>"},{"location":"inference/convert_tutorial/#33-onnx-om","title":"3.3 ONNX -&gt; OM","text":"<p>Please refer to ONNX -&gt; OM in the PaddleOCR section above.</p>"},{"location":"inference/environment/","title":"Environment.md","text":""},{"location":"inference/environment/#inference-environment-installation","title":"Inference - Environment Installation","text":"<p>MindOCR supports inference for Ascend310/Ascend310P device.</p> <p>Please make sure that the Ascend AI processor software package is correctly installed on your system. If it is not installed, please refer to the section Installing Ascend AI processor software package to install it.</p> <p>The MindOCR backend supports two types of inference: ACL and MindSpore Lite. Before inference using ACL mode, you need to use ATC tool to convert the model to om format, or to use converter_lite tool to convert the model to MindIR format, the specific differences are as follows:</p> ACL Mindspore Lite Conversion Tool ATC converter_lite Inference Model Format om MindIR"},{"location":"inference/environment/#1-acl-inference","title":"1. ACL inference","text":"<p>For the ACL inference of MindOCR, it currently relies on the Python API interface by MindX, which currently only supports Python 3.9.</p> package version Python 3.9 MindX 3.0.0 <p>On the basis of the Python 3.9 environment, download the mxVision SDK installation package for MindX and refer to the tutorial for installation. The main steps are as follows:</p> <pre><code># add executable permissions\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# execute the installation command\n# if prompted to specify the path to CANN, add parameters such as: --cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# set environment variable\nsource mxVision/set_env.sh\n</code></pre> <p>If use python interface, after installation, test whether mindx can be imported normally\uff1a<code>python -c \"import mindx\"</code></p> <p>If prompted that mindx cannot be found, go to the mxVision/Python directory and install the corresponding Whl package:</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> If use C++ interface, the above steps are not necessary.</p>"},{"location":"inference/environment/#2-mindspore-lite-inference","title":"2. MindSpore Lite inference","text":"<p>For the MindSpore Lite inference of MindOCR, It requires the version 2.2.0 or higher of the MindSpore Lite cloud-side inference toolkit.</p> <p>Download the Ascend version of the cloud-side inference toolkit tar.gz file, as well as the Python interface Wheel package. For example, when using Ascend\uff0csystem on Linux-x86_64\uff0cPython 3.7\uff0cand mindspore-lite 2.2.0\uff0cthe following package should be downloaded and installed:</p> <ul> <li> <p>toolkit tar.gz</p> </li> <li> <p>wheel Package</p> </li> </ul> <p>The download address provides the Python package for version 3.7. If you need other versions, please refer to the compilation tutorial.</p> <p>Just decompress the inference toolkit, and set environment variables:</p> <pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib::$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> <p>If using python interface, install the required .whl package using pip:</p> <p><pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre> The installation is not necessary if using the C++ interface.</p>"},{"location":"inference/inference_quickstart/","title":"MindOCR Models Offline Inference - Quick Start","text":""},{"location":"inference/inference_quickstart/#mindocr-models-offline-inference-quick-start","title":"MindOCR Models Offline Inference - Quick Start","text":""},{"location":"inference/inference_quickstart/#1-mindocr-model-support-list","title":"1. MindOCR Model Support List","text":""},{"location":"inference/inference_quickstart/#11-text-detection","title":"1.1 Text Detection","text":"Model Backbone Language Dataset F-score(%) FPS data shape (NCHW) Config Download DBNet MobileNetV3 en IC15 76.96 26.19 (1,3,736,1280) yaml ckpt | mindir ResNet-18 en IC15 81.73 24.04 (1,3,736,1280) yaml ckpt | mindir ResNet-50 en IC15 85.00 21.69 (1,3,736,1280) yaml ckpt | mindir ResNet-50 ch + en 12 Datasets 83.41 21.69 (1,3,736,1280) yaml ckpt | mindir DBNet++ ResNet-50 en IC15 86.79 8.46 (1,3,1152,2048) yaml ckpt | mindir ResNet-50 ch + en 12 Datasets 84.30 8.46 (1,3,1152,2048) yaml ckpt | mindir EAST ResNet-50 en IC15 86.86 6.72 (1,3,720,1280) yaml ckpt | mindir MobileNetV3 en IC15 75.32 26.77 (1,3,720,1280) yaml ckpt | mindir PSENet ResNet-152 en IC15 82.50 2.52 (1,3,1472,2624) yaml ckpt | mindir ResNet-50 en IC15 81.37 10.16 (1,3,736,1312) yaml ckpt | mindir MobileNetV3 en IC15 70.56 10.38 (1,3,736,1312) yaml ckpt | mindir FCENet ResNet50 en IC15 78.94 14.59 (1,3,736,1280) yaml ckpt | mindir"},{"location":"inference/inference_quickstart/#12-text-recognition","title":"1.2 Text Recognition","text":"Model Backbone Dict File Dataset Acc(%) FPS data shape (NCHW) Config Download CRNN VGG7 Default IC15 66.01 465.64 (1,3,32,100) yaml ckpt | mindir ResNet34_vd Default IC15 69.67 397.29 (1,3,32,100) yaml ckpt | mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) yaml ckpt | mindir SVTR Tiny Default IC15 79.92 338.04 (1,3,64,256) yaml ckpt | mindir Rare ResNet34_vd Default IC15 69.47 273.23 (1,3,32,100) yaml ckpt | mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) yaml ckpt | mindir RobustScanner ResNet-31 en_dict90.txt IC15 73.71 22.30 (1,3,48,160) yaml ckpt | mindir VisionLAN ResNet-45 Default IC15 80.07 321.37 (1,3,64,256) yaml(LA) ckpt(LA) | mindir(LA)"},{"location":"inference/inference_quickstart/#13-text-direction-classification","title":"1.3 Text Direction Classification","text":"Model Backbone Dataset F-score(%) FPS data shape (NCHW) Config Download MobileNetV3 MobileNetV3 / / / (1,3,48,192) yaml ckpt"},{"location":"inference/inference_quickstart/#2-overview-of-mindocr-inference","title":"2. Overview of MindOCR Inference","text":"<p><pre><code>graph LR;\n    subgraph Step 1\n        A[ckpt] -- export.py --&gt; B[MindIR]\n    end\n\n    subgraph Step 2\n        B -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    end\n\n    subgraph Step 3\n        C -- input --&gt; D[infer.py];\n    end\n\n    subgraph Step 4\n        D -- outputs --&gt; E[eval_rec.py/eval_det.py];\n    end\n\n    F[images] -- input --&gt; D;</code></pre> As shown in the figure above, the inference process is divided into the following steps:</p> <ol> <li>Use <code>tools/export.py</code> to export the ckpt model to MindIR model;</li> <li>Download and configure the model converter (i.e. converter_lite), and use the converter_lite tool to convert the MindIR to the MindSpore Lite MindIR;</li> <li>After preparing the MindSpore Lite MindIR and the input image, use <code>deploy/py_infer/infer.py</code> to perform inference;</li> <li>Depending on the type of model, use <code>deploy/eval_utils/eval_det.py</code> to evaluate the inference results of the text detection models, or use <code>deploy/eval_utils/eval_rec.py</code> for text recognition models.</li> </ol> <p>Note: Step 1 runs on Ascend910, GPU or CPU. Step 2, 3, 4 run on Ascend310 or 310P. </p>"},{"location":"inference/inference_quickstart/#3-mindocr-inference-methods","title":"3. MindOCR Inference Methods","text":""},{"location":"inference/inference_quickstart/#31-text-detection","title":"3.1 Text Detection","text":"<p>Let's take <code>DBNet ResNet-50 en</code> in the model support list as an example to introduce the inference method: - Download the ckpt file in the model support list and use the following command to export to MindIR, or directly download the exported mindir file from the model support list:</p> <pre><code> ``` shell\n # Use the local ckpt file to export the MindIR of the `DBNet ResNet-50 en` model\n # For more parameter usage details, please execute `python tools/export.py -h`\n python tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/dbnet.ckpt\n ```\n\n In the above command, ```--model_name_or_config``` is the model name in MindOCR or we can pass the yaml directory to it (for example ```--model_name_or_config configs/rec/crnn/crnn_resnet34.yaml```);\n\n The ```--data_shape 736 1280``` parameter indicates that the size of the model input image is [736, 1280], and each MindOCR model corresponds to a fixed export data shape. For details, see **data shape** in the model support list;\n\n ```--local_ckpt_path /path/to/dbnet.ckpt``` parameter indicates that the model file to be exported is ```/path/to/dbnet.ckpt```\n</code></pre> <ul> <li> <p>Use the converter_lite tool on Ascend310 or 310P to convert the MindIR to MindSpore Lite MindIR:</p> <p>Run the following command:  <pre><code>converter_lite \\\n     --saveType=MINDIR \\\n     --fmk=MINDIR \\\n     --optimize=ascend_oriented \\\n     --modelFile=dbnet_resnet50-c3a4aa24-fbf95c82.mindir \\\n     --outputFile=dbnet_resnet50_lite\n</code></pre>  In the above command:</p> <p><code>--fmk=MINDIR</code> indicates that the original format of the input model is MindIR, and the <code>--fmk</code> parameter also supports ONNX, etc.;</p> <p><code>--saveType=MINDIR</code> indicates that the output model format is MindIR format;</p> <p><code>--optimize=ascend_oriented</code> indicates that optimize for Ascend devices;</p> <p><code>--modelFile=dbnet_resnet50-c3a4aa24-fbf95c82.mindir</code> indicates that the current model path to be converted is <code>dbnet_resnet50-c3a4aa24-fbf95c82.mindir</code>;</p> <p><code>--outputFile=dbnet_resnet50_lite</code> indicates that the path of the output model is <code>dbnet_resnet50_lite</code>, which can be automatically generated without adding the .mindir suffix;</p> <p>After the above command is executed, the <code>dbnet_resnet50_lite.mindir</code> model file will be generated;</p> <p>Learn more about converter_lite</p> <p>Learn more about Model Conversion Tutorial</p> </li> <li> <p>Perform inference using <code>deploy/py_infer/infer.py</code> codes and <code>dbnet_resnet50_lite.mindir</code> file:</p> <p><pre><code>python deploy/py_infer/infer.py \\\n     --input_images_dir=/path/to/ic15/ch4_test_images \\\n     --det_model_path=/path/to/mindir/dbnet_resnet50_lite.mindir \\\n     --det_model_name_or_config=en_ms_det_dbnet_resnet50 \\\n     --res_save_dir=/path/to/dbnet_resnet50_results\n</code></pre>  After the execution is completed, the prediction file <code>det_results.txt</code> will be generated in the directory pointed to by the parameter <code>--res_save_dir</code></p> <p>When doing inference, you can use the <code>--vis_det_save_dir</code> parameter to visualize the results:  <p> </p> <p> Visualization of text detection results </p></p> <p>Learn more about infer.py inference parameters</p> </li> <li> <p>Evaluate the results with the following command:</p> <p><pre><code>python deploy/eval_utils/eval_det.py \\\n     --gt_path=/path/to/ic15/test_det_gt.txt \\\n     --pred_path=/path/to/dbnet_resnet50_results/det_results.txt\n</code></pre>  The result is: <code>{'recall': 0.8348579682233991, 'precision': 0.8657014478282576, 'f-score': 0.85}</code> </p> </li> </ul>"},{"location":"inference/inference_quickstart/#32-text-recognition","title":"3.2 Text Recognition","text":"<p>Let's take <code>CRNN ResNet34_vd en</code> in the model support list as an example to introduce the inference method:</p> <ul> <li> <p>Download the MindIR file in the model support list;</p> </li> <li> <p>Use the converter_lite tool on Ascend310 or 310P to convert the MindIR to MindSpore Lite MindIR:</p> <p>Run the following command:</p> <pre><code>converter_lite \\\n     --saveType=MINDIR \\\n     --fmk=MINDIR \\\n     --optimize=ascend_oriented \\\n     --modelFile=crnn_resnet34-83f37f07-eb10a0c9.mindir \\\n     --outputFile=crnn_resnet34vd_lite\n</code></pre> <p>After the above command is executed, the <code>crnn_resnet34vd_lite.mindir</code> model file will be generated;</p> <p>For a brief description of the converter_lite parameters, see the text detection example above.</p> <p>Learn more about converter_lite</p> <p>Learn more about Model Conversion Tutorial</p> </li> <li> <p>Perform inference using <code>deploy/py_infer/infer.py</code> codes and <code>crnn_resnet34vd_lite.mindir</code> file:</p> <pre><code>python deploy/py_infer/.py \\\n     --input_images_dir=/path/to/ic15/ch4_test_word_images \\\n     --rec_model_path=/path/to/mindir/crnn_resnet34vd_lite.mindir \\\n     --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n     --res_save_dir=/path/to/rec_infer_results\n</code></pre> <p>After the execution is completed, the prediction file <code>rec_results.txt</code> will be generated in the directory pointed to by the parameter <code>--res_save_dir</code>.</p> <p>Learn more about infer.py inference parameters</p> </li> <li> <p>Evaluate the results with the following command:</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n     --gt_path=/path/to/ic15/rec_gt.txt \\\n     --pred_path=/path/to/rec_infer_results/rec_results.txt\n</code></pre> </li> </ul>"},{"location":"inference/inference_quickstart/#33-text-direction-classification","title":"3.3 Text Direction Classification","text":"<p>Let's take <code>MobileNet</code> in the model support list as an example to introduce the inference method:</p> <ul> <li>Download ckpt\uff1b</li> <li>Use <code>export.py</code> and convert ckpt to mindIR<ul> <li>To Dynamic mindIR     <pre><code>python tools/export.py \\\n    --model_name_or_config configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --save_dir /path/to/save/cls_mv3 \\\n    --is_dynamic_shape True \\\n    --model_type cls\n</code></pre></li> <li>To Static mindIR     <pre><code>python tools/export.py \\\n    --model_name_or_config configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --save_dir /path/to/save/cls_mv3 \\\n    --is_dynamic_shape False \\\n    --data_shape 48 192\n</code></pre></li> </ul> </li> <li> <p>Use the converter_lite tool on Ascend310 or 310P to convert the MindIR to MindSpore Lite MindIR:</p> <p>Run the following command:  <pre><code>converter_lite \\\n     --saveType=MINDIR \\\n     --fmk=MINDIR \\\n     --optimize=ascend_oriented \\\n     --modelFile=/path/to/save/cls_mv3.mindir \\\n     --outputFile=cls_mv3_lite\n</code></pre>  After the above command is executed, the <code>cls_mv3_lite_lite.mindir</code> model file will be generated;</p> <p>Learn more about converter_lite</p> <p>Learn more about Model Conversion Tutorial</p> </li> </ul>"},{"location":"inference/inference_quickstart/#34-end-to-end-inference","title":"3.4 End to End Inference","text":"<p>Prepare mindIR according to Text Detection, Text Recognition, Text Direction Classification, and run the following command to do end-to-end inference <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50_lite.mindir \\\n    --det_model_name_or_config=en_ms_det_dbnet_resnet50 \\\n    --cls_model_path=/path/to/mindir/cls_mv3_lite.mindir \\\n    --cls_model_name_or_config=configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34vd_lite.mindir \\\n    --rec_model_name_or_config=configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=/path/to/infer_results\n</code></pre></p>"},{"location":"inference/inference_quickstart/#4faq-about-converting-and-inference","title":"4.FAQ about converting and inference","text":"<p>For problems about converting model and inference, please refer to FAQ for solutions.</p>"},{"location":"inference/inference_thirdparty_quickstart/","title":"Third-party Models Offline Inference - Quick Start","text":""},{"location":"inference/inference_thirdparty_quickstart/#third-party-models-offline-inference-quick-start","title":"Third-party Models Offline Inference - Quick Start","text":""},{"location":"inference/inference_thirdparty_quickstart/#1-third-party-model-support-list","title":"1. Third-Party Model Support List","text":"<p>MindOCR supports the inference of third-party models (PaddleOCR, MMOCR, etc.), and this document displays a list of adapted models. The performance test is based on Ascend310P, and some models have no test data set yet.</p>"},{"location":"inference/inference_thirdparty_quickstart/#11-text-detection","title":"1.1 Text Detection","text":"name model backbone dataset F-score(%) FPS source config download reference ch_pp_det_OCRv4 DBNet MobileNetV3 / / / PaddleOCR yaml infer model ch_PP-OCRv4_det ch_pp_server_det_v2.0 DBNet ResNet18_vd MLT17 46.22 21.65 PaddleOCR yaml infer model ch_ppocr_server_v2.0_det ch_pp_det_OCRv3 DBNet MobileNetV3 MLT17 33.89 22.40 PaddleOCR yaml infer model ch_PP-OCRv3_det ch_pp_det_OCRv2 DBNet MobileNetV3 MLT17 42.99 21.90 PaddleOCR yaml infer model ch_PP-OCRv2_det ch_pp_mobile_det_v2.0_slim DBNet MobileNetV3 MLT17 31.66 19.88 PaddleOCR yaml infer model ch_ppocr_mobile_slim_v2.0_det ch_pp_mobile_det_v2.0 DBNet MobileNetV3 MLT17 31.56 21.96 PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_det en_pp_det_OCRv3 DBNet MobileNetV3 IC15 42.14 55.55 PaddleOCR yaml infer model en_PP-OCRv3_det ml_pp_det_OCRv3 DBNet MobileNetV3 MLT17 66.01 22.48 PaddleOCR yaml infer model ml_PP-OCRv3_det en_pp_det_dbnet_resnet50vd DBNet ResNet50_vd IC15 79.89 21.17 PaddleOCR yaml infer model DBNet en_pp_det_psenet_resnet50vd PSE ResNet50_vd IC15 80.44 7.75 PaddleOCR yaml train model PSE en_pp_det_east_resnet50vd EAST ResNet50_vd IC15 85.58 20.70 PaddleOCR yaml train model EAST en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 81.77 22.14 PaddleOCR yaml train model SAST en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 81.36 10.66 MMOCR yaml train model DBNetpp en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 83.67 3.34 MMOCR yaml train model FCENet <p>Notice: When using the en_pp_det_psenet_resnet50vd model for inference, you need to modify the onnx file with the following command</p> <pre><code>python deploy/models_utils/onnx_optim/insert_pse_postprocess.py \\\n      --model_path=./pse_r50vd.onnx \\\n      --binary_thresh=0.0 \\\n      --scale=1.0\n</code></pre>"},{"location":"inference/inference_thirdparty_quickstart/#12-text-recognition","title":"1.2 Text recognition","text":"name model backbone dataset Acc(%) FPS source dict file config download reference ch_pp_rec_OCRv4 CRNN MobileNetV1Enhance / / / PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv4_rec ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (ch) 49.91 154.16 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_server_v2.0_rec ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (ch) 49.91 408.38 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv3_rec ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (ch) 44.59 203.34 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv2_rec ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (ch) 24.59 167.67 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_mobile_v2.0_rec en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (en) 79.79 917.01 PaddleOCR en_dict.txt yaml infer model en_PP-OCRv3_rec en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 / / / PaddleOCR en_dict.txt yaml infer model en_number_mobile_slim_v2.0_rec en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 / / / PaddleOCR en_dict.txt yaml infer model en_number_mobile_v2.0_rec korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR korean_dict.txt yaml infer model korean_PP-OCRv3_rec japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR japan_dict.txt yaml infer model japan_PP-OCRv3_rec chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR chinese_cht_dict.txt yaml infer model chinese_cht_PP-OCRv3_rec te_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR te_dict.txt yaml infer model te_PP-OCRv3_rec ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR ka_dict.txt yaml infer model ka_PP-OCRv3_rec ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR ta_dict.txt yaml infer model ta_PP-OCRv3_rec latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR latin_dict.txt yaml infer model latin_PP-OCRv3_rec arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR arabic_dict.txt yaml infer model arabic_PP-OCRv3_rec cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR cyrillic_dict.txt yaml infer model cyrillic_PP-OCRv3_rec devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR devanagari_dict.txt yaml infer model devanagari_PP-OCRv3_rec en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd IC15 66.35 420.80 PaddleOCR ic15_dict.txt yaml infer model CRNN en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd IC15 64.28 552.40 PaddleOCR ic15_dict.txt yaml infer model Rosetta en_pp_rec_vitstr_vitstr ViTSTR ViTSTR IC15 68.42 364.67 PaddleOCR EN_symbol_dict.txt yaml train model ViTSTR en_mm_rec_nrtr_resnet31 NRTR ResNet31 IC15 67.26 32.63 MMOCR english_digits_symbols.txt yaml train model NRTR en_mm_rec_satrn_shallowcnn SATRN ShallowCNN IC15 73.52 32.14 MMOCR english_digits_symbols.txt yaml train model SATRN"},{"location":"inference/inference_thirdparty_quickstart/#13-text-angle-classification","title":"1.3 Text angle classification","text":"name model dataset Acc(%) FPS source config download reference ch_pp_mobile_cls_v2.0 MobileNetV3 / / / PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_cls"},{"location":"inference/inference_thirdparty_quickstart/#2-overview-of-third-party-inference","title":"2. Overview of Third-Party Inference","text":"<pre><code>graph LR;\n    A[ThirdParty models] -- xx2onnx --&gt; B[ONNX] -- converter_lite --&gt; C[MindIR];\n    C --input --&gt; D[infer.py] -- outputs --&gt; eval_rec.py/eval_det.py;\n    H[images] --input --&gt; D[infer.py];</code></pre>"},{"location":"inference/inference_thirdparty_quickstart/#3-third-party-model-inference-methods","title":"3. Third-Party Model Inference Methods","text":""},{"location":"inference/inference_thirdparty_quickstart/#31-text-detection","title":"3.1 Text Detection","text":"<p>Let's take <code>ch_pp_det_OCRv4</code> in Third-Party Model Support List as an example to introduce the inference method:</p>"},{"location":"inference/inference_thirdparty_quickstart/#311-download-thirdparty-model-file","title":"3.1.1 Download Thirdparty model file","text":"<ul> <li>In Third-Party Model Support List, <code>infer model</code> indicates model file for inference; <code>train model</code> indicates model file for training, and it need to be converted to inference model first.</li> <li>If the model file is <code>infer model</code>, like <code>ch_pp_det_OCRv4</code>, dowload and extract infer model and get the following folder:      <code>text     ch_PP-OCRv4_det_infer/     \u251c\u2500\u2500 inference.pdmodel     \u251c\u2500\u2500 inference.pdiparams     \u251c\u2500\u2500 inference.pdiparams.info</code></li> <li>If the model file is <code>train model</code>, like <code>en_pp_det_psenet_resnet50vd</code>, dowload and extract train model and get the following folder:     <pre><code>det_r50_vd_pse_v2.0_train/\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 best_accuracy.pdparams\n</code></pre>     And it need to be converted by the following commands:     <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npython tools/export_model.py \\\n    -c configs/det/det_r50_vd_pse.yml \\\n    -o Global.pretrained_model=./det_r50_vd_pse_v2.0_train/best_accuracy  \\\n    Global.save_inference_dir=./det_db\n</code></pre>     and you will get the following folder:     <pre><code>det_db/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre></li> </ul>"},{"location":"inference/inference_thirdparty_quickstart/#312-convert-the-thirdparty-model-to-onnx-file","title":"3.1.2 Convert the thirdparty model to onnx file","text":"<p>Download and use the paddle2onnx tool</p> <p><pre><code>pip install paddle2onnx\n</code></pre> and convert the inference model into an onnx file:</p> <p><pre><code>paddle2onnx \\\n     --model_dir det_db \\\n     --model_filename inference.pdmodel \\\n     --params_filename inference.pdiparams\\\n     --save_file det_db.onnx \\\n     --opset_version 11 \\\n     --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n     --enable_onnx_checker True\n</code></pre> A brief explanation of parameters for paddle2onnx is as follows:</p> Parameter Description --model_dir Configures the directory path containing the Paddle model. --model_filename [Optional] Configures the file name storing the network structure located under <code>--model_dir</code>. --params_filename [Optional] Configures the file name storing the model parameters located under <code>--model_dir</code>. --save_file Specifies the directory path for saving the converted model. --opset_version [Optional] Configures the OpSet version for converting to ONNX. Multiple versions, such as 7~16, are currently supported, and the default is 9. --input_shape_dict Specifies the shape of the input tensor for generating a dynamic ONNX model. The format is \"{'x': [N, C, H, W]}\", where -1 represents dynamic shape. --enable_onnx_checker [Optional] Configures whether to check the correctness of the exported ONNX model. It is recommended to enable this switch, and the default is False. <p>The value of <code>--input_shape_dict</code> in the parameter can be viewed by opening the inference model through the Netron tool.</p> <p>Learn more about paddle2onnx</p> <p>The <code>det_db.onnx</code> file will be generated after the above command is executed;</p>"},{"location":"inference/inference_thirdparty_quickstart/#313-convert-onnx-file-to-lite-mindir-file","title":"3.1.3 Convert onnx file to Lite MindIR file","text":"<p>Use <code>converter_lite</code> tool on Ascend310/310P to convert onnx files to mindir:</p> <p>Create <code>config.txt</code> and specify the model input shape:</p> <ul> <li>If converting to static shape model, like static shape of <code>[1,3,736,1280]</code>, the config is as following      <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre></li> <li>If converting to dynamic shape(scaling) model, the config is as following      <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre></li> <li>If converting to dynamic shape model, the config is as following      <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></li> </ul> <p>A brief explanation of the configuration file parameters is as follows:</p> Parameter Attribute Function Description Data Type Value Description input_format Optional Specify the format of the model input String Optional values are \"NCHW\", \"NHWC\", \"ND\" input_shape Optional Specify the shape of the model input. The input_name must be the input name in the original model, arranged in order of input, separated by \";\" String For example: \"input1:[1,64,64,3];input2:[1,256,256,3]\" dynamic_dims Optional Specify dynamic BatchSize and dynamic resolution parameters String For example: \"dynamic_dims=[48,520],[48,320],[48,384]\" <p>Learn more about Configuration File Parameters</p> <p>Run the following command: <pre><code>converter_lite\\\n     --saveType=MINDIR \\\n     --fmk=ONNX \\\n     --optimize=ascend_oriented \\\n     --modelFile=det_db.onnx \\\n     --outputFile=det_db_lite \\\n     --configFile=config.txt\n</code></pre> After the above command is executed, the <code>det_db_lite.mindir</code> file will be generated;</p> <p>A brief explanation of the <code>converter_lite</code> parameters is as follows:</p> Parameter Required Parameter Description Value Range Default Remarks fmk Yes Input model format MINDIR, CAFFE, TFLITE, TF, ONNX - - saveType No Set the exported model to MINDIR or MS model format. MINDIR, MINDIR_LITE MINDIR The cloud-side inference version can only infer models converted to MINDIR format modelFile Yes Input model path - - - outputFile Yes Output model path. Do not add a suffix, \".mindir\" suffix will be generated automatically. - - - configFile No 1) Path to the quantization configuration file after training; 2) Path to the configuration file for extended functions - - - optimize No Set the model optimization type for the device. Default is none. none\u3001general\u3001gpu_oriented\u3001ascend_oriented - - <p>Learn more about converter_lite</p> <p>Learn more about Model Conversion Tutorial</p>"},{"location":"inference/inference_thirdparty_quickstart/#314-inference-with-lite-mindir","title":"3.1.4 Inference with Lite MindIR","text":"<p>Perform inference using <code>deploy/py_infer/infer.py</code> codes and <code>det_db_lite.mindir</code> model file:</p> <p><pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/det_db_lite.mindir \\\n    --det_model_name_or_config=ch_pp_det_OCRv4 \\\n    --res_save_dir=/path/to/ch_pp_det_OCRv4_results\n</code></pre> After the execution is completed, the prediction file <code>det_results.txt</code> will be generated in the directory pointed to by the parameter <code>--res_save_dir</code>.</p> <p>When doing inference, you can use the <code>--vis_det_save_dir</code> parameter to visualize the results</p> <p>Learn more about infer.py inference parameters</p>"},{"location":"inference/inference_thirdparty_quickstart/#315-evalution","title":"3.1.5 Evalution","text":"<p>Evaluate the results using the following command:</p> <pre><code>python deploy/eval_utils/eval_det.py\\\n    --gt_path=/path/to/ic15/test_det_gt.txt\\\n    --pred_path=/path/to/ch_pp_det_OCRv4_results/det_results.txt\n</code></pre>"},{"location":"inference/inference_thirdparty_quickstart/#32-text-recognition","title":"3.2 Text Recognition","text":"<p>Let's take <code>ch_pp_rec_OCRv4</code> in Third-Party Model Support List as an example to introduce the inference method:</p>"},{"location":"inference/inference_thirdparty_quickstart/#321-download-thirdparty-model-file","title":"3.2.1 Download Thirdparty model file","text":"<ul> <li>In Third-Party Model Support List, <code>infer model</code> indicates model file for inference; <code>train model</code> indicates model file for training, and it need to be converted to inference model first.</li> <li> <p>If the model file is <code>infer model</code>, like <code>ch_pp_rec_OCRv4</code>, dowload and extract infer model and get the following folder:</p> <pre><code>ch_PP-OCRv4_det_infer/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre> </li> <li> <p>If the model file is <code>train model</code>, like <code>en_pp_rec_vitstr_vitstr</code>, dowload and extract train model and get the following folder:</p> <p><pre><code>rec_vitstr_none_ce_train/\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 best_accuracy.pdparams\n</code></pre> And it need to be converted by the following commands:</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npython tools/export_model.py \\\n    -c configs/rec/rec_vitstr_none_ce.yml \\\n    -o Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy  \\\n    Global.save_inference_dir=./rec_vitstr\n</code></pre> <p>and you will get the following folder:</p> <pre><code>rec_vitstr/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre> </li> </ul>"},{"location":"inference/inference_thirdparty_quickstart/#322-convert-the-thirdparty-model-to-onnx-file","title":"3.2.2 Convert the thirdparty model to onnx file","text":"<p>Download and use the paddle2onnx tool</p> <p><pre><code>pip install paddle2onnx\n</code></pre> and convert the inference model into an onnx file:</p> <pre><code>paddle2onnx \\\n    --model_dir ch_PP-OCRv4_rec_infer \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file rec_crnn.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,48,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>The <code>rec_crnn.onnx</code> file will be generated after the above command is executed;</p> <p>Please refer to 3.1.2 Convert the thirdparty model to onnx file for details about <code>paddle2onnx</code>.</p>"},{"location":"inference/inference_thirdparty_quickstart/#323-convert-onnx-file-to-lite-mindir-file","title":"3.2.3 Convert onnx file to Lite MindIR file","text":"<p>Use <code>converter_lite</code> tool on Ascend310/310P to convert onnx files to mindir:</p> <p>Create <code>config.txt</code> and specify the model input shape:</p> <ul> <li>If converting to static shape model, like static shape of <code>[1,3,48,320]</code>, the config is as following     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,48,320]\n</code></pre></li> <li>If converting to dynamic shape(scaling) model, the config is as following     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[48,520],[48,320],[48,384],[48,360],[48,394],[48,321],[48,336],[48,368],[48,328],[48,685],[48,347]\n</code></pre></li> <li>If converting to dynamic shape model, the config is as following     <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></li> </ul> <p>For a brief description of the configuration parameters, please refer to 3.1.3 Convert onnx file to Lite MindIR file</p> <p>Run the following command: <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=rec_crnn.onnx \\\n    --outputFile=rec_crnn_lite \\\n    --configFile=config.txt\n</code></pre></p> <p>After the above command is executed, the <code>rec_crnn_lite.mindir.mindir</code> file will be generated;</p> <p>For a brief description of the <code>converter_lite</code> parameters, see the text detection example above.</p> <p>Learn more about converter_lite</p> <p>Learn more about Model Conversion Tutorial</p>"},{"location":"inference/inference_thirdparty_quickstart/#324-download-the-dictionary-file-for-recognition","title":"3.2.4 Download the Dictionary File for Recognition","text":"<p>According to Third-Party Model Support List, download ppocr_keys_v1.txt which matches with <code>ch_pp_rec_OCRv4</code>.</p>"},{"location":"inference/inference_thirdparty_quickstart/#325-inference-with-lite-mindir","title":"3.2.5 Inference with Lite MindIR","text":"<p>Perform inference using <code>deploy/py_infer/infer.py</code> codes and <code>rec_crnn_lite.mindir</code> model file:</p> <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/mlt17_ch \\\n    --rec_model_path=/path/to/mindir/rec_crnn_lite.mindir \\\n    --rec_model_name_or_config=ch_pp_rec_OCRv4 \\\n    --character_dict_path=/path/to/ppocr_keys_v1.txt \\\n    --res_save_dir=/path/to/ch_rec_infer_results\n</code></pre> <p>After the execution is completed, the prediction file <code>rec_results.txt</code> will be generated in the directory pointed to by the parameter <code>--res_save_dir</code>.</p> <p>Learn more about infer.py inference parameters</p>"},{"location":"inference/inference_thirdparty_quickstart/#326-evalution","title":"3.2.6 Evalution","text":"<p>Evaluate the results using the following command:</p> <p><pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/mlt17_ch/chinese_gt.txt \\\n    --pred_path=/path/to/en_rec_infer_results/rec_results.txt\n</code></pre> Refer Dataset converters for dataset preparation.</p>"},{"location":"inference/inference_thirdparty_quickstart/#33-text-direction-classification","title":"3.3 Text Direction Classification","text":"<p>Let's take <code>ch_pp_mobile_cls_v2</code> in Third-Party Model Support List as an example to introduce the inference method:</p>"},{"location":"inference/inference_thirdparty_quickstart/#331-download-thirdparty-model-file","title":"3.3.1 Download Thirdparty model file","text":"<p>In Third-Party Model Support List \uff0c<code>ch_pp_mobile_cls_v2.0</code> is a infer model\uff0cso convertion is not needed. dowload and extract it and get the following folder:</p> <pre><code>ch_ppocr_mobile_v2.0_cls_infer/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre>"},{"location":"inference/inference_thirdparty_quickstart/#332-convert-the-thirdparty-model-to-onnx-file","title":"3.3.2 Convert the thirdparty model to onnx file","text":"<p>convert the inference model into an onnx file:</p> <pre><code>paddle2onnx \\\n    --model_dir cls_mv3 \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file cls_mv3.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>The <code>cls_mv3.onnx</code> file will be generated after the above command is executed;</p> <p>Please refer to 3.1.2 Convert the thirdparty model to onnx file for details about <code>paddle2onnx</code>.</p>"},{"location":"inference/inference_thirdparty_quickstart/#333-convert-onnx-file-to-lite-mindir-file","title":"3.3.3 Convert onnx file to Lite MindIR file","text":"<p>Refer to 3.1.3 Convert onnx file to Lite MindIR file and create <code>config.txt</code>, here we take dynamic shape config as example <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre> And run the following command:</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=cls_mv3.onnx \\\n    --outputFile=cls_mv3_lite \\\n    --configFile=config.txt\n</code></pre> <p>After the above command is executed, the <code>cls_mv3_lite.mindir.mindir</code> file will be generated</p>"},{"location":"inference/inference_thirdparty_quickstart/#34-end-to-end-inference","title":"3.4 End to End Inference","text":"<p>Prepare mindIR according to Text Detection, Text Recognition, Text Direction Classification, and run the following command to do end-to-end inference <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/det_db_lite.mindir \\\n    --det_model_name_or_config=ch_pp_det_OCRv4 \\\n    --cls_model_path=/path/to/mindir/cls_mv3_lite.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --rec_model_path=/path/to/mindir/rec_crnn_lite.mindir \\\n    --rec_model_name_or_config=ch_pp_rec_OCRv4 \\\n    --character_dict_path=/path/to/ppocr_keys_v1.txt \\\n    --res_save_dir=/path/to/infer_results\n</code></pre></p>"},{"location":"inference/inference_thirdparty_quickstart/#4faq-about-converting-and-inference","title":"4.FAQ about converting and inference","text":"<p>For problems about converting model and inference, please refer to FAQ for solutions.</p>"},{"location":"inference/inference_tutorial/","title":"Python/C++ Inference on Ascend 310","text":""},{"location":"inference/inference_tutorial/#pythonc-inference-on-ascend-310","title":"Python/C++ Inference on Ascend 310","text":""},{"location":"inference/inference_tutorial/#1-introduction","title":"1. Introduction","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend, integrates text detection, angle classification, and text recognition, implements end-to-end OCR inference process, and optimizes inference performance using pipeline parallelism.</p> <p>The overall process of MindOCR Lite inference is as follows:</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    D[ThirdParty models] -- xx2onnx --&gt; E[ONNX] -- converter_lite --&gt; C;\n    C --input --&gt; F[MindOCR Infer] -- outputs --&gt; G[Evaluation];\n    H[images] --input --&gt; F[MindOCR Infer];</code></pre>"},{"location":"inference/inference_tutorial/#2-environment","title":"2. Environment","text":"<p>Please refer to the environment installation to configure the inference runtime environment for MindOCR, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"inference/inference_tutorial/#3-model-conversion","title":"3. Model conversion","text":"<p>MindOCR inference not only supports exported models from trained ckpt file, but also supports the third-party models, as listed in the MindOCR Models Support List and Third-party Models Support List (PaddleOCR, MMOCR, etc.).</p> <p>Please refer to the Conversion Tutorial, to convert it into a model format supported by MindOCR inference.</p>"},{"location":"inference/inference_tutorial/#4-inference-python","title":"4. Inference (Python)","text":"<p>Enter the inference directory\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"inference/inference_tutorial/#41-command-example","title":"4.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_cls_rec \\\n    --vis_pipeline_save_dir=det_cls_rec\n</code></pre> <p>The visualization images are stored in det_cls_rec, as shown in the picture.</p> <p> </p> <p> Visualization of text detection and recognition result </p> <p>The results are saved in det_cls_rec/pipeline_results.txt in the following format:</p> <pre><code>img_182.jpg   [{\"transcription\": \"cocoa\", \"points\": [[14.0, 284.0], [222.0, 274.0], [225.0, 325.0], [17.0, 335.0]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_rec \\\n    --vis_pipeline_save_dir=det_rec\n</code></pre> <p>The visualization images are stored in det_rec folder, as shown in the picture.</p> <p> </p> <p> Visualization of text detection and recognition result </p> <p>The recognition results are saved in det_rec/pipeline_results.txt in the following format:</p> <pre><code>img_498.jpg   [{\"transcription\": \"keep\", \"points\": [[819.0, 71.0], [888.0, 67.0], [891.0, 104.0], [822.0, 108.0]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --res_save_dir=det \\\n    --vis_det_save_dir=det\n</code></pre> <p>The visualization results are stored in the det folder, as shown in the picture.</p> <p> </p> <p> Visualization of text detection result </p> <p>The detection results are saved in the det/det_results.txt file in the following format:</p> <pre><code>img_108.jpg   [[[226.0, 442.0], [402.0, 416.0], [404.0, 433.0], [228.0, 459.0]], [...]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code># cls_mv3.mindir is converted from ppocr\npython infer.py \\\n    --input_images_dir=/path/to/images \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --res_save_dir=cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>recognition</li> </ul> <p>Run text recognition alone.</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=rec\n</code></pre> <p>The results will be saved in rec/rec_results.txt, with the following format:</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"inference/inference_tutorial/#42-detail-of-inference-parameter","title":"4.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Image or folder path for inference device str Ascend Device type, support Ascend device_id int 0 Device id backend str lite Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism precision_mode str None Precision mode, only supports setting by Model Conversion currently, and it takes no effect here <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results vis_det_save_dir str None Saving dir for images of with detection boxes vis_pipeline_save_dir str None Saving dir for images of with detection boxes and text vis_font_path str None Font path for drawing text crop_save_dir str None Saving path for cropped images after detection show_log bool False Whether show log when inferring save_log_dir str None Log saving dir <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection det_model_name_or_config str None Model name or YAML config file path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification cls_model_name_or_config str None Model name or YAML config file path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_model_name_or_config str None Model name or YAML config file path for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase <p>Notes\uff1a</p> <p><code>*_model_name_or_config</code> can be the model name or YAML config file path, please refer to MindOCR Models Support List and Third-party Models Support List (PaddleOCR, MMOCR, etc.).</p>"},{"location":"inference/inference_tutorial/#5-inference-c","title":"5. Inference (C++)","text":"<p>Currently, only the Chinese DBNet, CRNN, and SVTR models in the PP-OCR series are supported.</p> <p>Enter the inference directory\uff1a<code>cd deploy/cpp_infer</code>,then execute the compilation script <code>bash build.sh</code>. Once the build process is complete, an executable file named 'infer' will be generated in the 'dist' directory located in the current path.</p>"},{"location":"inference/inference_tutorial/#51-command-example","title":"5.1 Command example","text":"<ul> <li>detection + classification + recognition</li> </ul> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --cls_model_path /path/to/mindir/crnn \\\n    --rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n    --character_dict_path /path/to/ppocr_keys_v1.txt \\\n    --res_save_dir det_cls_rec\n</code></pre> <p>The results will be saved in det_cls_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection + recognition</li> </ul> <p>If you don't enter the parameters related to classification, it will skip and only perform detection+recognition.</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n    --character_dict_path /path/to/ppocr_keys_v1.txt \\\n    --res_save_dir det_rec\n</code></pre> <p>The results will be saved in det_rec/pipeline_results.txt, with the following format:</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>detection</li> </ul> <p>Run text detection alone.</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --res_save_dir det\n</code></pre> <p>The results will be saved in det/det_results.txt, with the following format:</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>classification</li> </ul> <p>Run text angle classification alone.</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --cls_model_path /path/to/mindir/crnn \\\n    --res_save_dir cls\n</code></pre> <p>The results will be saved in cls/cls_results.txt, with the following format:</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"inference/inference_tutorial/#52-detail-of-inference-parameter","title":"5.2 Detail of inference parameter","text":"<ul> <li>Basic settings</li> </ul> name type default description input_images_dir str None Image or folder path for inference device str Ascend Device type, support Ascend device_id int 0 Device id backend str acl Inference backend, support acl, lite parallel_num int 1 Number of parallel in each stage of pipeline parallelism <ul> <li>Saving Result</li> </ul> name type default description res_save_dir str inference_results Saving dir for inference results <ul> <li>Text detection</li> </ul> name type default description det_model_path str None Model path for text detection <ul> <li>Text angle classification</li> </ul> name type default description cls_model_path str None Model path for text angle classification <ul> <li>Text recognition</li> </ul> name type default description rec_model_path str None Model path for text recognition rec_config_path str None Config file for text recognition character_dict_path str None Dict file for text recognition\uff0cdefault only supports numbers and lowercase"},{"location":"inference/model_evaluation/","title":"Model evaluation.md","text":""},{"location":"inference/model_evaluation/#model-inference-evaluation","title":"Model Inference Evaluation","text":""},{"location":"inference/model_evaluation/#1-text-detection","title":"1. Text detection","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_det.py \\\n    --gt_path=/path/to/det_gt.txt \\\n    --pred_path=/path/to/prediction/det_results.txt\n</code></pre>"},{"location":"inference/model_evaluation/#2-text-recognition","title":"2. Text recognition","text":"<p>After inference, please use the following command to evaluate the results:</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/rec_gt.txt \\\n    --pred_path=/path/to/prediction/rec_results.txt \\\n    --character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>Please note that character_dict_path is an optional parameter, and the default dictionary only supports numbers and English lowercase.</p> <p>When evaluating the PaddleOCR or MMOCR series models, please refer to Third-party Model Support List to use the corresponding dictionary.</p>"},{"location":"mkdocs/contributing/","title":"Contributing","text":""},{"location":"mkdocs/contributing/#mindocr-contributing-guidelines","title":"MindOCR Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little helps, and credit will always be given.</p>"},{"location":"mkdocs/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindOCR community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"mkdocs/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"mkdocs/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"mkdocs/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"mkdocs/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to fix it.</p>"},{"location":"mkdocs/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindOCR could always use more documentation, whether as part of the official MindOCR docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"mkdocs/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindocr/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"mkdocs/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindocr</code> for local development.</p> <ol> <li>Fork the <code>mindocr</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindocr.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindocr\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindocr python=3.8\nconda activate mindocr\ncd mindocr\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you finish making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"mkdocs/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindocr/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"mkdocs/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"mkdocs/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed. Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"mkdocs/customize_data_transform/","title":"Customize Data Transformation","text":""},{"location":"mkdocs/customize_data_transform/#guideline-for-developing-your-transformation","title":"Guideline for Developing Your Transformation","text":""},{"location":"mkdocs/customize_data_transform/#writing-guideline","title":"Writing Guideline","text":"<ol> <li> <p>Each transformation is a class with a callable function. An example is shown below.</p> </li> <li> <p>The input to the transformation function is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>Please write comments for the call function to clarify the required/modified/added keys in the data dict.</p> </li> <li> <p>Add kwargs in the class init function for extension, which is used to parse global config, such as is_train.</p> </li> </ol> <pre><code>class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    \"\"\"\n\n    def __init__(self, channel, **kwargs):\n        self.is_train = kwargs.get('is_train', True)\n\n    def __call__(self, data: dict):\n        '''\n        required keys:\n            - image\n        modified keys:\n            - image\n        '''\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre>"},{"location":"mkdocs/customize_data_transform/#add-unit-test-and-visualization","title":"Add Unit Test and Visualization","text":"<p>Please add unit test in <code>tests/ut/transforms</code> for the written transformation and try to cover different cases (inputs and settings).</p> <p>Please visually check the correctness of the transformation on image and annotation using the jupyter notebook. See <code>transform_tutorial.ipynb</code>.</p>"},{"location":"mkdocs/customize_data_transform/#important-notes","title":"Important Notes","text":"<ol> <li>For spatial transformation operaions that will be used in text detection inference or evaluation (e.g. determinstic resize, scale), please record the space transformation information in <code>shape_list</code>. Otherwise, the postprocessing method won't be able to map the results back to the orignal image space. On how to record <code>shape_list</code>, please refer to DetResize.</li> </ol>"},{"location":"mkdocs/customize_dataset/","title":"Customize Dataset","text":""},{"location":"mkdocs/customize_dataset/#guideline-for-data-module","title":"Guideline for Data Module","text":""},{"location":"mkdocs/customize_dataset/#code-structure","title":"Code Structure","text":"<pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base_dataset.py                 # base dataset class with __getitem__\n\u251c\u2500\u2500 builder.py                  # API for create dataset and loader\n\u251c\u2500\u2500 det_dataset.py              # general text detection dataset class\n\u251c\u2500\u2500 rec_dataset.py              # general rec detection dataset class\n\u251c\u2500\u2500 rec_lmdb_dataset.py             # LMDB dataset class\n\u2514\u2500\u2500 transforms\n    \u251c\u2500\u2500 det_transforms.py           # processing and augmentation ops (callabel classes) especially for detection tasks\n    \u251c\u2500\u2500 general_transforms.py           # general processing and augmentation ops (callabel classes)\n    \u251c\u2500\u2500 modelzoo_transforms.py          # transformations adopted from modelzoo\n    \u251c\u2500\u2500 rec_transforms.py           # processing and augmentation ops (callabel classes) especially for recognition tasks\n    \u2514\u2500\u2500 transforms_factory.py           # API for create and run transforms\n</code></pre>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-dataset-class","title":"How to add your own dataset class","text":"<ol> <li> <p>Inherit from BaseDataset class</p> </li> <li> <p>Rewrite the following file and annotation parsing functions in BaseDataset.</p> <p>def load_data_list(self, label_file: Union[str, List[str]], sample_ratio: Union[float, List] = 1.0,  shuffle: bool = False, **kwargs) -&gt; List[dict]</p> <p>def _parse_annotation(self, data_line: str) -&gt; Union[dict, List[dict]]</p> </li> </ol>"},{"location":"mkdocs/customize_dataset/#how-to-add-your-own-data-transformation","title":"How to add your own data transformation","text":"<p>Please refer to Guideline for Developing Your Transformation</p>"},{"location":"mkdocs/customize_model/","title":"Customize a New Model","text":""},{"location":"mkdocs/customize_model/#guideline-for-model-module","title":"Guideline for Model Module","text":""},{"location":"mkdocs/customize_model/#how-to-add-a-new-model-in-mindocr","title":"How to Add a New Model in MindOCR","text":"<ol> <li> <p>Decompose the model into 3 (or 2) modules: backbone, (neck,) head. Neck is usually not involved in recognition tasks.</p> </li> <li> <p>For each module:</p> <p>a. if it is implemented in MindOCR, skip since you can get the module by the <code>build_{module}</code> function .</p> <p>b. if not, please implement it and follow the module format guideline</p> </li> <li> <p>Define your model in two ways</p> <p>a. Write a model py file, which includes the model class and specification functions. Please follow the model format guideline. It is to allows users to invoke a pre-defined model easily, such as <code>model = build_model('dbnet_resnet50', pretrained=True)</code>  .</p> <p>b. Config the architecture in a yaml file. Please follow the yaml format guideline . It is to allows users to modify a base architecture quickly in yaml file.</p> </li> <li> <p>To verify the correctness of the written model, please add your yaml config file path in <code>test_models.py</code>, modify the main function to build the desired model, and then run <code>test_models.py</code></p> </li> </ol> <pre><code>python tests/ut/test_models.py --config /path/to/yaml_config_file\n</code></pre>"},{"location":"mkdocs/customize_model/#format-guideline-for-writing-a-new-module","title":"Format Guideline for Writing a New Module","text":""},{"location":"mkdocs/customize_model/#backbone","title":"Backbone","text":"<ul> <li>File naming format: <code>models/backbones/{task}_{backbone}.py</code>, e.g, <code>det_resnet.py</code>   (since the same backbone for det and rec may differ, the task prefix is necessary)</li> <li>Class naming format: {Task}{BackboneName}{Variant} e.g. <code>class DetResNet</code></li> <li>Class <code>__init__</code> args: no limitation, define by model need.</li> <li>Class attributes: MUST contain <code>out_channels</code> (List), to describe channels of each output features. e.g. <code>self.out_channels=[256, 512, 1024, 2048]</code></li> <li>Class <code>construct</code> args: x (Tensor)</li> <li>Class <code>construct</code> return: features (List[Tensor]) for features extracted from different layers in the backbone, feature dim order <code>[bs, channels, \u2026]</code>. Expect shape of each feature: <code>[bs, channels, H, W]</code></li> </ul>"},{"location":"mkdocs/customize_model/#neck","title":"Neck","text":"<ul> <li>File naming format: <code>models/necks/{neck_name}.py</code>, e.g, <code>fpn.py</code></li> <li>Class naming format: {NeckName} e.g. <code>class FPN</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=256, **kwargs)</code>.</li> <li>Class attributes: MUST contain <code>out_channels</code> attribute, to describe channel of the output feature. e.g. <code>self.out_channels=256</code></li> <li>Class <code>construct</code> args: features (List(Tensor))</li> <li>Class <code>construct</code> return: feature (Tensor) for output feature, feature dim order <code>[bs, channels, \u2026]</code></li> </ul>"},{"location":"mkdocs/customize_model/#head","title":"Head","text":"<ul> <li>File naming: <code>models/heads/{head_name}.py</code>, e.g., <code>dbhead.py</code></li> <li>Class naming: {HeadName} e.g. <code>class DBHead</code></li> <li>Class <code>__init__</code> args: MUST contain <code>in_channels</code> param as the first position, e.g. <code>__init__(self, in_channels, out_channels=2, **kwargs)</code>.</li> <li>Class <code>construct</code> args: feature (Tensor), extra_input (Optional[Tensor]). The extra_input tensor is only applicable for head that needs recurrent input (e.g., Attention head), or heads with multiple inputs.</li> <li>Class <code>construct</code> return: prediction (Union(Tensor, Tuple[Tensor])). If there is only one output, return Tensor. If there are multiple outputs, return Tuple of Tensor, e.g., <code>return output1, output2, output_N</code>. Note that the order should match the loss function or the postprocess function.</li> </ul> <p>Note: if there is no neck in the model architecture like crnn, you can skip writing for neck. <code>BaseModel</code> will select the last feature of the features (List(Tensor)) output by Backbone, and forward it to Head module.</p>"},{"location":"mkdocs/customize_model/#format-guideline-for-model-py-file","title":"Format Guideline for Model Py File","text":"<ul> <li>File naming: <code>models/{task}_{model_class_name}.py</code>, e.g., <code>det_dbnet.py</code></li> <li>Class naming: {ModelName}, e.g., <code>class DBNet</code></li> <li>Class MUST inherent from <code>BaseModel</code>, e.g., <code>class DBNet(BaseModel)</code></li> <li>Spec. function naming: <code>{model_class_name}_{specifiation}.py</code>, e.g. <code>def dbnet_resnet50()</code> (Note: no need to add task prefix assuming no one model can solve any two tasks)</li> <li>Spec. function args: (pretrained=False, **kwargs), e.g. <code>def dbnet_resnet50(pretrained=False, **kwargs)</code>.</li> <li>Spec. function return: model (nn.Cell), which is the model instance</li> <li>Spec. function decorator: MUST add @register_model decorator, and import model file in <code>mindocr/models/__init__.py</code>, which is to register the model to the supported model list.</li> </ul> <p>After writing and registration, model can be created via the <code>build_model</code> func.  ``` python</p>"},{"location":"mkdocs/customize_model/#in-a-python-script","title":"in a python script","text":"<p>model = build_model('dbnet_resnet50', pretrained=False) <pre><code>## Format Guideline for Yaml File\n\nTo define/config the model architecture in yaml file, you should follow the keys in the following examples.\n\n\n- For models with a neck.\n\n``` python\nmodel:              # R\n  type: det\n  backbone:             # R\n    name: det_resnet50      # R, backbone specification function name\n    pretrained: False\n  neck:             # R\n    name: FPN           # R, neck class name\n    out_channels: 256       # D, neck class __init__ arg\n    #use_asf: True\n  head:             # R, head class name\n    name: ConvHead      # D, head class __init__ arg\n    out_channels: 2\n    k: 50\n</code></pre></p> <ul> <li>For models without a neck <pre><code>model:              # R\n  type: rec\n  backbone:         # R\n    name: resnet50      # R\n    pretrained: False\n  head:             # R\n    name: ConvHead      # R\n    out_channels: 30        # D\n</code></pre></li> </ul> <p>(R - Required. D - Depends on model)</p>"},{"location":"mkdocs/customize_postprocess/","title":"Customize Postprocessing Method","text":""},{"location":"mkdocs/customize_postprocess/#guideline-for-postprocessing-module","title":"Guideline for Postprocessing Module","text":""},{"location":"mkdocs/customize_postprocess/#common-protocols","title":"Common Protocols","text":"<ol> <li>Each postprocessing module is a class with a callable function.</li> <li>The input to the postprocessing function is network prediction and additional data information if needed.</li> <li>The output of the postprocessing function is a alwasy a dict, where the key is a field name, such as 'polys' for polygons in text detection, 'text' for text detection.</li> </ol>"},{"location":"mkdocs/customize_postprocess/#detection-postprocessing-api-protocols","title":"Detection Postprocessing API Protocols","text":"<ol> <li> <p>class naming: Det{Method}Postprocess</p> </li> <li> <p>class  <code>__init__()</code> args:</p> <ul> <li><code>box_type</code> (string): options are [\"quad', 'polys\"] for quadriateral and polygon text representation.</li> <li><code>rescale_fields</code> (List[str]='polys'): indicates which fields in the output dict will be rescaled to the original image space. Field name: \"polys\" for polygons</li> </ul> </li> <li> <p><code>__call__()</code> method: If inherit from <code>DetBasePostprocess</code>DetBasePostprocess<code>`, you don't need to implement this method in your Postproc. class.     Execution entry for postprocessing, which postprocess network prediction on the transformed image space to get text boxes (by</code>self._postprocess()<code>function) and then rescale them back to the original image space (by</code>self.rescale()` function).</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>shape_list</code> (Union[List, np.ndarray, ms.Tensor]): shape and scale info for each image in the batch, shape [batch_size, 4]. Each internal array of length 4 is [src_h, src_w, scale_h, scale_w], where src_h and src_w are height and width of the original image, and scale_h and scale_w are their scale ratio after image resizing respectively.</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: detection result as a dictionary with the following keys</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons mapped on the original image space, shape [batch_size, num_polygons, num_points, 2]. If <code>box_type</code> is 'quad', num_points=4, and the internal np.ndarray is of shape [4, 2]</li> <li><code>scores</code> (List[float]): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> </ul> </li> <li> <p><code>_postprocess()</code> method: Implement your postprocessing method here if inherit from <code>DetBasePostprocess</code>     Postprocess network prediction to get text boxes on the transformed image space (which will be rescaled back to original image space in call function)</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction for input batch data, shape [batch_size, ...]</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: postprocessing result as a dict with keys:</p> <ul> <li><code>polys</code> (List[List[np.ndarray]): predicted polygons on the transformed (i.e. resized normally) image space, of shape (batch_size, num_polygons, num_points, 2). If <code>box_type</code> is 'quad', num_points=4.</li> <li><code>scores</code> (np.ndarray): confidence scores for the predicted polygons, shape (batch_size, num_polygons)</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Please cast <code>pred</code> to the type you need in your implementation. Some postprocesssing steps use ops from mindspore.nn and prefer Tensor type, while some steps prefer np.ndarray type required in other libraries.</li> <li><code>_postprocess()</code> should NOT round the text box <code>polys</code> to integer in return, because they will be recaled and then rounded in the end. Rounding early will cause larger error in polygon rescaling and results in evaluation performance degradation, especially on small datasets.</li> </ul> </li> </ul> </li> <li> <p>About rescaling the polygons back to the original image spcae</p> <ul> <li>The rescaling step is necessary for a fair evaluation and is needed in cropping text regions from the orginal image in inference.</li> <li>To enable rescaling for evaluation<ol> <li>add \"shape_list\" to the <code>eval.dataset.output_columns</code> in the YAML config file of the model.</li> <li>make sure <code>rescale_fields</code> is not None (default is [\"polys\"])</li> </ol> </li> <li>To enable rescaling in inference:<ol> <li>directly parse <code>shape_list</code> (which is got from data[\"shape_list\"] after data loading) to the postprocessing function.  It works with <code>rescale_fields</code> to decide whether to do rescaling and which fields are to be rescaled.</li> </ol> </li> <li><code>shape_list</code> is originally recorded in image resize transformation, such as <code>DetResize</code>.</li> </ul> </li> </ol> <p>Example Code: DetBasePostprocess and DetDBPostprocess</p>"},{"location":"mkdocs/customize_postprocess/#recognition-postprocessing-api-protocols","title":"Recognition Postprocessing API Protocols","text":"<ol> <li> <p>class  <code>__init__()</code> should support the follow args:         - character_dict_path         - use_space_char         - blank_at_last         - lower     Please see the API docs in RecCTCLabelDecode for argument illustration.</p> </li> <li> <p><code>__call__()</code> method:</p> <ul> <li> <p>Input args:</p> <ul> <li><code>pred</code> (Union[Tensor, Tuple[Tensor]]): network prediction</li> <li><code>**kwargs</code>: args for extension</li> </ul> </li> <li> <p>Return: det_res as a dictionary with the following keys</p> <ul> <li><code>texts</code> (List[str]): list of preditected text string</li> <li><code>confs</code> (List[float]): confidence of each prediction</li> </ul> </li> </ul> </li> </ol> <p>Example code: RecCTCLabelDecode</p>"},{"location":"mkdocs/license/","title":"License.md","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"mkdocs/modelzoo_training/","title":"1. Training","text":""},{"location":"mkdocs/modelzoo_training/#training-mindocr-models-list","title":"Training - MindOCR Models List","text":""},{"location":"mkdocs/modelzoo_training/#text-detection","title":"Text Detection","text":"model dataset bs cards F-score ms/step fps amp config dbnet_mobilenetv3 icdar2015 10 1 77.28 100 100 O0 mindocr_dbnet dbnet_resnet18 icdar2015 20 1 81.73 186 108 O0 mindocr_dbnet dbnet_resnet50 icdar2015 10 1 85.05 133 75.2 O0 mindocr_dbnet dbnet++_resnet50 icdar2015 32 1 86.74 571 56 O0 mindocr_dbnet++ psenet_resnet152 icdar2015 8 8 82.06 769.6 83.16 O0 mindocr_psenet psenet_resnet50 icdar2015 8 8 81.37 304.138 210.43 O0 mindocr_psenet psenet_mobilenetv3 icdar2015 8 8 70.56 173.604 368.66 O0 mindocr_psenet east_mobilenetv3 icdar2015 20 8 75.32 138 1185 O0 mindocr_east east_resnet50 icdar2015 20 8 84.87 256 625 O0 mindocr_east fcenet_resnet50 icdar2015 8 4 84.12 2978.65 10.36 O0 mindocr_fcenet"},{"location":"mkdocs/modelzoo_training/#text-recognition","title":"Text Recognition","text":"model dataset bs cards acc ms/step fps amp config svtr_tiny IC03,13,15,IIIT,etc 512 4 90.23 459 4560 O2 mindocr_svtr crnn_vgg7 IC03,13,15,IIIT,etc 16 8 82.03 22.06 5802.71 O3 mindocr_crnn crnn_resnet34_vd IC03,13,15,IIIT,etc 64 8 84.45 76.48 6694.84 O3 mindocr_crnn rare_resnet34_vd IC03,13,15,IIIT,etc 512 4 85.19 449 4561 O2 mindocr_rare visionlan_resnet45 IC03,13,15,IIIT,etc 192 4 90.61 417 1840 O2 mindocr_visionlan master_resnet31 IC03,13,15,IIIT,etc 512 4 90.37 747 2741 O2 mindocr_master robustscanner_resnet31 IC13,15,IIIT,SVT,etc 256 4 87.86 825 310 O0 mindocr_robustscanner abinet_resnet45 IC03,13,15,IIIT,etc 768 8 91.35 718 628.11 O0 mindocr_abinet"},{"location":"mkdocs/modelzoo_training/#text-direction-classification","title":"Text Direction Classification","text":"model dataset bs cards acc ms/step fps amp config mobilenetv3 RCTW17,MTWI,LSVT 256 4 94.59 172.9 5923.5 O0 mindocr_mobilenetv3"},{"location":"mkdocs/online_inference/","title":"Python Online Inference","text":""},{"location":"mkdocs/online_inference/#mindocr-online-inference","title":"MindOCR Online Inference","text":"<p>About Online Inference: Online inference is to infer based on the native MindSpore framework by loading the model checkpoint file then running prediction with MindSpore APIs.</p> <p>Compared to offline inference (which is implemented in <code>deploy/py_infer</code> in MindOCR), online inferece does not require model conversion for target platforms and can run directly on the training devices (e.g. Ascend 910). But it requires installing the heavy AI framework and the model is not optimized for deployment.</p> <p>Thus, online inference is more suitable for demonstration and to visually evaluate model generalization ability on unseen data.</p>"},{"location":"mkdocs/online_inference/#dependency-and-installation","title":"Dependency and Installation","text":"Environment Version MindSpore &gt;=1.9 Python &gt;=3.7 <p>Supported platforms: Linux, MacOS, Windows (Not tested)</p> <p>Supported devices: CPU, GPU, and Ascend.</p> <p>Please clone MindOCR at first <pre><code>git clone https://github.com/mindspore-lab/mindocr.git\n</code></pre></p> <p>Then install the dependency by <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>For MindSpore(&gt;=1.9) installation, please follow the official installation instructions for the best fit of your machine.</p>"},{"location":"mkdocs/online_inference/#text-detection","title":"Text Detection","text":"<p>To run text detection on an input image or a directory containing multiple images, please execute</p> <pre><code>python tools/infer/text/predict_det.py  --image_dir {path_to_img or dir_to_imgs} --det_algorithm DB++\n</code></pre> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection result on img_108.jpg </p> <p>, where the saved txt file is as follows <pre><code>img_108.jpg [[[228, 440], [403, 413], [406, 433], [231, 459]], [[282, 280], [493, 252], [499, 293], [288, 321]], [[500, 253], [636, 232], [641, 269], [505, 289]], ...]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p> Visualization of text detection result on paper_sam.png </p> <p>, where the saved txt file is as follows <pre><code>paper_sam.png   [[[1161, 340], [1277, 340], [1277, 378], [1161, 378]], [[895, 335], [1152, 340], [1152, 382], [894, 378]], ...]\n</code></pre></p> <p>Notes: - For input images with high resolution, please set <code>--det_limit_side_len</code> larger, e.g., 1280. <code>--det_limit_type</code> can be set as \"min\" or \"max\", where \"min \" means limiting the image size to be at least  <code>--det_limit_side_len</code>, \"max\" means limiting the image size to be at most <code>--det_limit_side_len</code>.</p> <ul> <li> <p>For more argument illustrations and usage, please run <code>python tools/infer/text/predict_det.py -h</code> or view <code>tools/infer/text/config.py</code></p> </li> <li> <p>Currently, this script runs serially to avoid dynamic shape issue and achieve better performance.</p> </li> </ul>"},{"location":"mkdocs/online_inference/#supported-detection-algorithms-and-networks","title":"Supported Detection Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | DB  | dbnet_resnet50 | English |   | DB++ | dbnetpp_resnet50 | English |   | DB_MV3 | dbnet_mobilenetv3 | English |   | PSE | psenet_resnet152 | English |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_det.py</code>.</p>"},{"location":"mkdocs/online_inference/#text-recognition","title":"Text Recognition","text":"<p>To run text recognition on an input image or a directory containing multiple images, please execute</p> <p><pre><code>python tools/infer/text/predict_rec.py  --image_dir {path_to_img or dir_to_imgs} --rec_algorithm CRNN\n</code></pre> After running, the inference results will be saved in <code>{args.draw_img_save_dir}/rec_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <ul> <li>English text recognition</li> </ul> <p> </p> <p>  word_1216.png  </p> <p> </p> <p>  word_1217.png  </p> <p>Recognition results: <pre><code>word_1216.png   coffee\nword_1217.png   club\n</code></pre></p> <ul> <li>Chinese text recognition:</li> </ul> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>Recognition results: <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre></p> <p>Notes: - For more argument illustrations and usage, please run <code>python tools/infer/text/predict_rec.py -h</code> or view <code>tools/infer/text/config.py</code> - Both batch-wise and single-mode inference are supported. Batch mode is enabled by default for better speed. You can set the batch size via <code>--rec_batch_size</code>. You can also run in single-mode by set <code>--det_batch_mode</code> False, which may improve accuracy if the text length varies a lot.</p>"},{"location":"mkdocs/online_inference/#supported-recognition-algorithms-and-networks","title":"Supported Recognition Algorithms and Networks","text":"| **Algorithm Name** | **Network Name** | **Language** |   | :------: | :------: | :------: |   | CRNN | crnn_resnet34 | English |   | RARE | rare_resnet34 | English |   | SVTR | svtr_tiny | English|   | CRNN_CH | crnn_resnet34_ch | Chinese |   | RARE_CH | rare_resnet34_ch | Chinese |   <p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_rec.py</code></p> <p>Currently, space char recognition is not supported for the listed models. We will support it soon.</p>"},{"location":"mkdocs/online_inference/#text-detection-and-recognition-concatenation","title":"Text Detection and Recognition Concatenation","text":"<p>To run text spoting (i.e., detect all text regions then recognize each of them) on an input image or multiple images in a directory, please run:</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN\n</code></pre> <p>Note: set <code>--visualize_output True</code> if you want to visualize the detection and recognition results on the input image.</p> <p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>,  where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p> <p>Example 1:</p> <p> </p> <p>  Visualization of text detection and recognition result on img_10.jpg  </p> <p>, where the saved txt file is as follows <pre><code>img_10.jpg  [{\"transcription\": \"residential\", \"points\": [[43, 88], [149, 78], [151, 101], [44, 111]]}, {\"transcription\": \"areas\", \"points\": [[152, 83], [201, 81], [202, 98], [153, 100]]}, {\"transcription\": \"when\", \"points\": [[36, 56], [101, 56], [101, 78], [36, 78]]}, {\"transcription\": \"you\", \"points\": [[99, 54], [143, 52], [144, 78], [100, 80]]}, {\"transcription\": \"pass\", \"points\": [[140, 54], [186, 50], [188, 74], [142, 78]]}, {\"transcription\": \"by\", \"points\": [[182, 52], [208, 52], [208, 75], [182, 75]]}, {\"transcription\": \"volume\", \"points\": [[199, 30], [254, 30], [254, 46], [199, 46]]}, {\"transcription\": \"your\", \"points\": [[164, 28], [203, 28], [203, 46], [164, 46]]}, {\"transcription\": \"lower\", \"points\": [[109, 25], [162, 25], [162, 46], [109, 46]]}, {\"transcription\": \"please\", \"points\": [[31, 18], [109, 20], [108, 48], [30, 46]]}]\n</code></pre></p> <p>Example 2:</p> <p> </p> <p>  Visualization of text detection and recognition result on web_cvpr.png  </p> <p>, where the saved txt file is as follows</p> <pre><code>web_cvpr.png    [{\"transcription\": \"canada\", \"points\": [[430, 148], [540, 148], [540, 171], [430, 171]]}, {\"transcription\": \"vancouver\", \"points\": [[263, 148], [420, 148], [420, 171], [263, 171]]}, {\"transcription\": \"cvpr\", \"points\": [[32, 69], [251, 63], [254, 174], [35, 180]]}, {\"transcription\": \"2023\", \"points\": [[194, 44], [256, 45], [255, 72], [194, 70]]}, {\"transcription\": \"june\", \"points\": [[36, 45], [110, 44], [110, 70], [37, 71]]}, {\"transcription\": \"1822\", \"points\": [[114, 43], [190, 45], [190, 70], [113, 69]]}]\n</code></pre> <p>Notes: 1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_system.py -h</code> or view <code>tools/infer/text/config.py</code></p>"},{"location":"mkdocs/online_inference/#evaluation-of-the-inference-results","title":"Evaluation of the Inference Results","text":"<p>To infer on the whole ICDAR15 test set, please run: <pre><code>python tools/infer/text/predict_system.py --image_dir /path/to/icdar15/det/test_images  /\n                                          --det_algorithm {DET_ALGO}    /\n                                          --rec_algorithm {REC_ALGO}  /\n                                          --det_limit_type min  /\n                                          --det_limit_side_len 720\n</code></pre></p> <p>Note: Here we set<code>det_limit_type</code> as <code>min</code> for better performance, due to the input image in ICDAR15 is of high resolution (720x1280).</p> <p>After running, the results including image names, bounding boxes (<code>points</code>) and recognized texts (<code>transcription</code>) will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>. The format of prediction results is shown as follows.</p> <pre><code>img_1.jpg   [{\"transcription\": \"hello\", \"points\": [600, 150, 715, 157, 714, 177, 599, 170]}, {\"transcription\": \"world\", \"points\": [622, 126, 695, 129, 694, 154, 621, 151]}, ...]\nimg_2.jpg   [{\"transcription\": \"apple\", \"points\": [553, 338, 706, 318, 709, 342, 556, 362]}, ...]\n   ...\n</code></pre> <p>Prepare the ground truth file (in the same format as above), which can be obtained from the dataset conversion script in <code>tools/dataset_converters</code>, and run the following command to evaluate the prediction results.</p> <pre><code>python deploy/eval_utils/eval_pipeline.py --gt_path path/to/gt.txt --pred_path path/to/system_results.txt\n</code></pre> <p>Evaluation of the text spotting inference results on Ascend 910 with MindSpore 2.0rc1 are shown as follows.</p>   | Det. Algorithm| Rec. Algorithm |  Dataset     | Accuracy(%) | FPS (imgs/s) | |---------|----------|--------------|---------------|-------| | DBNet   | CRNN    | ICDAR15 | 57.82 | 4.86 | | PSENet  | CRNN    | ICDAR15 | 47.91 | 1.65| | PSENet (det_limit_side_len=1472 )  | CRNN    | ICDAR15 | 55.51 | 0.44 | | DBNet++   | RARE | ICDAR15 | 59.17  | 3.47 | | DBNet++   | SVTR | ICDAR15 | 64.42  | 2.49 |  <p>Notes: 1. Currently, online inference pipeline is not optimized for efficiency, thus FPS is only for comparison between models. If FPS is your highest priority, please refer to Inference on Ascend 310, which is much faster. 2. Unless extra inidication, all experiments are run with <code>--det_limit_type</code>=\"min\" and <code>--det_limit_side</code>=720. 3. SVTR is run in mixed precision mode (amp_level=O2) since it is optimized for O2.</p>"},{"location":"mkdocs/online_inference/#argument-list","title":"Argument List","text":"<p>All CLI argument definition can be viewed via <code>python tools/infer/text/predict_system.py -h</code> or reading <code>tools/infer/text/config.py</code>.</p>"},{"location":"mkdocs/online_inference/#developer-guide-how-to-add-a-new-model-for-inference","title":"Developer Guide - How to Add a New Model for Inference","text":""},{"location":"mkdocs/online_inference/#preprocessing","title":"Preprocessing","text":"<p>The optimal preprocessing strategy can vary from model to model, especially for the resize setting (keep_ratio, padding, etc). We define the preprocessing pipeline for each model in <code>tools/infer/text/preprocess.py</code> for different tasks.</p> <p>If you find the default preprocessing pipeline or hyper-params does not meet the network requirement, please extend by changing the if-else conditions or adding a new key-value pair to the <code>optimal_hparam</code> dict in <code>tools/infer/text/preprocess.py</code>, where key is the algorithm name and the value is the suitable hyper-param setting for the target network inference.</p>"},{"location":"mkdocs/online_inference/#network-inference","title":"Network Inference","text":"<p>Supported alogirhtms and their corresponding network names (which can be checked by using the <code>list_model()</code> API) are defined in the <code>algo_to_model_name</code> dict in <code>predict_det.py</code> and <code>predict_rec.py</code>.</p> <p>To add a new detection model for inference, please add a new key-value pair to <code>algo_to_model_name</code> dict, where the key is an algorithm name and the value is the corresponding network name registered in <code>mindocr/models/{your_model}.py</code>.</p> <p>By default, model weights will be loaded from the pro-defined URL in <code>mindocr/models/{your_model}.py</code>. If you want to load a local checkpoint instead, please set <code>--det_model_dir</code> or <code>--rec_model_dir</code> to the path of your local checkpoint or the directory containing a model checkpoint.</p>"},{"location":"mkdocs/online_inference/#postproprocess","title":"Postproprocess","text":"<p>Similar to preprocessing, the postprocessing method for each algorithm can vary. The postprocessing method for each algorithm is defined in <code>tools/infer/text/postprocess.py</code>.</p> <p>If you find the default postprocessing method or hyper-params does not meet the model need, please extend the if-else conditions or add a new key-value pair  to the <code>optimal_hparam</code> dict in <code>tools/infer/text/postprocess.py</code>, where the key is an algorithm name and the value is the hyper-param setting.</p>"},{"location":"reference/api_doc/","title":"API doc","text":"<p>coming soon...</p>"},{"location":"tutorials/advanced_train/","title":"Advanced Training","text":""},{"location":"tutorials/advanced_train/#tricks-gradient-accumulation-gradient-clipping-and-ema","title":"Tricks: Gradient Accumulation, Gradient Clipping, and EMA","text":"<p>All the training tricks can be configured in the model config files. After setting, please run <code>tools/train.py</code> script to initiate training.</p> <p>Example Yaml Config</p> <pre><code>train:\n  gradient_accumulation_steps: 2\n  clip_grad: True\n  clip_norm: 5.0\n  ema: True\n  ema_decay: 0.9999\n</code></pre>"},{"location":"tutorials/advanced_train/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradient accumulation is an effective way to address  memory limitation issue and allow training with large global batch size.</p> <p>To enable it, set <code>train.gradient_accumulation_steps</code> to values larger than 1 in yaml config.</p> <p>The equivalent global batch size would be <code>global_batch_size = batch_size * num_devices * gradient_accumulation_steps</code></p>"},{"location":"tutorials/advanced_train/#gradient-clipping","title":"Gradient Clipping","text":"<p>Gradient clipping is a method to address gradient explosion/overflow problem and stabilize model convergence.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code> and optionally adjust the norm value in <code>train.clip_norm</code>.</p>"},{"location":"tutorials/advanced_train/#ema","title":"EMA","text":"<p>Exponential Moving Average (EMA) can be viewed as a model ensemble method that smooths the model weights. It can help stabilize model convergence in training and usually leads to better model performance.</p> <p>To enable it, set <code>train.ema</code> to <code>True</code>. You may also adjust <code>train.ema_decay</code> to control the decay rate.</p>"},{"location":"tutorials/advanced_train/#resume-training","title":"Resume Training","text":"<p>Resuming training is useful when the training was interrupted unexpectedly.</p> <p>To resume training, set <code>model.resume</code> to <code>True</code> in the yaml config as follows: <pre><code>model:\n  resume: True\n</code></pre></p> <p>By default, it will resume from the \"train_resume.ckpt\" checkpoint file located in the directory specified in <code>train.ckpt_save_dir</code>.</p> <p>If you want to use another checkpoint to resume from, specify the checkpoint path in <code>resume</code> as follows:</p> <pre><code>model:\n  resume: /some/path/to/train_resume.ckpt\n</code></pre>"},{"location":"tutorials/advanced_train/#training-on-openi-cloud-platform","title":"Training on OpenI Cloud Platform","text":"<p>Please refer to the MindOCR OpenI Training Guideline</p>"},{"location":"tutorials/distribute_train/","title":"Distributed parallel training","text":"<p>This document provides a tutorial on distributed parallel training. There are two ways to train on the Ascend AI processor: by running scripts with OpenMPI or configuring <code>RANK_TABLE_FILE</code> for training. On GPU processors, scripts can be run with OpenMPI for training.</p> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the following commands for distributed training.</p> <ul> <li>Distributed parallel training</li> <li>1. Ascend<ul> <li>1.1 Run scripts with OpenMPI</li> <li>1.2 Configure RANK_TABLE_FILE for training</li> <li>1.2.1 Running on Eight (All) Devices</li> <li>1.2.2 Running on Four (Partial) Devices</li> </ul> </li> <li>2. GPU<ul> <li>2.1 Run scripts with OpenMPI</li> </ul> </li> </ul>"},{"location":"tutorials/distribute_train/#1-ascend","title":"1. Ascend","text":"<p>Notes:</p> <p>On Ascend platform, some common restrictions on using the distributed service are as follows:</p> <ul> <li> <p>In a single-node system, a cluster of 1, 2, 4, or 8 devices is supported. In a multi-node system, a cluster of 8 x N devices is supported.</p> </li> <li> <p>Each host has four devices numbered 0 to 3 and four devices numbered 4 to 7 deployed on two different networks. During training of 2 or 4 devices, the devices must be connected and clusters cannot be created across networks. This means, when training with 4 devices, only <code>{0, 1, 2, 3}</code> and  <code>{4, 5, 6, 7}</code> are available. While in training with 2 devices, devices cross networks, such as <code>{0, 4}</code> are not allowed. However, devices within networks, such as <code>{0, 1}</code>or <code>{1, 2}</code>, are allowed.</p> </li> </ul>"},{"location":"tutorials/distribute_train/#11-run-scripts-with-openmpi","title":"1.1 Run scripts with OpenMPI","text":"<p>On Ascend hardware platform, users can use OpenMPI's <code>mpirun</code> to run distributed training with <code>n</code> devices. For example, in DBNet Readme, the following command is used to train the model on devices <code>0</code> and <code>1</code>:</p> <pre><code># n is the number of NPUs used in training\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Note that <code>mpirun</code> will run training on sequential devices starting from device <code>0</code>. For example, <code>mpirun -n 4 python-command</code> will run training on the four devices: <code>{0, 1, 2, 3}</code>.</p>"},{"location":"tutorials/distribute_train/#12-configure-rank_table_file-for-training","title":"1.2 Configure RANK_TABLE_FILE for training","text":""},{"location":"tutorials/distribute_train/#121-running-on-eight-all-devices","title":"1.2.1 Running on Eight (All) Devices","text":"<p>Before using this method for distributed training, it is necessary to create an HCCL configuration file in json format, i.e. generate RANK_TABLE_FILE. The following is the command to generate the corresponding configuration file for 8 devices (for more information please refer to HCCL tools):</p> <p><pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> This command produces the following output file: <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre></p> <p>An example of the content in <code>hccl_8p_10234567_127.0.0.1.json</code>:</p> <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"0\",\n                    \"device_ip\": \"192.168.100.101\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"1\",\n                    \"device_ip\": \"192.168.101.101\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"2\",\n                    \"device_ip\": \"192.168.102.101\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"3\",\n                    \"device_ip\": \"192.168.103.101\",\n                    \"rank_id\": \"3\"\n                },\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"4\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"5\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"6\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"7\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> <p>Then start the training by running the following command:</p> <pre><code>bash ascend8p.sh\n</code></pre> <p>Please ensure that the <code>distribute</code> parameter in the yaml file is set to <code>True</code> before running the command.</p> <p>Here is an example of the <code>ascend8p.sh</code> script for CRNN training:</p> <p><pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$i\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre> When training other models, simply replace the yaml config file path in the script, i.e. <code>path/to/model_config.yaml</code>.</p> <p>After the training has started, and you can find the training log <code>train.log</code> in the project root directory.</p>"},{"location":"tutorials/distribute_train/#122-running-on-four-partial-devices","title":"1.2.2 Running on Four (Partial) Devices","text":"<p>To run training on four devices, for example, <code>{4, 5, 6, 7}</code>, the <code>RANK_TABLE_FILE</code> and the run script are different from those for running on eight devices.</p> <p>The <code>rank_table.json</code> is created by running the following command:</p> <pre><code>python hccl_tools.py --device_num \"[4,8)\"\n</code></pre> <p>This command produces the following output file: <pre><code>hccl_4p_4567_127.0.0.1.json\n</code></pre></p> <p>An example of the content in <code>hccl_4p_4567_127.0.0.1.json</code>:</p> <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"3\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> <p>Then start the training by running the following command:</p> <pre><code>bash ascend4p.sh\n</code></pre> <p>Here is an example of the <code>ascend4p.sh</code> script for CRNN training:</p> <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=4\nexport RANK_TABLE_FILE=\"./hccl_4p_4567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$((i+4))\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre> <p>Note that the <code>DEVICE_ID</code> and <code>RANK_ID</code> should be matched with <code>hccl_4p_4567_127.0.0.1.json</code>.</p>"},{"location":"tutorials/distribute_train/#2-gpu","title":"2. GPU","text":""},{"location":"tutorials/distribute_train/#21-run-scripts-with-openmpi","title":"2.1 Run scripts with OpenMPI","text":"<p>On GPU hardware platform, only OpenMPI's <code>mpirun</code> can be used for distributed training. The following command will run training on devices <code>0</code> and <code>1</code>.</p> <pre><code># n is the number of GPUs used in training\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>In the case when users want to run training on <code>device 2</code> and <code>device 3</code>, users can run <code>export CUDA_VISIBLE_DEVICES=2,3</code> before running the command above, or run the following command:</p> <pre><code># n is the number of GPUs used in training\nCUDA_VISIBLE_DEVICES=2,3 mpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/","title":"FAQ","text":""},{"location":"tutorials/frequently_asked_questions/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<ul> <li>Undefined symbol</li> <li>Ascend so Not Found</li> <li>Ascend Error Message <code>A39999</code></li> <li><code>acl open device 0 failed</code></li> <li>Fail to install mindocr dependency on windows</li> <li><code>RunTimeError:The device address tpe is wrong</code></li> <li>Problems related to model converting</li> <li>Problems related to inference</li> <li>Training speed of DBNet not as fast as expexted</li> <li>Error about <code>libgomp-d22c30c5.so.1.0.0</code></li> <li>Dataset Pipeline Error when training abinet on lmdb dataset</li> <li>Runtime Error when training dbnet on synthtext dataset</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q1-undefined-symbol","title":"Q1 Undefined symbol","text":"<ul> <li><code>undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore\n&gt;&gt;&gt; import mindspore lite\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/_init_.py\", line 26, in &lt;module&gt;\n      from mindspore lite.context import Context\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/context.py\", line 22, in &lt;module&gt;\n      from mindspore lite.lib import-c lite wrapper\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE\n</code></pre> <ul> <li><code>undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\". \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore_lite\n&gt;&gt;&gt; import mindspore\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/ root/miniconda3/envs/xxx/1ib/python3.7/site-packages/mindspore/_init_.py\", line 18, in &lt;module&gt;\n      from mindspore.run check import run check\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore/ run_check/_init_.py\", line 17, in &lt;module&gt;\n      from . check_version import check_version_and_env_config\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspo re/run_check/check version.py\", line 29, in &lt;module&gt;\n      from mindspore._c_expression \"import MSContext, ms_ctx_param\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv\n</code></pre> <ul> <li><code>undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv</code></li> </ul> <pre><code>[WARNING] LITE(20788, 7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.973 [mindspore/lite/tools/opt imizer/common/fommat_utils.cc:385] ConvertAbstractFommatShape] abstract must be a tensor, but got: ValueAny.\n[WARNING] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.998 [mindspore/lite/tools/optimizer/common/gllo_utils.cc: 1071] GenTransposeNode] Convertabstract failed for node: args0_nh2nc\n[WARNING] LITE(20788,7f897fO04ff40, converter_lite) :2023-10-19-07:24: 11.035.069 [mindspore/lite/src/extendrt/cxx_api/dlutils.h:124] DLSopen] dlopen /xxx/mindspore/to0ls/converter/lib/libascend pass plugin.so failed, error: /xxx/mindspore/tools/converter/1ib/libmslite_shared lib.s0: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n\n[ERROR] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 11.035.121 [mindspore/lite/tools/converter/adapter/acl/plugin/acl_pass_plugin.cc:86] CreateAclPassInner] DLSopen failed, so path: /xxx/mindspore-1ite-2.2.0.20231019-1inux-x64/tools/converter/lib/1ibascend_pass_plugin.so, ret: dlopen /xxx/mindspore/tools/converter/lib/libascend_pass_plugin.so failed, error: /xxx/mindspore/tools/converter/lib/libmslite shared lib.so: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n</code></pre> <p>The problems occur due to the mismatch of <code>mindspore</code> python whl package, <code>mindspore_lite</code> python whl package and<code>mindspore_lite</code> tar package. Please check the following items according MindSpore download and MindSpore Lite download:</p> <ul> <li>Consistent version of <code>mindspore</code>, <code>mindspore_lite</code>, e.g. 2.2.0</li> <li>Consistent version of <code>mindspore_lite</code> whl and <code>mindspore_lite</code> tar package, e.g. 2.2.0</li> <li>Both <code>mindspore_lite</code> whl package and <code>mindspore_lite</code> tar package are Cloud-side</li> </ul> <p>For example, the following combination of packages is suitable when the platform is linux x86_64 with Ascend</p> <ul> <li><code>mindspore_lite whl</code>: mindspore_lite-2.2.0-cp37-cp37m-linux_x86_64.whl</li> <li><code>mindspore_lite tar.gz</code>: mindspore-lite-2.2.0-linux-x64.tar.gz</li> <li><code>mindspore whl</code>: mindspore-2.2.0-cp37-cp37m-linux_x86_64.whl</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q2-ascend-so-not-found","title":"Q2 Ascend so Not Found","text":"<ul> <li><code>dlopen mindspore_lite/lib/libascend_kernel_plugin.so</code>, No such file or directory</li> </ul> <pre><code>File \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 95, in warpper\n  return func(*args, **kwargs)\nFile \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n  raise RuntimeError(f\"build from_file failed! Error is {ret.Tostring()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[WARNING] ME(15411,7f07f56be100, python) : 2023-10-16-00:51:42.509.780 [mindspore/lite/src/extend rt/cxx_api/dlutils.h:124] DLSopen]\ndlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python) :2023-10-16-00:51:42.509.877 [mindspo re/lite/src/extendrt/kernel/ascend/plugin/ascend_allocator_plugin.cc:70] Register] DLSopen failed, so path: /home/xxx/miniconda3/envs/ yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so , func name: CreateAclAllocator. err: dlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_ kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python):2023-10-16-00:51:42.509.893 [mindspore/lite/src/extendrt/infer_session.cc:66] HandleContext] failed register ascend allocator plugin.\n...\nraise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>This problem occur when <code>libascend_kernel plugin.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>), which contained in <code>mindspore_lite</code> tar package. The solution is as follows:</p> <ol> <li> <p>Check <code>mindspore_lite</code> tar package has been installed. If not, please follow the MindSpore Lite download, install tar package and whl package. Please make sure the packages are Clound-side and support Ascend platform. See MindSpore download for details</p> </li> <li> <p>Find and move to the <code>mindspore_lite</code> installation path, like <code>/your_path_to/mindspore-lite</code></p> </li> <li> <p>Run command <code>find ./ -name libascend_kernel_plugin.so</code> to find the so file, and you will get the following path:</p> </li> </ol> <pre><code>./runtime/lib/libascend_kernel_plugin.so\n</code></pre> <ol> <li>Add the path to to environment variables:</li> </ol> <pre><code>export LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LD_LIBRARY_PATH\n</code></pre> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>python -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n[WARNING] ME(60105:13981374421 1776, MainProcess):2023-10-25-08: 14:33.640.411 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\nTraceback (most recent call last):\nFile \"&lt;string&gt;\", line 1, in module&gt;\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/_checkparam.py\", line 1313, in wrapper\n  return func(*args, **kwargs)\nFile \"/xxx/py37/1ib/python3.7/site-packages/mindspore/context.py\", line 1456, in set_context\n  ctx.set_device_target(kwargs['device target'])\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 381, in set_device_target\n  self.set_param(ms_ctx_param.device_target, target)\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 175, in set_param\n  self._context_handle.set_param(param, value)\nRuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory\nLoad dynamic library: 1ibmindspore ascend.so.1 failed. liboptiling.so: cannot open shared object file: No such file or directory\n----------------------------------------------------\n...\n</code></pre> <p>This problem occur when <code>liboptiling.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). The solution is as follows:</p> <ol> <li> <p>Check <code>CANN</code> tar package has been installed. If not, please follow the Installing Ascend AI processor software package and install CANN</p> </li> <li> <p>Find and move to the <code>CANN</code> installation path, like <code>/your_path_to/cann</code></p> </li> <li> <p>Run command <code>find ./ -name liboptiling.so</code> to find the so file, and you will get the following path:</p> <pre><code>./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/minios/aarch64/liboptiling.so\n</code></pre> </li> <li> <p>Add the path to to environment variables(e.g. x86_64 system):</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/:$LD_LIBRARY_PATH\n</code></pre> <p>You will find the following message if the problem is solved:</p> <pre><code>The result of multiplication calculation is correct. MindSpore has been installed on platform [Ascend] successfully!\n</code></pre> </li> </ol> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>RuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_ascend.so.1 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n...\n</code></pre> <p>This problem occur when <code>libaicpu_ascend_engine.so</code> is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). The solution is as follows:</p> <ol> <li>Check <code>CANN</code> tar package has been installed. If not, please follow the Installing Ascend AI processor software package and install CANN</li> <li>Find and move to the <code>CANN</code> installation path, like <code>/your_path_to/cann</code></li> <li>Run command <code>find ./ -name libaicpu_ascend_engine.so</code> to find the so file, and you will get the following path:</li> </ol> <pre><code>./CANN-7.0/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./CANN-7.0/compiler/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./latest/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n</code></pre> <ol> <li> <p>Add the path to to environment variables(e.g. x86_64 system):</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/compiler/lib64/plugin/opskernel/:$LD_LIBRARY_PATH\n</code></pre> </li> </ol>"},{"location":"tutorials/frequently_asked_questions/#q3-ascend-error-message-a39999","title":"Q3 Ascend Error Message A39999","text":"<ul> <li>Error 1</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n     TraceBack (most recent call last):\n     Start aicpu executor failed, retCode=0x7020009 devId=0[FUNC :DeviceRetain][FILE: runtime.cc][LINE:2698]\n     check param failed, dev can not be NULL![FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2544]\n     Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2571]\n     Check param failed, context can not be null.[FUNC:NewDevice][FILE:api impl.cc][LINE:1899]\n     New device failed, retcode=0x70 10006[FUNC:SetDevice][FILE:api_impL-cc][LINE:1922]\n     rtsetDevice execute failed, reason=[device retain error][FUNC:FuncErrorReason][FILE :error message manage.ccl[LINE:50]\n     open device 0 failed runtime result = 507033.[FUNC: ReportCallError][FILE:log_inner.cpp][LINE:161]\n\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <ul> <li>Error 2</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 tsd client wait response fail, device response code[1]. load aicpu ops package failed, device[O], host pid[5653], error stack:\n[TSDaemon] checksum aicpu package failed, ret=103, [tsd_common.cpp:2242:SaveProcessConfig]17580\nCheck head tag failed, ret=279, [package_worker.cpp:537:VerifyAicpuPackage]2369\nVerify Aicpu package failed, srcPathlIhome/HMHiAiuser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz]..[package_worker.cpp:567:DecompressionAicpuPackage]2369\nDecompression AicpuPackage [/home/HwHiAiUser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [package_worker.cpp:218:LoadAICPUPackageForProcessMode]2369\nLoad aicpu package path[/home/HwHiAiUser/hdcd/device0/] fileName[ 5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [inotify_watcher.cpp:311:HandleEvent]2369\n[TSDaemon] load aicpu ops package failed, device[0], host pid[5653], [tsd_common.cpp:2054:CheckAndHandleTimeout]2374\n[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:270]\n  TraceBack (most recent call last):\n  TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n  Start aicpu executor failed, retCode=Ox7020009 devId=0[FUNC:DeviceRetain][FILE: runtime.cc] [LINE:2698]\n  Check param failed, dev can not be NULL! [FUNC:PrimaryContextRetain] [FILE: runtime.cc][LINE:2544]\n  Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain J[FILE: runtime.cc][LINE:2571]\n  Check param failed, context can not be null. [FUNC:NewDevice] [FILE:api_impl.cc][LINE: 1893]\n  New device failed, retCode=0x7010006[FUNC:SetDevice] [FILE:api impl.cc][LINE:1916]\n  rtSetDevice execute failed, reason=[device retain errorl[FUNC:FuncErrorReason] [FILE:error_message_manage.cc][LINE:50]\n  open device 0 failed, runtime result = 507033. [FUNC:ReportCalLError][FILE:log_inner.cpp]ILINE:161]\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <p>Potential Reason:</p> <ul> <li> <p>The mismatch of Ascend driver and CANN</p> </li> <li> <p>Unsuccessfully configuration of environment variables make aicpu fail, try to add the following items to environment variables(<code>LD_LIBRARY_PATH</code>):</p> </li> </ul> <pre><code>export ASCEND_OPP_PATH=${ASCEND_HOME}/latest/opp\nexport ASCEND_AICPU_PATH=${ASCEND_OPP_PATH}/..\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/#q4-acl-open-device-0-failed","title":"Q4 <code>acl open device 0 failed</code>","text":"<p>Problem <code>acl open device 0 failed</code> may occur when doing inference. For example</p> <pre><code>benchmark --modelFile=dbnet_mobilenetv3_lite.mindir --device=Ascend --inputShapes='1,3,736,1280' --loopCount=100 - -wammUpLoopCount=10\nModelPath = dbnet_mobilenetv3_lite.mindir\nModelType = MindIR\nInDatapath =\nGroupInfoFile =\nConfigFilepath =\nInDataType = bin\nLoopCount = 100\nDeviceType = Ascend\nAccuracyThreshold = 0.5\nCosineDistanceThreshold = -1.1\nWarmUpLoopCount = 10\nNumThreads = 2  InterOpParallelNum = 1\nFpl16Priority = 0   Enableparal\u00cdel = 0\ncalibDataPath =\nEnableGLTexture = 0\ncpuBindMode = HIGHER CPU\nCalibDataType = FLOAT\nResize Dims: 1 3 736 1280\nstart unified benchmark run\nIERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 : 54.833.515 Imindspore/lite/src/extend rt/kernel/ascend/model/model_infer.cc:59] Init] Acl open device 0 failed.\n[ERROR] ME (26748,7f6c73867fc0,benchmark):2023-10-26-09:51:54.833.573 [mindspore/lite/src/extend rt/kernel/ascend/src/custom_ascend_kernel.cc:141] Init] Model i\nnfer init failed.   [ERROR] ME (26748, 7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.604 [mindspore/lite/src/extendrt/session/single_op_session.cc:198] BuildCustomAscendKernelImpl] kernel init failed CustomAscend\n[ERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 :54.833.669 [mindspore/li te/src/extendrt/session/single_op_sess ion.cc:220] BuildCustomAscendKernel] Build ascend kernel failed for node: custom_0\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51 : 54.833.699 [mindspore/lite/src/extend rt/session/single_op_session.cc:302] CompileGraph] Failed to Build custom ascend kernel\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.727 [mindspore/lite/s rc/extendrt/cxx_api/model/model_impl.cc:413] BuildByBufferImpl] compile graph failed.\n[ERROR] ME (26748, 7f6c73867fc0, benchmark):2023-10-26-09:51:54.835.590 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1256] CompileGraph] ms_model_.Build failed while running\nIERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.835.627 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1325] RunBenchmark] Compile graph failed.\n[ERROR] ME(26748,7f6c73867fc0, benchmark):2023-10-26-09: 51:54.835.662 [mindspore/lite/tools/benchmark/ run_benchmark.cc :78] RunBenchmark] Run Benchmark dbnet_mobilenetv3_lite.mindi r Failed : -1\nms_model_.Build failed while running Run Benchmark dbnet mobilenetv3 lite.mindir Failed : -1\n</code></pre> <p>This problem occur when <code>acllib</code> library is not successfully added to environment variables(<code>LD_LIBRARY_PATH</code>). try to add the following items to environment variables.</p> <pre><code>export NPU_HOST_LIB=$ASCEND_HOME/latest/acllib/lib64/stub\nexport DDK_PATH=$ASCEND_HOME/latest\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/acllib/lib64\nexport ASCEND_AICPU_PATH=$ASCEND_HOME/latest/x86_64-linux\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/x86_64-linux/lib64:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"tutorials/frequently_asked_questions/#q5-fail-to-install-mindocr-dependency-on-windows","title":"Q5 Fail to install mindocr dependency on windows","text":"<p>Running the following command on windows</p> <pre><code>git clone git@gitee.com:mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>The error may occur with the following message, <code>lanms</code> package fail to install.</p> <pre><code>FileNotFoundError: [WinError 2]  the system cannot find the file specified.\n</code></pre> <p><code>lanma</code> from https://github.com/argman/EAST/  on windows may not work, and linux platform is recommanded. You can try to use <code>lanms-neo</code> to replace <code>lanms</code> on windows. You may find the following error when installing <code>lanms-neo</code>:</p> <pre><code>Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting lanms-neo==1.0.2\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7b/fe/beff7e7e4455cb9f69c5734897ca8552a57f6423b062ec86b2ebc1d79c0d/lanms_neo-1.0.2.tar.gz (39 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: lanms-neo\n  Building wheel for lanms-neo (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Building wheel for lanms-neo (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [10 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build\\lib.win-amd64-cpython-37\n      creating build\\lib.win-amd64-cpython-37\\lanms\n      copying lanms\\__init__.py -&gt; build\\lib.win-amd64-cpython-37\\lanms\n      running build_ext\n      building 'lanms._C' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for lanms-neo\nFailed to build lanms-neo\nERROR: Could not build wheels for lanms-neo, which is required to install pyproject.toml-based projects\n</code></pre> <p>The Visual-cpp-build-tools should be installed first, and you can successfully run <code>pip install lanms-neo</code>.</p> <p>Remove the <code>lanms</code> item from <code>requirements.txt</code>, and run <code>pip install -r requirements.txt</code> to finish installation.</p>"},{"location":"tutorials/frequently_asked_questions/#q6-runtimeerror-the-device-address-type-is-wrong-type-name-in-addresscpu-type-name-in-contextascend","title":"Q6 <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code>","text":"<ul> <li><code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code> may occur when exporting models:</li> </ul> <pre><code>[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.141.25 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.143.96 [mindspore/run_check/_check_version.py:460] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.144.71 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\nTraceback (most recent call last):\n  File \"tools/export.py\", line 173, in &lt;module&gt;\n    export(**vars(args))\n  File \"tools/export.py\", line 73, in export\n    net = build_model(model_cfg, pretrained=True, amp_level=amp_level)\n  File \"/xxx/mindocr/mindocr/models/builder.py\", line 52, in build_model\n    network = create_fn(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 122, in svtr_tiny_ch\n    model = SVTR(model_config)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 26, in __init__\n    BaseModel.__init__(self, config)\n  File \"/xxx/mindocr/mindocr/models/base_model.py\", line 34, in __init__\n    self.backbone = build_backbone(backbone_name, **config.backbone)\n  File \"/xxx/mindocr/mindocr/models/backbones/builder.py\", line 48, in build_backbone\n    backbone = backbone_class(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/backbones/rec_svtr.py\", line 486, in __init__\n    ops.zeros((1, num_patches, embed_dim[0]), ms.float32)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/function/array_func.py\", line 1039, in zeros\n    output = zero_op(size, value)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 314, in __call__\n    return _run_op(self, self.name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 913, in _run_op\n    stub = _pynative_executor.run_op_async(obj, op_name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/common/api.py\", line 1186, in run_op_async\n    return self._executor.run_op_async(*args)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <ul> <li>An error occurred in the calculation in Ascend mode, which raise <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import mindspore as ms\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.384 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.675 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n&gt;&gt;&gt; import mindspore.ops as ops\n&gt;&gt;&gt; ms.set_context(device_target=\"Ascend\")\n&gt;&gt;&gt; ms.run_check()\nMindSpore version:  2.2.0.20231025\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n&gt;&gt;&gt; x = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; y = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; print(ops.add(x, y))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/_stub_tensor.py\", line 49, in fun\n    return method(*arg, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 493, in __str__\n    return str(self.asnumpy())\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 964, in asnumpy\n    return Tensor_.asnumpy(self)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <p>Ascend 310 or Ascend 310P3 may not support calculation in MindSpore Ascend mode. Please refer  MindSpore download and check the consistence of MindSpore version and platform. You can also</p> <ul> <li>Change Ascend mode to CPU</li> <li>Use MindSpore Lite instead</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q7-problems-related-to-model-converting","title":"Q7 Problems related to model converting","text":"<ul> <li><code>SetGraphInputShape] Failed to find input xxx in input_shape yyy:xxxxxxxxxxx</code> occurs when converting model to Device-side <code>mindir</code> by <code>converter_lite</code>. If you convert dbnet_resnet50.mindir to Device-side <code>mindir</code> with the <code>config.txt</code> as following:</li> </ul> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=args0:[1,3,736,1280]\n</code></pre> <p>and run the following command to convert model:</p> <pre><code>converter_lite --saveType=MINDIR --fmk=MINDIR --optimize=ascend_oriented --modelFile=dbnet_resnet50.mindir --outputFile=dbnet_resnet50_lite --configFile=config.txt\n</code></pre> <p>Error may occur:</p> <pre><code>[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.385 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:756] SetGraphInputShape] Failed to find input x in input_shape args0:1,3,736,1280\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.416 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:773] ConvertGraphToOm] Failed to set graph input shape\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.427 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.439 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.450 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.461 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.476 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.486 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.555 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.564 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.006.118 [mindspore/lite/tools/converter/converter.cc:1029] HandleGraphCommon] Optimize func graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.133 [mindspore/lite/tools/converter/converter.cc:979] Convert] Handle graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.150 [mindspore/lite/tools/converter/converter.cc:1166] RunConverter] Convert model failed\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.163 [mindspore/lite/tools/converter/cxx_api/converter.cc:348] Convert] Convert model failed, ret=NULL pointer returned.\nERROR [mindspore/lite/tools/converter/converter_lite/main.cc:104] main] Convert failed. Ret: NULL pointer returned.\nConvert failed. Ret: NULL pointer returned.\n</code></pre> <p>The problem is caused by the mismatch of variable names. The error message</p> <pre><code>Failed to find input x in input_shape args0:1,3,736,1280\n</code></pre> <p>shows that variable <code>x</code> does not match with variable <code>args0</code>. Try to replace <code>args0</code> with <code>x</code> in the <code>config.txt</code>.</p> <ul> <li><code>Save ge model to buffer failed.</code> when using Cloud-Side <code>mindir</code> model to do inference.   For example, if you use Cloud-Side <code>mindir</code> model to do inference in detection stage, it may raise:</li> </ul> <pre><code>[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.120 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.157 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.253 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.292 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.224 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.280 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.307 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.332 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.359 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.388 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.430 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.453 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.673 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.698 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.216 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.270 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.351 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43138:139649752078144,Process-1:18):2023-11-10-03:40:46.255.926 [src/parallel/framework/module_base.py:38] DetInferNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:18:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_infer_node.py\", line 12, in init_self_args\n    self.text_detector.init(preprocess=False, model=True, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.698 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.755 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.782 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.807 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.834 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.864 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.904 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.928 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.162 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.188 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.599 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.646 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.712 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43123:139649752078144,Process-1:17):2023-11-10-03:40:46.324.506 [src/parallel/framework/module_base.py:38] DetPreNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:17:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_pre_node.py\", line 13, in init_self_args\n    self.text_detector.init(preprocess=True, model=False, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>Reason: - Cloud-Side <code>mindir</code> model should convert to Device-Side <code>mindir</code> model first. - Mismatch version of <code>converter_lite</code> tool and <code>mindspore_lite</code>. For example, it may fail when using <code>converter_lite 2.2</code> to get Device-Side <code>mindir</code>, and do inference with <code>mindspore_lite 2.1</code>.</p>"},{"location":"tutorials/frequently_asked_questions/#q8-problems-related-to-inference","title":"Q8 Problems related to inference","text":"<ul> <li>When doing inference with <code>deploy/py_infer/infer.py</code>, it may occur <code>TypeError: unhashable type: 'numpy.ndarray'</code></li> </ul> <pre><code>[ERROR] MINDOCR(51913:140354674829120,Process-1:28):2023-11-10-06:52:34.304.673 [src/parallel/framework/module_base.py:66] ERROR occurred in RecPostNode module for test.jpg: unhashable type: 'numpy.ndarray'.\nTraceback (most recent call last):\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 62, in call_process\n    self.process(send_data)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/recognition/rec_post_node.py\", line 24, in process\n    output = self.text_recognizer.postprocess(data[\"pred\"], batch)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_rec.py\", line 132, in postprocess\n    return self.postprocess_ops(pred)\n  File \"/home/mindocr/deploy/py_infer/src/data_process/postprocess/builder.py\", line 32, in __call__\n    return self._ops_func(*args, **kwargs)\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in __call__\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\nTypeError: unhashable type: 'numpy.ndarray'\n</code></pre> <p>This problem occurs due to shape error, please check:</p> <ul> <li>Use suitable model. For example, it may fail and pass detection model to <code>--rec_model_path</code> parameter.</li> <li>Use inference model(not training model) to do converting.</li> </ul>"},{"location":"tutorials/frequently_asked_questions/#q9-training-speed-of-dbnet-not-as-fast-as-expexted","title":"Q9 Training speed of DBNet not as fast as expexted","text":"<p>When traning DBNet series networks (including DBNet MobileNetV3, DBNet ResNet-18, DBNet ResNet-50, and DBNet++ ResNet-50) using following command, the training speed is not as fast as expexted. For instance, the training speed of DBNet MobileNetV3 can reach only 80fps which is slower than the expecting 100fps.</p> <pre><code>python tools/train.py -c configs/det/dbnet/db_mobilenetv3_icdar15.yaml\n</code></pre> <p>This problem is due to the complex data pre-processing procedures of DBNet. The data pre-processing procedures will become the performance bottleneck if the computation ability of a CPU core of the training server is relatively weak.</p> <p>Solutions</p> <ol> <li> <p>Try to set the <code>train.dataset.use_minddata</code> and <code>eval.dataset.use_minddata</code> in the configuration file to <code>True</code>. MindOCR will execute parts of data pre-processing procedures using MindSporeMindData:</p> <pre><code>...\ntrain:\n  ckpt_save_dir: './tmp_det'\n  dataset_sink_mode: True\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/train/ch4_training_images\n    label_file: ic15/det/train/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- Set this configuration\n...\neval:\n  ckpt_load_path: tmp_det/best.ckpt\n  dataset_sink_mode: False\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/test/ch4_test_images\n    label_file: ic15/det/test/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- Set this configuration\n...\n</code></pre> </li> <li> <p>Try to set the <code>train.loader.num_workers</code> in the configuration file to a larger value to enhance the number of threads fetching dataset if the training server has enough CPU cores:</p> <pre><code>...\ntrain:\n  ...\n  loader:\n    shuffle: True\n    batch_size: 10\n    drop_remainder: True\n    num_workers: 12                             &lt;-- Set this configuration\n...\n</code></pre> </li> </ol>"},{"location":"tutorials/frequently_asked_questions/#q10-error-about-libgomp-d22c30c5so100","title":"Q10 Error about <code>libgomp-d22c30c5.so.1.0.0</code>","text":"<p>The following error may occur when running mindocr <pre><code>ImportError: /root/mindocr_env/lib/python3.8/site-packages/sklearn/__check_build/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block\n</code></pre> You can try the following steps to fix it:  - search <code>libgomp-d22c30c5.so.1.0.0</code> in your python install path    <pre><code>cd /root/mindocr_env/lib/python3.8\nfind ~ -name libgomp-d22c30c5.so.1.0.0\n</code></pre>    and get the following search result:    <pre><code>/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0\n</code></pre>  - Add the so file to environment variable <code>LD_PRELOAD</code> <pre><code>export LD_PRELOAD=/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"tutorials/frequently_asked_questions/#q11-dataset-pipeline-error-when-training-abinet-on-lmdb-dataset","title":"Q11 Dataset Pipeline Error when training abinet on lmdb dataset","text":"<p>The following error may occur when training abinet on lmdb dataset <pre><code>mindocr.data.rec_lmdb_dataset WARNING - Error occurred during preprocess.\n Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'.\n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message:\n------------------------------------------------------------------\n[ERROR] No cast for the specified DataType was found.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers)\n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/py_func_op.cc(143).\n</code></pre> You can try the following steps to fix it:  - find the folder of mindspore package  - open file: <code>mindspore/dataset/transforms/transform.py</code>  - switch to line 93:   <pre><code>93        if key in EXECUTORS_LIST:\n94           # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor.UpdateOperation(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>  - replace line 97 with <code>executor = cde.Execute(self.parse())</code>, and get   <pre><code>93        if key in EXECUTORS_LIST:\n94            # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor = cde.Execute(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>   - save the file, and try to train the model.</p>"},{"location":"tutorials/frequently_asked_questions/#q12-runtime-error-when-training-dbnet-on-synthtext-dataset","title":"Q12 Runtime Error when training dbnet on synthtext dataset","text":"<p>Runtime Error occur as following when training dbnet on synthtext dataset: <pre><code>Traceback (most recent call last):\n  ...\n  File \"/root/archiconda3/envs/Python380/lib/python3.8/site-packages/mindspore/common/api.py\", line 1608, in _exec_pip\n    return self.graph_executor(args, phase)\nRuntimeError: Run task for graph:kernel_graph_1 error! The details reger to 'Ascend Error Message'\n</code></pre> Please update CANN to 7.1 version.</p>"},{"location":"tutorials/training_detection_custom_dataset/","title":"Training Detection Network with Custom Datasets","text":"<p>This document provides tutorials on how to train text detection networks using custom datasets.</p> <ul> <li>Training Detection Network with Custom Datasets</li> <li>1. Dataset preperation<ul> <li>1.1 Preparing Training Data</li> <li>1.2 Preparing Validation Data</li> </ul> </li> <li>2. Configuration File Preperation<ul> <li>2.1 Configure train/validation datasets</li> <li>2.2 Configure train/validation transform pipelines</li> <li>2.3 Configure the model architecture</li> <li>2.4 Configure training hyperparameters</li> </ul> </li> <li>3. Model Training, Evaluation, and Inference<ul> <li>3.1 Training</li> <li>3.2 Evaluation</li> <li>3.3 Inference</li> <li>3.3.1 Environment Preparation</li> <li>3.3.2 Model Conversion</li> <li>3.3.3 Inference (Python)</li> </ul> </li> </ul>"},{"location":"tutorials/training_detection_custom_dataset/#1-dataset-preperation","title":"1. Dataset preperation","text":"<p>Currently, MindOCR detection network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images, text bounding boxes, and transcriptions. An example of the target file format is: <pre><code>img_1.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>It is read by DetDataset. If your dataset is not in the same format as the example format, see instructions on how convert different datasets' annotations into the supported format.</p> <ul> <li><code>SynthTextDataset</code>: A file format provided by SynthText800k. More details about this dataset can be found here. The annotation file is a <code>.mat</code> file consisting of <code>imnames</code>(image names), <code>wordBB</code>(word-level bounding-boxes), <code>charBB</code>(character-level bounding boxes), and <code>txt</code> (text strings). It is read by SynthTextDataset. Users can take <code>SynthTextDataset</code> as a reference to write their custom dataset class.</li> </ul> <p>We recommend users to prepare text detection datasets in the <code>Common Dataset</code> format, and then use DetDataset to load the data. The following tutorials further explain on the detailed steps.</p>"},{"location":"tutorials/training_detection_custom_dataset/#11-preparing-training-data","title":"1.1 Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file <code>train_det.txt</code> at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # A list of dictionaries\nimg_1.jpg\\t[{\"transcription\": \"Genaxis Theatre\", \"points\": [[377, 117], [463, 117], [465, 130], [378, 130]]}, {\"transcription\": \"[06]\", \"points\": [[493, 115], [519, 115], [519, 131], [493, 131]]}, {...}]\nimg_2.jpg\\t[{\"transcription\": \"guardian\", \"points\": [[642, 250], [769, 230], [775, 255], [648, 275]]}]\n...\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- train_det.txt\n    |- training\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#12-preparing-validation-data","title":"1.2 Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file <code>val_det.txt</code> at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- val_det.txt\n    |- validation\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#2-configuration-file-preperation","title":"2. Configuration File Preperation","text":"<p>To prepare the corresponding configuration file, users should specify the directories for the training and validation datasets.</p>"},{"location":"tutorials/training_detection_custom_dataset/#21-configure-trainvalidation-datasets","title":"2.1 Configure train/validation datasets","text":"<p>Please select <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: train_det.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: DetDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: val_det.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre>"},{"location":"tutorials/training_detection_custom_dataset/#22-configure-trainvalidation-transform-pipelines","title":"2.2 Configure train/validation transform pipelines","text":"<p>Take the <code>train.dataset.transform_pipeline</code> field in the <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example. It specifies a set of transformations applied on the image or labels to generate the data as the model inputs or the loss function inputs. These transform functions are defined in <code>mindocr/data/transforms</code>.</p> <pre><code>...\ntrain:\n...\n  dataset:\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - RandomColorAdjust:\n          brightness: 0.1255  # 32.0 / 255\n          saturation: 0.5\n      - RandomHorizontalFlip:\n          p: 0.5\n      - RandomRotate:\n          degrees: [ -10, 10 ]\n          expand_canvas: False\n          p: 1.0\n      - RandomScale:\n          scale_range: [ 0.5, 3.0 ]\n          p: 1.0\n      - RandomCropWithBBox:\n          max_tries: 10\n          min_crop_ratio: 0.1\n          crop_size: [ 640, 640 ]\n          p: 1.0\n      - ValidatePolygons:\n      - ShrinkBinaryMap:\n          min_text_size: 8\n          shrink_ratio: 0.4\n      - BorderMap:\n          shrink_ratio: 0.4\n          thresh_min: 0.3\n          thresh_max: 0.7\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n  ...\n</code></pre> <ul> <li> <p><code>DecodeImage</code> and <code>DetLabelEncode</code>: the two transform functions parse the strings in <code>train_det.txt</code> file, load both the image and the labels, and save them as a dictionary;</p> </li> <li> <p><code>RandomColorAdjust</code>,  <code>RandomHorizontalFlip</code>, <code>RandomRotate</code>, <code>RandomScale</code>, and <code>RandomCropWithBBox</code>: these transform functions perform typical image augmentation operations. Except for <code>RandomColorAdjust</code>, all other functions alter the bounding box labels;</p> </li> <li> <p><code>ValidatePolygons</code>: it filters out the bounding boxes that are outside of the image due to previous augmentations;</p> </li> <li> <p><code>ShrinkBinaryMap</code> and <code>BorderMap</code>: they make the binary map and the border map needed for dbnet training;</p> </li> <li> <p><code>NormalizeImage</code>: it normalizes the image by the mean and variance of the ImageNet dataset;</p> </li> <li> <p><code>ToCHWImage</code>: it changes <code>HWC</code> images to <code>CHW</code> images.</p> </li> </ul> <p>For validation transform pipeline, all image augmentation operations are removed, and replaced by a simple resize function:</p> <p><pre><code>eval:\n  dataset\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - DetResize:\n          target_size: [ 736, 1280 ]\n          keep_ratio: False\n          force_divisable: True\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n</code></pre> More tutorials on transform functions can be found in the transform tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#23-configure-the-model-architecture","title":"2.3 Configure the model architecture","text":"<p>Although different models have different architectures, MindOCR formulates them as a general three-stage architecture: <code>[backbone]-&gt;[neck]-&gt;[head]</code>. Take <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> as an example:</p> <p><pre><code>model:\n  type: det\n  transform: null\n  backbone:\n    name: det_resnet50  # Only ResNet50 is supported at the moment\n    pretrained: True    # Whether to use weights pretrained on ImageNet\n  neck:\n    name: DBFPN         # FPN part of the DBNet\n    out_channels: 256\n    bias: False\n    use_asf: False      # Adaptive Scale Fusion module from DBNet++ (use it for DBNet++ only)\n  head:\n    name: DBHead\n    k: 50               # amplifying factor for Differentiable Binarization\n    bias: False\n    adaptive: True      # True for training, False for inference\n</code></pre>  The backbone, neck, and head modules are all defined under <code>mindocr/models/backbones</code>, <code>mindocr/models/necks</code>, and <code>mindocr/models/heads</code>.</p>"},{"location":"tutorials/training_detection_custom_dataset/#24-configure-training-hyperparameters","title":"2.4 Configure training hyperparameters","text":"<p>Some training hyperparameters in <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> are defined as follows: <pre><code>metric:\n  name: DetMetric\n  main_indicator: f-score\n\nloss:\n  name: DBLoss\n  eps: 1.0e-6\n  l1_scale: 10\n  bce_scale: 5\n  bce_replace: bceloss\n\nscheduler:\n  scheduler: polynomial_decay\n  lr: 0.007\n  num_epochs: 1200\n  decay_rate: 0.9\n  warmup_epochs: 3\n\noptimizer:\n  opt: SGD\n  filter_bias_and_bn: false\n  momentum: 0.9\n  weight_decay: 1.0e-4\n</code></pre> It uses <code>SGD</code> optimizer (in <code>mindocr/optim/optim.factory.py</code>) and <code>polynomial_decay</code> (in <code>mindocr/scheduler/scheduler_factory.py</code>) as the learning scheduler. The loss function is <code>DBLoss</code> (in <code>mindocr/losses/det_loss.py</code>) and the evaluation metric is <code>DetMetric</code> ( in <code>mindocr/metrics/det_metrics.py</code>).</p>"},{"location":"tutorials/training_detection_custom_dataset/#3-model-training-evaluation-and-inference","title":"3. Model Training, Evaluation, and Inference","text":"<p>When all configurations have been specified, users can start training their models. MindOCR supports evaluation and inference after the model is trained.</p>"},{"location":"tutorials/training_detection_custom_dataset/#31-training","title":"3.1 Training","text":"<ul> <li>Standalone training</li> </ul> <p>In standalone training, the model is trained on a single device (<code>device:0</code> by default). Users should set <code>system.distribute</code> in yaml config file to be <code>False</code>, and the <code>system.device_id</code> to the target device id if users want to run this model on a device other than <code>device:0</code>.</p> <p>Take <code>configs/det/dbnet/db_r50_icdar15.yaml</code> as an example, the training command is:</p> <pre><code>python tools/train.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <ul> <li>Distributed training</li> </ul> <p>In distributed training, <code>distribute</code> in yaml config file should be True. On both GPU and Ascend devices, users can use <code>mpirun</code> to launch distributed training. For example, using <code>device:0</code> and <code>device:1</code> to train:</p> <pre><code># n is the number of GPUs/NPUs\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>Sometimes, users may want to specify the device ids to run distributed training, for example, <code>device:2</code> and <code>device:3</code>.</p> <p>On GPU devices, before running the <code>mpirun</code> command above, users can run the following command:</p> <pre><code>export CUDA_VISIBLE_DEVICES=2,3\n</code></pre> <p>On Ascend devices, users should create a <code>rank_table.json</code> like this: <pre><code>Copy{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"10.155.111.140\",\n            \"device\": [\n                {\"device_id\": \"2\",\"device_ip\": \"192.3.27.6\",\"rank_id\": \"2\"},\n                {\"device_id\": \"3\",\"device_ip\": \"192.4.27.6\",\"rank_id\": \"3\"}],\n             \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> To get the <code>device_ip</code> of the target device, run <code>cat /etc/hccn.conf</code> and look for the value of <code>address_x</code>, which is the ip address. More details can be found in distributed training tutorial.</p>"},{"location":"tutorials/training_detection_custom_dataset/#32-evaluation","title":"3.2 Evaluation","text":"<p>To evaluate the accuracy of the trained model, users can use <code>tools/eval.py</code>.</p> <p>Take standalone evaluation as an example. In the yaml config file, <code>system.distribute</code> should be <code>False</code>; the <code>eval.ckpt_load_path</code> should be the target ckpt path; <code>eval.dataset_root</code>, <code>eval.data_dir</code>, and <code>eval.label_file</code> should be correctly specified. Then the evaluation can be started by running:</p> <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>MindOCR also supports to specify the arguments in the command line, by running: <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml \\\n            --opt eval.ckpt_load_path=\"/path/to/local_ckpt.ckpt\" \\\n                  eval.dataset_root=\"/path/to/val_set/root\" \\\n                  eval.data_dir=\"val_set/dir\"\\\n                  eval.label_file=\"val_set/label\"\n</code></pre></p>"},{"location":"tutorials/training_detection_custom_dataset/#33-inference","title":"3.3 Inference","text":"<p>MindOCR inference supports Ascend310/Ascend310P devices, supports MindSpore Lite and ACL inference backend. Inference Tutorial gives detailed steps on how to run inference with MindOCR, which include mainly three steps: environment preparation, model conversion, and inference.</p>"},{"location":"tutorials/training_detection_custom_dataset/#331-environment-preparation","title":"3.3.1 Environment Preparation","text":"<p>Please refer to the environment installation for more information, and pay attention to selecting the ACL/Lite environment based on the model.</p>"},{"location":"tutorials/training_detection_custom_dataset/#332-model-conversion","title":"3.3.2 Model Conversion","text":"<p>Before runing infernence, users need to export a MindIR file from the trained checkpoint. MindSpore IR (MindIR) is a function-style IR based on graph representation. The MindIR filew stores the model structure and weight parameters needed for inference.</p> <p>Given the trained dbnet checkpoint file, user can use the following commands to export MindIR:</p> <pre><code>python tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n# or\npython tools/export.py --model_name_or_config configs/det/dbnet/db_r50_icdar15.yaml --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n</code></pre> <p>The <code>data_shape</code> is the model input shape of height and width for MindIR file. It may change when the model is changed.</p> <p>Please refer to the Conversion Tutorial for more details about model conversion.</p>"},{"location":"tutorials/training_detection_custom_dataset/#333-inference-python","title":"3.3.3 Inference (Python)","text":"<p>After model conversion, the <code>output.mindir</code> is obtained. Users can go to the <code>deploy/py_infer</code> directory, and use the following command for inference:</p> <pre><code>python infer.py \\\n    --input_images_dir=/your_path_to/test_images \\\n    --device=Ascend \\\n    --device_id=0 \\\n    --det_model_path=your_path_to/output.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --backend=lite \\\n    --res_save_dir=results_dir\n</code></pre> <p>Please refer to the Inference Tutorials chapter <code>4.1 Command example</code> on more examples of inference commands.</p>"},{"location":"tutorials/training_on_openi/","title":"Training on openi.md","text":""},{"location":"tutorials/training_on_openi/#mindocr-openi-training-guideline","title":"MindOCR OpenI Training Guideline","text":"<p>This tutorial introduces the training method of MindOCR using the OpenI platform.</p>"},{"location":"tutorials/training_on_openi/#clone-the-project","title":"Clone the project","text":"<p>Click on the plus sign and choose to New Migration to clone MindOCR from GitHub to the Openi platform.</p> <p>Enter the MindOCR git url: https://github.com/mindspore-lab/mindocr.git</p>"},{"location":"tutorials/training_on_openi/#prepare-dataset","title":"Prepare Dataset","text":"<p>You can upload your own dataset or associate the project with existing datasets on the platform.</p> <p>Uploading personal datasets requires setting the available clusters to NPU.</p>"},{"location":"tutorials/training_on_openi/#prepare-pretrained-model-optional","title":"Prepare pretrained model (optional)","text":"<p>To upload pre-trained weights, choose the Model tab of your repository.</p> <p>During the import of a local model, set the model's framework to MindSpore.</p>"},{"location":"tutorials/training_on_openi/#new-training-task","title":"New Training Task","text":"<p>Select Training Task -&gt; New Training Task in the Cloudbrain tab.</p> <p>In computing resources choose Ascend NPU.</p> <p>Set the training entry point (Start File) and add run parameters.</p> <ul> <li>To load pre-trained weights, choose the uploaded previously model file in the Select Model field and add <code>ckpt_dir</code> to the run parameters. The <code>ckpt_dir</code> parameter must have the following path: <code>/cache/*.ckpt</code>, where <code>*</code> is the model's file name.</li> <li>In the AI engine, it is necessary to select MindSpore version 1.9 or higher, and set the start file to <code>tools/train.py</code></li> <li>:warning: It is necessary to set <code>enable_modelarts</code> to <code>True</code> in the run parameters.</li> <li>The model's architecture is specified in the <code>config</code> file set in the run parameters. The prefix of the file is always <code>/home/work/user-job-dir/run-version-number</code>, where <code>run-version-number</code> for the newly created training task is usually <code>V0001</code>.</li> </ul>"},{"location":"tutorials/training_on_openi/#modify-existing-training-tasks","title":"Modify existing training tasks","text":"<p>Click the modify button of an existing training task to modify its parameters and run a new training task.</p> <p>Note: <code>run-version-number</code> will change to Parents Version (current run version number) + 1, e.g. <code>V0002</code>.</p>"},{"location":"tutorials/training_on_openi/#view-training-status","title":"View training status","text":"<p>Select a training task to view configuration information, logs, resource occupancy, and download model weights.</p>"},{"location":"tutorials/training_on_openi/#reference","title":"Reference","text":"<p>[1] Modified from https://github.com/mindspore-lab/mindyolo/blob/master/tutorials/cloud/openi.md</p>"},{"location":"tutorials/training_recognition_custom_dataset/","title":"Training Recognition Network with Custom Datasets","text":"<p>This document provides tutorials on how to train recognition networks using custom datasets, including the training of recognition networks in Chinese and English languages.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#dataset-preperation","title":"Dataset preperation","text":"<p>Currently, MindOCR recognition network supports two input formats, namely - <code>Common Dataset</code>\uff1aA file format that stores images and text files. It is read by RecDataset. - <code>LMDB Dataset</code>: A file format provided by LMDB. It is read by LMDBDataset.</p> <p>The following tutorials take the use of the <code>Common Dataset</code> file format as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-training-data","title":"Preparing Training Data","text":"<p>Please place all training images in a single folder, and specify a txt file at a higher directory to label all training image names and corresponding labels. An example of the txt file is as follows:</p> <p><pre><code># File Name # Corresponding label\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> Note: Please separate image names and labels using \\tab, and avoid using spaces or other delimiters.</p> <p>The final training set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-validation-data","title":"Preparing Validation Data","text":"<p>Similarly, please place all validation images in a single folder, and specify a txt file at a higher directory to label all validation image names and corresponding labels. The final validation set will be stored in the following format:</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#dictionary-preperation","title":"Dictionary Preperation","text":"<p>To train recognition networks for different languages, users need to configure corresponding dictionaries. Only characters that exist in the dictionary will be correctly predicted by the model. MindOCR currently provides three dictionaries, corresponding to Default, Chinese and English respectively. - <code>Default Dictionary</code>\uff1aincludes lowercase English letters and numbers only. If users do not configure the dictionay, this one will be used by default. - <code>English Dictionary</code>\uff1aincludes uppercase and lowercase English letters, numbers and punctuation marks, it is place at <code>mindocr/utils/dict/en_dict.txt</code>. - <code>Chinese Dictionary</code>\uff1aincludes commonly used Chinese characters, uppercase and lowercase English letters, numbers, and punctuation marks, it is placed at <code>mindocr/utils/dict/ch_dict.txt</code>.</p> <p>Currently, MindOCR does not provide a dictionary configuration for other languages. This feature will be released in a upcoming version.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configuration-file-preperation","title":"Configuration File Preperation","text":"<p>To configure the corresponding configuration file for a specific network architecture, users need to provide the necessary settings. As an example, we can take CRNN (with backbone Resnet34) as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-english-model","title":"Configure an English Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre> <p>And also modify the corresponding dictionary location to the the English dictionary path.</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>To use the complete English dictionary, users need to modify the <code>common:num_classes</code> attribute in the corresponding configuration file, as the initial configuration file\u2019s dictionary only includes lowercase English and numbers.</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 95                                        # The number is equal to the number of dictionary characters plus 1\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 96                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\n  use_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-english-dictionary","title":"Configuring a custom English dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#configure-an-chinese-model","title":"Configure an Chinese Model","text":"<p>Please select <code>configs/rec/crnn/crnn_resnet34_ch.yaml</code> as the initial configuration file and modify the <code>train.dataset</code> and <code>eval.dataset</code> fields in it.</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: training/                                               # Training dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_training.txt                                       # Path of the training label. It will be concatenated with `dataset_root` to form a complete path.\n...\neval:\n  dataset:\n    type: RecDataset                                                  # File reading method. Here we use the `Common Dataset` format\n    dataset_root: dir/to/data/                                        # Root directory of the data\n    data_dir: validation/                                             # Validation dataset directory. It will be concatenated with `dataset_root` to form a complete path.\n    label_file: gt_validation.txt                                     # Path of the validation label. It will be concatenated with `dataset_root` to form a complete path.\n  ...\n</code></pre> <p>And also modify the corresponding dictionary location to the the Chinese dictionary path.</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>If the network needs to output spaces, it is necessary to modify the <code>common.use_space_char</code> attribute and the <code>common: num_classes</code> attribute as follows:</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 6625                                      # The number must be equal to the number of characters in the dictionary plus the number of spaces plus 1.\n  use_space_char: &amp;use_space_char True                                # Output `space` character additonaly\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#configuring-a-custom-chinese-dictionary","title":"Configuring a custom Chinese dictionary","text":"<p>The user can add, delete, or modify characters within the dictionary as needed. It is important to note that characters must be separated by newline characters <code>\\n</code>, and it is necessary to avoid having duplicate characters in the same dictionary. Additionally, the user must also modify the <code>common: num_classes</code> attribute in the configuration file to ensure that it is equal to the number of characters in the dictionary plus 1 (in the case of a seq2seq model, it is equal to the number of characters in the dictionary plus 2).</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-training","title":"Model Training","text":"<p>When all datasets and configuration files are ready, users can start training models with their own data. As each model has different training methods, users can refer to the corresponding model introduction documentation for the Model Training and Model Evaluation sections. Here, we will only use CRNN as an example.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#preparing-pre-trained-model","title":"Preparing Pre-trained Model","text":"<p>Users can use the pre-trained models that we provide as a starting point for training. Pre-trained models can often improve the convergence speed and even accuracy of the model. Taking the Chinese model as an example, the url for the pre-trained model that we provide is https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt. Users only need to add <code>model.pretrained</code> with the corresponding url in the configuration file as follows:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users encounter network issues, they can try downloading the pre-trained model to their local machine in advance, and then change <code>model.pretrained</code> to the local path as follows:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: /local_path_to_the_ckpt/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>If users do not need to use the pre-trained model, they can simply delete <code>model.pretrained</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#start-training","title":"Start Training","text":""},{"location":"tutorials/training_recognition_custom_dataset/#distributed-training","title":"Distributed Training","text":"<p>In the case of a large amount of data, we recommend that users use distributed training. For distributed training across multiple Ascend 910 devices or GPU devices, please modify the configuration parameter <code>system.distribute</code> to True, for example:</p> <pre><code># To perform distributed training on 4 GPU/Ascend devices\nmpirun -n 4 python tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#single-device-training","title":"Single Device Training","text":"<p>If you want to train or fine-tune the model on a smaller dataset without distributed training, please modify the configuration parameter <code>system.distribute</code> to <code>False</code> and run:</p> <pre><code># Training on single CPU/GPU/Ascend devices\npython tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>The training results (including checkpoint, performance of each epoch, and curve graph) will be saved in the directory configured by the <code>train.ckpt_save_dir</code> parameter in the YAML configuration file, which is set to <code>./tmp_rec</code> by default.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#resuming-training-from-checkpoint","title":"Resuming Training From Checkpoint","text":"<p>If users expect to load the optimizer, learning rate, and other information of the model while starting or continue training, they can add <code>model.resume</code> to the corresponding local model path in the configuration file as follows, and start training:</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  resume: /local_path_to_the_ckpt/model.ckpt\n...\n</code></pre>"},{"location":"tutorials/training_recognition_custom_dataset/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Some models (including CRNN, RARE, SVTR) support mixed precision training to accelerate training speed. Users can try setting the <code>system.amp_level</code> in the configuration file to <code>O2</code> to start mixed precision training, as shown in the following example:</p> <pre><code>system:\n  mode: 0\n  distribute: True\n  amp_level: O2  # Mixed precision training\n  amp_level_infer: O2\n  seed: 42\n  log_interval: 100\n  val_while_train: True\n  drop_overflow_update: False\n  ckpt_max_keep: 5\n...\n</code></pre> <p>To disable mixed precision training, change <code>system.amp_level</code> to <code>O0</code>.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-evaluation","title":"Model Evaluation","text":"<p>To evaluate the accuracy of a trained model, users can use <code>tools/eval.py</code>. Please set the <code>ckpt_load_path</code> parameter in the <code>eval</code> section of the configuration file to the file path of the model checkpoint, and set <code>system.distribute</code> to False, as shown below:</p> <pre><code>system:\n  distribute: False # During evaluation stage, set to False\n...\neval:\n  ckpt_load_path: /local_path_to_the_ckpt/model.ckpt\n</code></pre> <p>and run</p> <pre><code>python tools/eval.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>You will get a model evaluation result similar to the following:</p> <pre><code>2023-06-16 03:41:20,237:INFO:Performance: {'acc': 0.821939, 'norm_edit_distance': 0.917264}\n</code></pre> <p>The number corresponding to <code>acc</code> is the accuracy of the model.</p>"},{"location":"tutorials/training_recognition_custom_dataset/#model-inference","title":"Model Inference","text":"<p>Users can quickly obtain the inference results of the model by using the inference script. First, place the images in the same folder, and then execute:</p> <pre><code>python tools/infer/text/predict_rec.py --image_dir {dir_to_your_image_data} --rec_algorithm CRNN_CH --draw_img_save_dir inference_results\n</code></pre> <p>The results will be stored in <code>draw_img_save_dir/rec_results.txt</code>. Here are some examples:</p> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>You will get inference results similar to the following:</p> <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre>"},{"location":"tutorials/transform_tutorial/","title":"Transformation Tutorial","text":""},{"location":"tutorials/transform_tutorial/#mechanism","title":"Mechanism","text":"<ol> <li>Each transformation is a class with a callable function. An example is as follows</li> </ol> <pre><code>class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>The input for transformation is always a dict, which contain data info like img_path, raw label, etc.</p> </li> <li> <p>The transformation api should have clarify the required keys in input and the modified or/and added keys in output the data dict.</p> </li> </ol> <p>Available transformations can be checked in <code>mindocr/data/transforms/*_transform.py</code></p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#text-detection","title":"Text detection","text":""},{"location":"tutorials/transform_tutorial/#1-load-image-and-annotations","title":"1. Load image and annotations","text":""},{"location":"tutorials/transform_tutorial/#preparation","title":"Preparation","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"tutorials/transform_tutorial/#decode-the-image-decodeimage","title":"Decode the image  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"tutorials/transform_tutorial/#detlabelencode","title":"DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"tutorials/transform_tutorial/#2-image-and-annotation-processingaugmentation","title":"2. Image and annotation processing/augmentation","text":""},{"location":"tutorials/transform_tutorial/#randomcrop-eastrandomcropdata","title":"RandomCrop - EastRandomCropData","text":"<pre><code>from mindocr.data.transforms.general_transforms import RandomCropWithBBox\nimport copy\n\n#crop_data = det_transforms.EastRandomCropData(size=(640, 640))\ncrop_data = RandomCropWithBBox(crop_size=(640, 640))\n\nshow_img(data['image'])\nfor i in range(2):\n    data_cache = copy.deepcopy(data)\n    data_cropped = crop_data(data_cache)\n\n    res_crop = draw_boxes(data_cropped['image'], data_cropped['polys'])\n    show_img(res_crop)\n</code></pre>"},{"location":"tutorials/transform_tutorial/#colorjitter","title":"ColorJitter","text":"<pre><code>random_color_adj = general_transforms.RandomColorAdjust(brightness=0.4, saturation=0.5)\n\ndata_cache = copy.deepcopy(data)\n#data_cache['image'] = data_cache['image'][:,:, ::-1]\ndata_adj = random_color_adj(data_cache)\n#print(data_adj)\nshow_img(data_adj['image'], is_bgr_img=True)\n</code></pre>"},{"location":"tutorials/yaml_configuration/","title":"Configuration parameter description","text":"<ul> <li>system</li> <li>common</li> <li>model</li> <li>postprocess</li> <li>metric</li> <li>loss</li> <li>scheduler, optimizer, loss_scaler</li> <li>scheduler</li> <li>optimizer</li> <li>loss_scaler</li> <li>train, eval</li> <li>train</li> <li>eval</li> </ul> <p>This document takes <code>configs/rec/crnn/crnn_icdar15.yaml</code> as an example to describe the usage of parameters in detail.</p>"},{"location":"tutorials/yaml_configuration/#1-environment-parameters-system","title":"1. Environment parameters (system)","text":"Parameter Description Default Optional Values \u200b\u200b Remarks mode Mindspore running mode (static graph/dynamic graph) 0 0 / 1 0: means running in GRAPH_MODE mode; 1: PYNATIVE_MODE mode distribute Whether to enable parallel training True True / False \\ device_id Specify the device id while standalone training 7 The ids of all devices in the server Only valid when distribute=False (standalone training) and environment variable 'DEVICE_ID' is NOT set. While standalone training, if both this arg and environment variable 'DEVICE_ID' are NOT set, use device 0 by default. amp_level Mixed precision mode O0 O0/O1/O2/O3 'O0' - no change.  'O1' - convert the cells and operations in the whitelist to float16 precision, and keep the rest in float32 precision.  'O2' - Keep the cells and operations in the blacklist with float32 precision, and convert the rest to float16 precision.  'O3' - Convert all networks to float16 precision.  Notice: Model prediction or evaluation does not support 'O3' on GPU platform. If amp_level is set to 'O3' for model prediction and evaluation on GPU platform, the program will switch it to 'O2' automatically. seed Random seed 42 Integer \\ ckpt_save_policy The policy for saving model weights top_k \"top_k\" or \"latest_k\" \"top_k\" means to keep the top k checkpoints according to the metric score; \"latest_k\" means to keep the last k checkpoints. The value of <code>k</code> is set via <code>ckpt_max_keep</code> ckpt_max_keep The maximum number of checkpoints to keep during training 5 Integer \\ log_interval The interval of printing logs (unit: epoch) 100 Integer \\ val_while_train Whether to enable the evaluation mode while training True True/False If the value is True, please configure the eval data set synchronously val_start_epoch From which epoch to run the evaluation 1 Interger val_interval Evaluation interval (unit: epoch) 1 Interger drop_overflow_update Whether not updating network parameters when loss/gradient overflows True True/False If value is true, network parameters will not be updated when overflow occurs"},{"location":"tutorials/yaml_configuration/#2-shared-parameters-common","title":"2. Shared parameters (common)","text":"<p>Because the same parameter may need to be reused in different configuration sections, you can customize some common parameters in this section for easy management.</p>"},{"location":"tutorials/yaml_configuration/#3-model-architecture-model","title":"3. Model architecture (model)","text":"<p>In MindOCR, the network architecture of the model is divided into four modules: Transform, Backbone, Neck and Head. For details, please refer to documentation, the following are the configuration instructions and examples of each module.</p> Parameter Description Default Remarks type Network type - Currently supports rec/det; 'rec' means recognition task, 'det' means detection task pretrained Specify pre-trained weight path or url null Supports local checkpoint path or url transform: Transformation method configuration null name Specify transformation method name - Currently supports STN_ON backbone: Backbone network configuration name Specify the backbone network class name or function name - Currently defined classes include rec_resnet34, rec_vgg7, SVTRNet and det_resnet18, det_resnet50, det_resnet152, det_mobilenet_v3. You can also customize new classes, please refer to the documentation for definition. pretrained Whether to load pre-trained backbone weights False Supports bool type or str type to be passed in. If it is True, the default weight will be downloaded and loaded through the url link defined in the backbone py file. If str is passed in, the local checkpoint path or url path can be specified for loading. neck: Network Neck configuration name Neck class name - Currently defined classes include RNNEncoder, DBFPN, EASTFPN and PSEFPN. New classes can also be customized, please refer to the documentation for definition. hidden_size RNN hidden layer unit number - \\ head: Network prediction header configuration name Head class name - Currently supports CTCHead, AttentionHead, DBHead, EASTHead and PSEHead weight_init Set weight initialization 'normal' \\ bias_init Set bias initialization 'zeros' \\ out_channels Set the number of classes - \\ <p>Note: For different networks, the configurable parameters of the backbone/neck/head module will be different. The specific configurable parameters are determined by the init input parameter of the class specified by the <code>name</code> parameter of the module in the above table (For example, assume you specify the neck module is DBFPN. Since the DBFPN class initialization includes adaptive input parameters, parameters such as adaptive can be configured under the model.head in yaml.)</p> <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#4-postprocessing-postprocess","title":"4. Postprocessing (postprocess)","text":"<p>Please see the code in mindocr/postprocess</p> Parameter Description Example Remarks name Post-processing class name - Currently supports DBPostprocess, EASTPostprocess, PSEPostprocess, RecCTCLabelDecode and RecAttnLabelDecode character_dict_path Recognition dictionary path None If None, then use the default dictionary [0-9a-z] use_space_char Set whether to add spaces to the dictionary False True/False <p>Note: For different post-processing methods (specified by name), the configurable parameters are different, and are determined by the input parameters of the initialization method <code>__init__</code> of the post-processing class.</p> <p>Reference example: DBNet, PSENet</p>"},{"location":"tutorials/yaml_configuration/#5-evaluation-metrics-metric","title":"5. Evaluation metrics (metric)","text":"<p>Please see the code in mindocr/metrics</p> Parameter Description Default Remarks name Metric class name - Currently supports RecMetric, DetMetric main_indicator Main indicator, used for comparison of optimal models 'hmean' 'acc' for recognition tasks, 'f-score' for detection tasks character_dict_path Recognition dictionary path None If None, then use the default dictionary \"0123456789abcdefghijklmnopqrstuvwxyz\" ignore_space Whether to filter spaces True True/False print_flag Whether to print log False If set True, then output information such as prediction results and standard answers"},{"location":"tutorials/yaml_configuration/#6-loss-function-loss","title":"6. Loss function (loss)","text":"<p>Please see the code in mindocr/losses</p> Parameter Description Default Remarks name loss function name - Currently supports DBLoss, CTCLoss, AttentionLoss, PSEDiceLoss, EASTLoss and CrossEntropySmooth pred_seq_len length of predicted text 26 Determined by network architecture max_label_len The longest label length 25 The value is less than the length of the text predicted by the network batch_size single card batch size 32 \\ <p>Note: For different loss functions (specified by name), the configurable parameters are different and determined by the input parameters of the selected loss function.</p>"},{"location":"tutorials/yaml_configuration/#7-learning-rate-adjustment-strategy-and-optimizer-scheduler-optimizer-loss_scaler","title":"7. Learning rate adjustment strategy and optimizer (scheduler, optimizer, loss_scaler)","text":""},{"location":"tutorials/yaml_configuration/#learning-rate-adjustment-strategy-scheduler","title":"Learning rate adjustment strategy (scheduler)","text":"<p>Please see the code in mindocr/scheduler</p> Parameter Description Default Remarks scheduler Learning rate scheduler name 'constant' Currently supports 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay' min_lr Minimum learning rate 1e-6 Lower lr bound for 'cosine_decay' schedulers. lr Learning rate value 0.01 num_epochs Number of total epochs 200 The number of total epochs for the entire training. warmup_epochs The number of epochs in the training learning rate warmp phase 3 For 'cosine_decay', 'warmup_epochs' indicates the epochs to warmup learning rate from 0 to <code>lr</code>. decay_epochs The number of epochs in the training learning rate decay phase 10 For 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>."},{"location":"tutorials/yaml_configuration/#optimizer","title":"optimizer","text":"<p>Please see the code location: mindocr/optim</p> Parameter Description Default Remarks opt Optimizer name 'adam' Currently supports 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'nadam', 'adan', 'rmsprop', 'adagrad', 'lamb'. filter_bias_and_bn Set whether to exclude the weight decrement of bias and batch norm True If True, weight decay will not apply on BN parameters and bias in Conv or Dense layers. momentum momentum 0.9 \\ weight_decay weight decay rate 0 It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. nesterov Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. False True/False"},{"location":"tutorials/yaml_configuration/#loss-scaling-loss_scaler","title":"Loss scaling (loss_scaler)","text":"Parameter Description Default Remarks type Loss scaling method type static Currently supports static, dynamic loss_scale Loss scaling value 1.0 \\ scale_factor When using dynamic loss scaler, the coefficient to dynamically adjust the loss_scale 2.0 At each training step, the loss scaling value is updated to <code>loss_scale</code>/<code>scale_factor</code> when overflow occurs. scale_window When using the dynamic loss scaler, when there is no overflow after the scale_window training step, enlarge the loss_scale by scale_factor times 1000 If the continuous <code>scale_window</code> steps does not overflow, the loss will be increased by <code>loss_scale</code> * <code>scale_factor</code> to update the scaling number"},{"location":"tutorials/yaml_configuration/#8-training-evaluation-and-predict-process-train-eval-predict","title":"8. Training, evaluation and predict process (train, eval, predict)","text":"<p>The configuration of the training process is placed under <code>train</code>, and the configuration of the evaluation phase is placed under <code>eval</code>. Note that during model training, if the training-while-evaluation mode is turned on, that is, when val_while_train=True, an evaluation will be run according to the configuration under <code>eval</code> after each epoch is trained. During the non-training phase, only the <code>eval</code> configuration is read when only running model evaluation.</p>"},{"location":"tutorials/yaml_configuration/#training-process-train","title":"Training process (train)","text":"Parameter Description Default Remarks ckpt_save_dir Set model save path ./tmp_rec \\ resume Resume training after training is interrupted, you can set True/False, or specify the ckpt path that needs to be loaded to resume training False If True, load resume_train.ckpt under the ckpt_save_dir directory to continue training. You can also specify the ckpt file path to load and resume training. dataset_sink_mode Whether the data is directly sinked to the processor for processing - If set to True, the data sinks to the processor, and the data can be returned at least after the end of each epoch gradient_accumulation_steps Number of steps to accumulate the gradients 1 Each step represents a forward calculation, and a reverse correction is performed after the gradient accumulation is completed. clip_grad Whether to clip the gradient False If set to True, gradients are clipped to <code>clip_norm</code> clip_norm The norm of clipping gradient if set clip_grad as True 1 \\ ema Whether to use EMA algorithm False \\ ema_decay EMA decay rate 0.9999 \\ pred_cast_fp32 Whether to cast the data type of logits to fp32 False \\ dataset Dataset configuration For details, please refer to Data document type Dataset class name - Currently supports LMDBDataset, RecDataset and DetDataset dataset_root The root directory of the dataset None Optional data_dir The subdirectory where the dataset is located - If <code>dataset_root</code> is not set, please set this to the full directory label_file The label file path of the dataset - If <code>dataset_root</code> is not set, please set this to the full path, otherwise just set the subpath sample_ratio Data set sampling ratio 1.0 If value &lt; 1.0, random selection shuffle Whether to shuffle the data order True if undering training, otherwise False True/False transform_pipeline Data processing flow None For details, please see transforms output_columns Data loader (data loader) needs to output a list of data attribute names (given to the network/loss calculation/post-processing) (type: list), and the candidate data attribute names are determined by transform_pipeline. None If the value is None, all columns are output. Take crnn as an example, output_columns: ['image', 'text_seq'] net_input_column_index In output_columns, the indices of the input items required by the network construct function [0] \\ label_column_index In output_columns, the indices of the input items required by the loss function [1] \\ loader Data Loading Settings shuffle Whether to shuffle the data order for each epoch True if undering training, otherwise False True/False batch_size Batch size of a single card - \\ drop_remainder Whether to drop the last batch of data when the total data cannot be divided by batch_size True if undering training, otherwise False \\ max_rowsize Specifies the maximum space allocated by shared memory when copying data between multiple processes 64 Default value: 64 num_workers Specifies the number of concurrent processes/threads for batch operations n_cpus / n_devices - 2 This value should be greater than or equal to 2 <p>Reference example: DBNet, CRNN</p>"},{"location":"tutorials/yaml_configuration/#evaluation-process-eval","title":"Evaluation process (eval)","text":"<p>The parameters of <code>eval</code> are basically the same as <code>train</code>, only a few additional parameters are added, and for the rest, please refer to the parameter description of <code>train</code> above.</p> Parameter Usage Default Remarks ckpt_load_path Set model loading path - \\ num_columns_of_labels Set the number of labels in the dataset output columns None If None, assuming the columns after image (data[1:]) are labels. If not None, the num_columns_of_labels columns after image (data[1:1+num_columns_of_labels]) are labels, and the remaining columns are additional info like image_path. drop_remainder Whether to discard the last batch of data when the total number of data cannot be divided by batch_size True if undering training, otherwise False It is recommended to set it to False when doing model evaluation. If it cannot be divisible, mindocr will automatically select a batch size that is the largest divisible"},{"location":"en/#dataset-list","title":"Dataset List","text":"<p>MindOCR provides a dataset conversion tool to OCR datasets with different formats and support customized dataset by users. We have validated the following public OCR datasets in model training/evaluation.</p> General OCR Datasets <ul> <li>Born-Digital Images [download]</li> <li>CASIA-10K [download]</li> <li>CCPD [download]</li> <li>Chinese Text Recognition Benchmark [paper] [download]</li> <li>COCO-Text [download]</li> <li>CTW [download]</li> <li>ICDAR2015 [paper] [download]</li> <li>ICDAR2019 ArT [download]</li> <li>LSVT [download]</li> <li>MLT2017 [paper] [download]</li> <li>MSRA-TD500 [paper] [download]</li> <li>MTWI-2018 [download]</li> <li>RCTW-17 [download]</li> <li>ReCTS [download]</li> <li>SCUT-CTW1500 [paper] [download]</li> <li>SROIE [download]</li> <li>SVT [download]</li> <li>SynText150k [paper] [download]</li> <li>SynthText [paper] [download]</li> <li>TextOCR [download]</li> <li>Total-Text [paper] [download]</li> </ul> Layout Analysis Datasets <ul> <li>PublayNet [paper] [download]</li> </ul> Key Information Extraction Datasets <ul> <li>XFUND [paper] [download]</li> </ul> Table Recognition Datasets <ul> <li>PubTabNet [paper] [download]</li> </ul> <p>We will include more datasets for training and evaluation. This list will be continuously updated.</p>"},{"location":"cn/","title":"\u4e3b\u9875","text":""},{"location":"cn/#mindocr","title":"MindOCR","text":"<p>English | \u4e2d\u6587</p> <p>\ud83d\udcdd\u7b80\u4ecb | \ud83d\udd28\u5b89\u88c5\u6559\u7a0b | \ud83d\ude80\u5feb\u901f\u5f00\u59cb | \ud83d\udcda\u4f7f\u7528\u6559\u7a0b | \ud83c\udf81\u6a21\u578b\u5217\u8868 | \ud83d\udcf0\u6570\u636e\u96c6\u5217\u8868 | \ud83d\udcd6\u5e38\u89c1\u95ee\u9898 | \ud83c\udf89\u66f4\u65b0\u65e5\u5fd7</p>"},{"location":"cn/#_1","title":"\u7b80\u4ecb","text":"<p>MindOCR\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore \u6846\u67b6\u5f00\u53d1\u7684OCR\u5f00\u6e90\u5de5\u5177\u7bb1\uff0c\u96c6\u6210\u7cfb\u5217\u4e3b\u6d41\u6587\u5b57\u68c0\u6d4b\u8bc6\u522b\u7684\u7b97\u6cd5\u3001\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u6613\u7528\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5f00\u53d1\u548c\u5e94\u7528\u4e1a\u754cSoTA\u6587\u672c\u68c0\u6d4b\u3001\u6587\u672c\u8bc6\u522b\u6a21\u578b\uff0c\u5982DBNet/DBNet++\u548cCRNN/SVTR\uff0c\u6ee1\u8db3\u56fe\u50cf\u6587\u6863\u7406\u89e3\u7684\u9700\u6c42\u3002</p>  \u4e3b\u8981\u7279\u6027  <ul> <li>\u6a21\u5757\u5316\u8bbe\u8ba1: MindOCR\u5c06OCR\u4efb\u52a1\u89e3\u8026\u6210\u591a\u4e2a\u53ef\u914d\u7f6e\u6a21\u5757\uff0c\u7528\u6237\u53ea\u9700\u4fee\u6539\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u5b9a\u5236\u5316\u7684\u6570\u636e\u548c\u6a21\u578b\u4e0a\u914d\u7f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u7684\u5168\u6d41\u7a0b\uff1b</li> <li>\u9ad8\u6027\u80fd: MindOCR\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u5176\u8fbe\u5230OCR\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\uff1b</li> <li>\u6613\u7528\u6027: MindOCR\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u5e2e\u52a9\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u8fdb\u884c\u6587\u672c\u7684\u68c0\u6d4b\u548c\u8bc6\u522b\u3002</li> </ul>"},{"location":"cn/#_2","title":"\u5b89\u88c5\u6559\u7a0b","text":""},{"location":"cn/#mindspore","title":"MindSpore\u76f8\u5173\u73af\u5883\u51c6\u5907","text":"<p>MindOCR\u57fa\u4e8eMindSpore AI\u6846\u67b6\uff08\u652f\u6301CPU/GPU/NPU\uff09\u5f00\u53d1\uff0c\u5e76\u9002\u914d\u4ee5\u4e0b\u6846\u67b6\u7248\u672c\u3002\u5b89\u88c5\u65b9\u5f0f\u8bf7\u53c2\u89c1\u4e0b\u65b9\u7684\u5b89\u88c5\u94fe\u63a5\u3002</p> <ul> <li>mindspore &gt;= 2.2.0 [\u5b89\u88c5]</li> <li>python &gt;= 3.7</li> <li>openmpi 4.0.3 (\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u9a8c\u8bc1)  [\u5b89\u88c5]</li> <li>mindspore lite (\u7528\u4e8e\u79bb\u7ebf\u63a8\u7406) &gt;= 2.2.0  [\u5b89\u88c5]</li> </ul>"},{"location":"cn/#_3","title":"\u5305\u4f9d\u8d56","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"cn/#_4","title":"\u901a\u8fc7\u6e90\u6587\u4ef6\u5b89\u88c5\uff08\u63a8\u8350\uff09","text":"<pre><code>git clone https://github.com/mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>\u4f7f\u7528 <code>-e</code> \u4ee3\u8868\u53ef\u7f16\u8f91\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5e2e\u52a9\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p>"},{"location":"cn/#pypi","title":"\u901a\u8fc7PyPI\u5b89\u88c5","text":"<pre><code>pip install mindocr\n</code></pre> <p>\u7531\u4e8e\u6b64\u9879\u76ee\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u4e2d\uff0c\u4ecePyPI\u5b89\u88c5\u7684\u7248\u672c\u76ee\u524d\u5df2\u8fc7\u671f\uff0c\u6211\u4eec\u5c06\u5f88\u5feb\u66f4\u65b0\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"cn/#_5","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"cn/#1","title":"1. \u6587\u5b57\u68c0\u6d4b\u548c\u8bc6\u522b\u793a\u4f8b","text":"<p>\u5b89\u88c5\u5b8cMindOCR\u540e\uff0c\u6211\u4eec\u5c31\u5f88\u65b9\u4fbf\u5730\u8fdb\u884c\u4efb\u610f\u56fe\u50cf\u7684\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\uff0c\u5982\u4e0b\u3002</p> <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN\n</code></pre> <p>\u8fd0\u884c\u7ed3\u675f\u540e\uff0c\u7ed3\u679c\u5c06\u88ab\u9ed8\u8ba4\u4fdd\u5b58\u5728<code>./inference_results</code>\u8def\u5f84\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b\uff1a</p> <p> </p> <p>  \u6587\u672c\u68c0\u6d4b\u3001\u8bc6\u522b\u7ed3\u679c\u53ef\u89c6\u5316  </p> <p>\u53ef\u4ee5\u770b\u5230\u56fe\u50cf\u4e2d\u7684\u6587\u5b57\u5757\u5747\u88ab\u68c0\u6d4b\u51fa\u6765\u5e76\u6b63\u786e\u8bc6\u522b\u3002\u66f4\u8be6\u7ec6\u7684\u7528\u6cd5\u4ecb\u7ecd\uff0c\u8bf7\u53c2\u8003\u63a8\u7406\u6559\u7a0b\u3002</p>"},{"location":"cn/#2-","title":"2. \u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4e0e\u63a8\u7406-\u5feb\u901f\u6307\u5357","text":"<p>\u4f7f\u7528<code>tools/train.py</code>\u811a\u672c\u53ef\u4ee5\u8fdb\u884cOCR\u6a21\u578b\u8bad\u7ec3\uff0c\u8be5\u811a\u672c\u53ef\u652f\u6301\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\u6a21\u578b\u8bad\u7ec3\u3002 <pre><code>python tools/train.py --config {path/to/model_config.yaml}\n</code></pre> <code>--config</code> \u53c2\u6570\u7528\u4e8e\u6307\u5b9ayaml\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8be5\u6587\u4ef6\u5b9a\u4e49\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u4f18\u5316\u5668\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7b49\u3002</p> <p>MindOCR\u5728<code>configs</code>\u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u7cfb\u5217SoTA\u7684OCR\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u6237\u53ef\u4ee5\u5feb\u901f\u5c06\u5176\u9002\u914d\u5230\u81ea\u5df1\u7684\u4efb\u52a1\u6216\u6570\u636e\u96c6\u4e0a\uff0c\u53c2\u8003\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># train text detection model DBNet++ on icdar15 dataset\npython tools/train.py --config configs/det/dbnet/dbpp_r50_icdar15.yaml\n</code></pre> <pre><code># train text recognition model CRNN on icdar15 dataset\npython tools/train.py --config configs/rec/crnn/crnn_icdar15.yaml\n</code></pre></p> <p>\u4f7f\u7528<code>tools/eval.py</code> \u811a\u672c\u53ef\u4ee5\u8bc4\u4f30\u5df2\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>python tools/eval.py \\\n    --config {path/to/model_config.yaml} \\\n    --opt eval.dataset_root={path/to/your_dataset} eval.ckpt_load_path={path/to/ckpt_file}\n</code></pre></p> <p>\u4f7f\u7528<code>tools/infer/text/predict_system.py</code> \u811a\u672c\u53ef\u8fdb\u884c\u6a21\u578b\u7684\u5728\u7ebf\u63a8\u7406\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>python tools/infer/text/predict_system.py --image_dir {path_to_img or dir_to_imgs} \\\n                                          --det_algorithm DB++  \\\n                                          --rec_algorithm CRNN\n</code></pre></p> <p>\u66f4\u591a\u4f7f\u7528\u65b9\u6cd5\uff0c\u8bf7\u53c2\u8003\u4f7f\u7528\u6559\u7a0b\u4e2d\u7684\u6a21\u578b\u8bad\u7ec3\u3001\u63a8\u7406\u7ae0\u8282\u3002</p>"},{"location":"cn/#3-","title":"3. \u6a21\u578b\u79bb\u7ebf\u63a8\u7406-\u5feb\u901f\u6307\u5357","text":"<p>\u4f60\u53ef\u4ee5\u5728MindOCR\u4e2d\u5bf9**MindOCR\u539f\u751f\u6a21\u578b**\u6216**\u7b2c\u4e09\u65b9\u6a21\u578b**\uff08\u5982PaddleOCR\u3001MMOCR\u7b49\uff09\u8fdb\u884cMindSpore Lite\u63a8\u7406\u3002\u8bf7\u53c2\u8003\u4ee5\u4e0b\u6587\u6863  - \u57fa\u4e8ePython/C++\u548c\u6607\u817e310\u7684OCR\u63a8\u7406  - MindOCR\u539f\u751f\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb  - \u7b2c\u4e09\u65b9\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb</p>"},{"location":"cn/#_6","title":"\u4f7f\u7528\u6559\u7a0b","text":"<ul> <li>\u6570\u636e\u96c6<ul> <li>\u6570\u636e\u96c6\u51c6\u5907</li> <li>\u6570\u636e\u589e\u5f3a\u7b56\u7565</li> </ul> </li> <li>\u6a21\u578b\u8bad\u7ec3<ul> <li>Yaml\u914d\u7f6e\u6587\u4ef6</li> <li>\u6587\u672c\u68c0\u6d4b</li> <li>\u6587\u672c\u8bc6\u522b</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u8fdb\u9636\u6280\u5de7\uff1a\u68af\u5ea6\u7d2f\u79ef\uff0cEMA\uff0c\u65ad\u70b9\u7eed\u8bad\u7b49</li> </ul> </li> <li>\u4f7f\u7528MindSpore\u8fdb\u884c\u5728\u7ebf\u63a8\u7406<ul> <li>\u57fa\u4e8ePython\u7684OCR\u5728\u7ebf\u63a8\u7406</li> </ul> </li> <li>\u4f7f\u7528MindSpore Lite\u8fdb\u884c\u79bb\u7ebf\u63a8\u7406<ul> <li>\u57fa\u4e8ePython/C++\u548c\u6607\u817e310\u7684OCR\u63a8\u7406</li> <li>MindOCR\u539f\u751f\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb</li> <li>\u7b2c\u4e09\u65b9\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb</li> </ul> </li> <li>\u5f00\u53d1\u8005\u6307\u5357<ul> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u96c6</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u65b9\u6cd5</li> <li>\u5982\u4f55\u521b\u5efa\u65b0\u7684OCR\u6a21\u578b</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u540e\u5904\u7406\u65b9\u6cd5</li> </ul> </li> </ul>"},{"location":"cn/#_7","title":"\u6a21\u578b\u5217\u8868","text":"\u6587\u672c\u68c0\u6d4b <ul> <li> DBNet (AAAI'2020)</li> <li> DBNet++ (TPAMI'2022)</li> <li> PSENet (CVPR'2019)</li> <li> EAST(CVPR'2017)</li> <li> FCENet (CVPR'2021)</li> </ul> \u6587\u672c\u8bc6\u522b <ul> <li> CRNN (TPAMI'2016)</li> <li> CRNN-Seq2Seq/RARE (CVPR'2016)</li> <li> SVTR (IJCAI'2022)</li> <li> MASTER (PR'2019)</li> <li> VISIONLAN (ICCV'2021)</li> <li> RobustScanner (ECCV'2020)</li> <li> ABINet (CVPR'2021)</li> </ul> \u7248\u9762\u5206\u6790 <ul> <li> YOLOv8 (Ultralytics Inc.)</li> </ul> \u5173\u952e\u4fe1\u606f\u62bd\u53d6 <ul> <li> LayoutXLM SER (arXiv'2016)</li> </ul> \u8868\u683c\u8bc6\u522b <ul> <li> TableMaster (arXiv'2021)</li> </ul> <p>\u5173\u4e8e\u4ee5\u4e0a\u6a21\u578b\u7684\u5177\u4f53\u8bad\u7ec3\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u8bf7\u53c2\u89c1configs\u4e0b\u5404\u6a21\u578b\u5b50\u76ee\u5f55\u7684readme\u6587\u6863\u3002</p> <p>\u5173\u4e8eMindSpore Lite\u548cACL\u6a21\u578b\u63a8\u7406\u7684\u652f\u6301\u5217\u8868\uff0c \u8bf7\u53c2\u89c1MindOCR\u539f\u751f\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868 \u548c \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u652f\u6301\u5217\u8868\uff08\u5982PaddleOCR\u3001MMOCR\u7b49\uff09\u3002</p>"},{"location":"cn/#_8","title":"\u6570\u636e\u96c6\u5217\u8868","text":"<p>MindOCR\u63d0\u4f9b\u4e86\u6570\u636e\u683c\u5f0f\u8f6c\u6362\u5de5\u5177 \uff0c\u4ee5\u652f\u6301\u4e0d\u540c\u683c\u5f0f\u7684OCR\u6570\u636e\u96c6\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002 \u5f53\u524d\u5df2\u5728\u6a21\u578b\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u8fc7\u7684\u516c\u5f00OCR\u6570\u636e\u96c6\u5982\u4e0b\u3002</p> \u901a\u7528OCR\u6570\u636e\u96c6 <ul> <li>Born-Digital Images [download]</li> <li>CASIA-10K [download]</li> <li>CCPD [download]</li> <li>Chinese Text Recognition Benchmark [paper] [download]</li> <li>COCO-Text [download]</li> <li>CTW [download]</li> <li>ICDAR2015 [paper] [download]</li> <li>ICDAR2019 ArT [download]</li> <li>LSVT [download]</li> <li>MLT2017 [paper] [download]</li> <li>MSRA-TD500 [paper] [download]</li> <li>MTWI-2018 [download]</li> <li>RCTW-17 [download]</li> <li>ReCTS [download]</li> <li>SCUT-CTW1500 [paper] [download]</li> <li>SROIE [download]</li> <li>SVT [download]</li> <li>SynText150k [paper] [download]</li> <li>SynthText [paper] [download]</li> <li>TextOCR [download]</li> <li>Total-Text [paper] [download]</li> </ul> \u7248\u9762\u5206\u6790\u6570\u636e\u96c6 <ul> <li>PublayNet [paper] [download]</li> </ul> \u5173\u952e\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u96c6 <ul> <li>XFUND [paper] [download]</li> </ul> \u8868\u683c\u8bc6\u522b\u6570\u636e\u96c6 <ul> <li>PubTabNet [paper] [download]</li> </ul> <p>\u6211\u4eec\u4f1a\u5728\u66f4\u591a\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u8be5\u5217\u8868\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p>"},{"location":"cn/#_9","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u5173\u4e8e\u914d\u7f6e\u73af\u5883\u3001\u4f7f\u7528mindocr\u9047\u5230\u7684\u9ad8\u9891\u95ee\u9898\uff0c\u53ef\u4ee5\u53c2\u8003\u5e38\u89c1\u95ee\u9898\u3002</p>"},{"location":"cn/#_10","title":"\u91cd\u8981\u4fe1\u606f","text":""},{"location":"cn/#_11","title":"\u66f4\u65b0\u65e5\u5fd7","text":"\u8be6\u7ec6 <ul> <li> <p>2023/12/25 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u8868\u683c\u8bc6\u522bTableMaster 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>PubTabNet</li> </ul> </li> <li> <p>2023/12/14 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u5173\u952e\u4fe1\u606f\u62bd\u53d6LayoutXLM SER</li> <li>\u5173\u952e\u4fe1\u606f\u62bd\u53d6VI-LayoutXLM SER</li> <li>\u6587\u672c\u68c0\u6d4bPP-OCRv3 DBNet\u548c\u6587\u672c\u8bc6\u522bPP-OCRv3 SVTR\uff0c\u652f\u6301\u5728\u7ebf\u63a8\u7406\u548c\u5fae\u8c03\u8bad\u7ec3 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>XFUND 3. \u6607\u817e910\u786c\u4ef6\u591a\u89c4\u683c\u652f\u6301\uff1aDBNet ResNet-50\u3001DBNet++ ResNet-50\u3001CRNN VGG7\u3001SVTR-Tiny\u3001FCENet\u3001ABINet</li> </ul> </li> <li>2023/11/28 1. \u589e\u52a0\u652f\u6301PP-OCRv4\u6a21\u578b\u79bb\u7ebf\u63a8\u7406<ul> <li>\u6587\u672c\u68c0\u6d4b PP-OCRv4 DBNet\u548c\u6587\u672c\u8bc6\u522b PP-OCRv4 CRNN\uff0c\u652f\u6301\u79bb\u7ebf\u63a8\u7406 2. \u4fee\u590d\u7b2c\u4e09\u65b9\u6a21\u578b\u79bb\u7ebf\u63a8\u7406bug</li> </ul> </li> <li>2023/11/17 1. \u589e\u52a0\u65b0\u6a21\u578b<ul> <li>\u7248\u9762\u5206\u6790YOLOv8 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>PublayNet</li> </ul> </li> <li>2023/07/06 1. \u589e\u52a0\u65b0\u6a21\u578b<ul> <li>\u6587\u672c\u8bc6\u522b RobustScanner</li> </ul> </li> <li>2023/07/05 1. \u589e\u52a0\u65b0\u6a21\u578b<ul> <li>\u6587\u672c\u8bc6\u522b VISIONLAN</li> </ul> </li> <li>2023/06/29 1. \u65b0\u589e2\u4e2aSoTA\u6a21\u578b<ul> <li>\u6587\u672c\u68c0\u6d4b FCENet</li> <li>\u6587\u672c\u8bc6\u522b MASTER</li> </ul> </li> <li> <p>2023/06/07 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4b PSENet</li> <li>\u6587\u672c\u68c0\u6d4b EAST</li> <li>\u6587\u672c\u8bc6\u522b SVTR 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>totaltext</li> <li>mlt2017</li> <li>chinese_text_recognition 3. \u589e\u52a0\u65ad\u70b9\u7eed\u8bad(resume training)\u529f\u80fd\uff0c\u53ef\u5728\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\u4f7f\u7528\u3002\u5982\u9700\u4f7f\u7528\uff0c\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d<code>model</code>\u5b57\u6bb5\u4e0b\u589e\u52a0<code>resume</code>\u53c2\u6570\uff0c\u5141\u8bb8\u4f20\u5165\u5177\u4f53\u8def\u5f84<code>resume: /path/to/train_resume.ckpt</code>\u6216\u8005\u901a\u8fc7\u8bbe\u7f6e<code>resume: True</code>\u6765\u52a0\u8f7d\u5728ckpt_save_dir\u4e0b\u4fdd\u5b58\u7684trian_resume.ckpt 4. \u6539\u8fdb\u68c0\u6d4b\u6a21\u5757\u7684\u540e\u5904\u7406\u90e8\u5206\uff1a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5c06\u68c0\u6d4b\u5230\u7684\u6587\u672c\u591a\u8fb9\u5f62\u91cd\u65b0\u7f29\u653e\u5230\u539f\u59cb\u56fe\u50cf\u7a7a\u95f4\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728<code>eval.dataset.output_columns</code>\u5217\u8868\u4e2d\u589e\u52a0\"shape_list\"\u5b9e\u73b0\u3002 5. \u91cd\u6784\u5728\u7ebf\u63a8\u7406\u4ee5\u652f\u6301\u66f4\u591a\u6a21\u578b\uff0c\u8be6\u60c5\u8bf7\u53c2\u89c1README.md \u3002</li> </ul> </li> <li> <p>2023/05/15 1. \u589e\u52a0\u65b0\u6a21\u578b</p> <ul> <li>\u6587\u672c\u68c0\u6d4b DBNet++</li> <li>\u6587\u672c\u8bc6\u522b CRNN-Seq2Seq</li> <li>\u5728SynthText\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684DBNet 2. \u6dfb\u52a0\u66f4\u591a\u57fa\u51c6\u6570\u636e\u96c6\u53ca\u5176\u7ed3\u679c</li> <li>SynthText, MSRA-TD500, CTW1500</li> <li>DBNet\u7684\u66f4\u591a\u57fa\u51c6\u7ed3\u679c\u53ef\u4ee5\u5728\u6b64\u627e\u5230. 3. \u6dfb\u52a0\u7528\u4e8e\u4fdd\u5b58\u524dk\u4e2acheckpoint\u7684checkpoint manager\u5e76\u6539\u8fdb\u65e5\u5fd7\u3002 4. Python\u63a8\u7406\u4ee3\u7801\u91cd\u6784\u3002 5. Bug\u4fee\u590d\uff1a\u5bf9\u5927\u578b\u6570\u636e\u96c6\u4f7f\u7528\u5e73\u5747\u635f\u5931meter\uff0c\u5728AMP\u8bad\u7ec3\u4e2d\u5bf9ctcloss\u7981\u7528<code>pred_cast_fp32</code>\uff0c\u4fee\u590d\u5b58\u5728\u65e0\u6548\u591a\u8fb9\u5f62\u7684\u9519\u8bef\u3002</li> </ul> </li> <li> <p>2023/05/04 1. \u652f\u6301\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3checkpoint\uff0c \u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>model-pretrained</code>\u8bbe\u7f6e\u4e3acheckpoint url\u6216\u672c\u5730\u8def\u5f84\u6765\u4f7f\u7528\u3002 2. \u652f\u6301\u8bbe\u7f6e\u6267\u884c\u5305\u62ec\u65cb\u8f6c\u548c\u7ffb\u8f6c\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u6982\u7387\u3002 3. \u4e3a\u6a21\u578b\u8bad\u7ec3\u6dfb\u52a0EMA\u529f\u80fd\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train-ema</code>\uff08\u9ed8\u8ba4\u503c\uff1aFalse\uff09\u548c<code>train-ema_decay</code>\u6765\u542f\u7528\u3002 4. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_to_net</code> -&gt; <code>net_input_column_index</code>: \u8f93\u5165\u7f51\u7edc\u7684columns\u6570\u91cf\u6539\u4e3a\u8f93\u5165\u7f51\u7edc\u7684columns\u7d22\u5f15 5. \u53c2\u6570\u4fee\u6539\uff1a<code>num_columns_of_labels</code> -&gt; <code>label_column_index</code>: \u7528\u7d22\u5f15\u66ff\u6362\u6570\u91cf\uff0c\u4ee5\u8868\u793alabel\u7684\u4f4d\u7f6e\u3002</p> </li> <li> <p>2023/04/21 1. \u6dfb\u52a0\u53c2\u6570\u5206\u7ec4\u4ee5\u652f\u6301\u8bad\u7ec3\u4e2d\u7684\u6b63\u5219\u5316\u3002\u7528\u6cd5\uff1a\u5728yaml config\u4e2d\u6dfb\u52a0<code>grouping_strategy</code>\u53c2\u6570\u4ee5\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u6216\u4f7f\u7528<code>no_weight_decay_params</code>\u53c2\u6570\u9009\u62e9\u8981\u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u7684\u5c42\uff08\u4f8b\u5982\uff0cbias\u3001norm\uff09\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 2. \u6dfb\u52a0\u68af\u5ea6\u7d2f\u79ef\uff0c\u652f\u6301\u5927\u6279\u91cf\u8bad\u7ec3\u3002\u7528\u6cd5\uff1a\u5728yaml\u914d\u7f6e\u4e2d\u6dfb\u52a0<code>gradient_accumulation_steps</code>\uff0c\u5168\u5c40\u6279\u91cf\u5927\u5c0f=batch_size * devices * gradient_aaccumulation_steps\u3002\u793a\u4f8b\u53ef\u53c2\u8003<code>configs/rec/crn/crnn_icdar15.yaml</code> 3. \u6dfb\u52a0\u68af\u5ea6\u88c1\u526a\uff0c\u652f\u6301\u8bad\u7ec3\u7a33\u5b9a\u3002\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>grad_clip</code>\u8bbe\u7f6e\u4e3aTrue\u6765\u542f\u7528\u3002</p> </li> <li> <p>2023/03/23 1. \u589e\u52a0dynamic loss scaler\u652f\u6301, \u4e14\u4e0edrop overflow update\u517c\u5bb9\u3002\u5982\u9700\u4f7f\u7528, \u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0<code>loss_scale</code>\u5b57\u6bb5\u5e76\u5c06<code>type</code>\u53c2\u6570\u8bbe\u4e3a<code>dynamic</code>\uff0c\u53c2\u8003\u4f8b\u5b50\u8bf7\u89c1<code>configs/rec/crnn/crnn_icdar15.yaml</code></p> </li> <li> <p>2023/03/20 1. \u53c2\u6570\u540d\u4fee\u6539\uff1a<code>output_keys</code> -&gt; <code>output_columns</code>\uff1b<code>num_keys_to_net</code> -&gt; <code>num_columns_to_net</code>\uff1b 2. \u66f4\u65b0\u6570\u636e\u6d41\u7a0b\u3002</p> </li> <li> <p>2023/03/13 1. \u589e\u52a0\u7cfb\u7edf\u6d4b\u8bd5\u548cCI\u5de5\u4f5c\u6d41\uff1b 2. \u589e\u52a0modelarts\u5e73\u53f0\u9002\u914d\u5668\uff0c\u4f7f\u5f97\u652f\u6301\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\uff0c\u5728OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a   <pre><code>  i)   \u5728OpenI\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u4e00\u4e2a\u8bad\u7ec3\u4efb\u52a1\uff1b\n  ii)  \u5728\u7f51\u9875\u4e0a\u5173\u8054\u6570\u636e\u96c6\uff0c\u5982ic15_mindocr\uff1b\n  iii) \u589e\u52a0 `config` \u53c2\u6570\uff0c\u5728\u7f51\u9875\u7684UI\u754c\u9762\u914d\u7f6eyaml\u6587\u4ef6\u8def\u5f84\uff0c\u5982'/home/work/user-job-dir/V0001/configs/rec/test.yaml'\uff1b\n  iv)  \u5728\u7f51\u9875\u7684UI\u754c\u9762\u589e\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u5c06\u5176\u8bbe\u7f6e\u4e3aTrue\uff1b\n  v)   \u586b\u5199\u5176\u4ed6\u9879\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre></p> </li> </ul>"},{"location":"cn/#_12","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u5305\u62ec\u95ee\u9898\u5355\u548cPR\u5728\u5185\u7684\u6240\u6709\u8d21\u732e\uff0c\u6765\u8ba9MindOCR\u53d8\u5f97\u66f4\u597d\u3002</p> <p>\u8bf7\u53c2\u8003CONTRIBUTING.md\u4f5c\u4e3a\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u6309\u7167Model Template and Guideline\u7684\u6307\u5f15\u8d21\u732e\u4e00\u4e2a\u9002\u914d\u6240\u6709\u63a5\u53e3\u7684\u6a21\u578b\uff0c\u591a\u8c22\u5408\u4f5c\u3002</p>"},{"location":"cn/#_13","title":"\u8bb8\u53ef","text":"<p>\u672c\u9879\u76ee\u9075\u4eceApache License 2.0\u5f00\u6e90\u8bb8\u53ef\u3002</p>"},{"location":"cn/#_14","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u672c\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore OCR 2023,\n    title={{MindSpore OCR }:MindSpore OCR Toolbox},\n    author={MindSpore Team},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindocr/}},\n    year={2023}\n}\n</code></pre>"},{"location":"cn/datasets/borndigital/","title":"Born-Digital Images \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/borndigital/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>\u539f\u751f\u6570\u5b57\u56fe\u50cf\u6570\u636e\u96c6\uff08Born-Digital Images\uff09\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u8be5\u6570\u636e\u96c6\u5206\u4e3a4\u4e2a\u4efb\u52a1: \u4efb\u52a11\u4e3a\u6587\u672c\u5b9a\u4f4d, \u4efb\u52a12\u4e3a\u6587\u672c\u5206\u5272, \u4efb\u52a13\u4e3a\u5355\u8bcd\u8bc6\u522b, \u4efb\u52a14\u4e3a\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u3002\u8fd9\u91cc\u6211\u4eec\u4ec5\u8003\u8651\u4e0b\u8f7d\u4f7f\u7528\u4efb\u52a11\u6570\u636e\u96c6\u3002</p> <p>\u4e0b\u8f7d\u56fe\u50cf\u548c\u6ce8\u91ca\u540e\uff0c\u89e3\u538b\u7f29\u6587\u4ef6\u5e76\u6839\u636e\u9700\u8981\u91cd\u547d\u540d\uff0c\u4f8b\u5982<code>train_images</code>\u662f\u56fe\u50cf\uff0c<code>train_labels</code> \u662f\u6807\u7b7e, \u6700\u7ec8\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a <pre><code>Born-Digital\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_labels\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"cn/datasets/borndigital/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/borndigital/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name borndigital --task det \\\n    --image_dir path/to/Born-Digital/train_images/ \\\n    --label_dir path/to/Born-Digital/train_labels \\\n    --output_path path/to/Born-Digital/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>Born-Digital/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/casia10k/","title":"CASIA-10K \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/casia10k/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>CASIA-10K \u6570\u636e\u96c6\u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>CASIA-10K\n  |--- test\n  |    |--- PAL00001.jpg\n  |    |--- PAL00001.txt\n  |    |--- PAL00005.jpg\n  |    |--- PAL00005.txt\n  |    |--- ...\n  |--- train\n  |    |--- PAL00003.jpg\n  |    |--- PAL00003.txt\n  |    |--- PAL00006.jpg\n  |    |--- PAL00006.txt\n  |    |--- ...\n  |--- CASIA-10K_test.txt\n  |--- CASIA-10K_train.txt\n</code></pre>"},{"location":"cn/datasets/casia10k/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/casia10k/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name casia10k --task det \\\n    --image_dir path/to/CASIA-10K/train/ \\\n    --label_dir path/to/CASIA-10K/train \\\n    --output_path path/to/CASIA-10K/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>CASIA-10K/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/ccpd/","title":"Chinese City Parking Dataset (CCPD) 2019 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/ccpd/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>CCPD\u6570\u636e\u96c6\u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8be5\u6570\u636e\u96c6\u88ab\u5206\u4e3a3\u4e2a\u90e8\u5206\uff1a\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u6bcf\u4e2a\u96c6\u5408\u7684\u6807\u7b7e\u53ef\u5728<code>splits</code>\u6587\u4ef6\u5939\u4e0b\u53d1\u73b0\u3002CCPD-Green\u6570\u636e\u96c6\u5df2\u7ecf\u88ab\u5206\u5230\u4e0d\u540c\u7684\u6587\u4ef6\u5939\u4e2d\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u6807\u7b7e\u3002</p> <p>\u56fe\u50cf\u7684\u6ce8\u91ca\u53ef\u5728\u56fe\u50cf\u7684\u6587\u4ef6\u540d\u4e2d\u627e\u5230\uff0c\u5177\u4f53\u683c\u5f0f\u53ca\u63cf\u8ff0\u53ef\u5728\u5b98\u7f51\u67e5\u9605\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>CCPD2019\n  |--- ccpd_base\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ccpd_blur\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- ...\n  |--- ...\n  |--- ...\n  |--- splits\n</code></pre>"},{"location":"cn/datasets/ccpd/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ccpd/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ccpd --task det \\\n    --image_dir path/to/CCPD2019/ccpd_base \\\n    --label_dir path/to/CCPD2019/splits/train.txt \\\n    --output_path path/to/CCPD2019/det_gt.txt\n</code></pre> <p>CCPD-Green\u6570\u636e\u96c6\u4e0d\u9700\u8981<code>label_dir</code>\u3002</p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>CCPD2019/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/chinese_text_recognition/","title":"\u4e2d\u6587\u6587\u5b57\u8bc6\u522b\u6570\u636e\u96c6","text":""},{"location":"cn/datasets/chinese_text_recognition/#_2","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6309\u7167 Benchmarking-Chinese-Text-Recognition \u4e2d\u7684\u8bbe\u7f6e\uff0c\u6211\u4eec\u4f7f\u7528\u4e0e Datasets \u7ae0\u8282\u4e2d\u63cf\u8ff0\u7684\u76f8\u540c\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6570\u636e\u3002</p> <p>\u8bf7\u4e0b\u8f7d Download \u7ae0\u8282\u4e2d\u4ecb\u7ecd\u7684\u4ee5\u4e0bLMDB\u6587\u4ef6\uff1a</p> <ul> <li>\u573a\u666f\u6570\u636e\u96c6\uff1a\u8054\u5408\u6570\u636e\u96c6\u5305\u542b RCTW, ReCTS, LSVT, ArT, CTW</li> <li>\u7f51\u9875\uff1aMTWI</li> <li>\u6587\u6863\uff1a\u4f7f\u7528 Text Render \u751f\u6210</li> <li>\u624b\u5199\u6570\u636e\u96c6\uff1aSCUT-HCCDoc</li> </ul>"},{"location":"cn/datasets/chinese_text_recognition/#_3","title":"\u6570\u636e\u7ed3\u6784\u6574\u7406","text":"<p>\u4e0b\u8f7d\u6587\u4ef6\u540e\uff0c\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u6587\u4ef6\u653e\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939 <code>training</code> \u4e0b\uff0c\u6240\u6709\u9a8c\u8bc1\u6570\u636e\u653e\u5728 <code>validation</code> \u6587\u4ef6\u5939\u4e0b\uff0c\u6240\u6709\u8bc4\u4f30\u6570\u636e\u653e\u5728<code>evaluation</code>\u4e0b\u3002</p> <p>\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>chinese-text-recognition/\n\u251c\u2500\u2500 evaluation\n\u2502   \u251c\u2500\u2500 document_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_test\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_test\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u251c\u2500\u2500 training\n\u2502   \u251c\u2500\u2500 document_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 handwriting_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u251c\u2500\u2500 scene_train\n|   |   \u251c\u2500\u2500 data.mdb\n|   \u2502   \u2514\u2500\u2500 lock.mdb\n\u2502   \u2514\u2500\u2500 web_train\n|       \u251c\u2500\u2500 data.mdb\n|       \u2514\u2500\u2500 lock.mdb\n\u2514\u2500\u2500 validation\n    \u251c\u2500\u2500 document_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 handwriting_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u251c\u2500\u2500 scene_val\n    |   \u251c\u2500\u2500 data.mdb\n    \u2502   \u2514\u2500\u2500 lock.mdb\n    \u2514\u2500\u2500 web_val\n        \u251c\u2500\u2500 data.mdb\n        \u2514\u2500\u2500 lock.mdb\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition/#_4","title":"\u6570\u636e\u96c6\u914d\u7f6e","text":"<p>\u8981\u4f7f\u7528\u6570\u636e\u96c6\uff0c\u60a8\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u6307\u5b9a\u6570\u636e\u96c6\uff0c\u5982\u4e0b\u6240\u793a\u3002</p>"},{"location":"cn/datasets/chinese_text_recognition/#_5","title":"\u6a21\u578b\u8bad\u7ec3","text":"<pre><code>...\ntrain:\n  ...\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of training dataset\n    data_dir: training/                                               # Dir of training dataset, concatenated with `dataset_root` to be the complete dir of training dataset\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/                    # Root dir of validation dataset\n    data_dir: validation/                                             # Dir of validation dataset, concatenated with `dataset_root` to be the complete dir of validation dataset\n  ...\n</code></pre>"},{"location":"cn/datasets/chinese_text_recognition/#_6","title":"\u6a21\u578b\u8bc4\u4f30","text":"<pre><code>...\ntrain:\n  # \u8bad\u7ec3\u90e8\u5206\u4e0d\u9700\u8981\u4fee\u6539\uff0c\u56e0\u4e0d\u4f1a\u8c03\u7528\n...\neval:\n  dataset:\n    type: LMDBDataset\n    dataset_root: dir/to/chinese-text-recognition/             # Root dir of evaluation dataset\n    data_dir: evaluation/                                      # Dir of evaluation dataset, concatenated with `dataset_root` to be the complete dir of evaluation dataset\n  ...\n</code></pre> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/cocotext/","title":"COCO-Text \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/cocotext/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>COCO-Text\u6570\u636e\u96c6\u5b98\u7f51</p> <p>\u6570\u636e\u96c6\u56fe\u50cf\u548cJSON\u6807\u6ce8\u6587\u4ef6<code>annotations v1.4 JSON</code>\u53ef\u4ece\u6b64\u5904\u4e0b\u8f7d\u3002</p> <p>\u6ce8\u610f\uff1a\u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>COCO-Text\n  |--- train_images\n  |    |--- COCO_train2014_000000000036.jpg\n  |    |--- COCO_train2014_000000000064.jpg\n  |    |--- ...\n  |--- COCO_Text.json\n</code></pre>"},{"location":"cn/datasets/cocotext/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/cocotext/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name cocotext --task det \\\n    --image_dir path/to/COCO-Text/train_images/ \\\n    --label_dir path/to/COCO-Text/COCO_Text.json \\\n    --output_path path/to/COCO-Text/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>COCO-Text/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56deataset converters</p>"},{"location":"cn/datasets/converters/","title":"Dataset Preparation","text":"<p>\u672c\u6587\u6863\u5c55\u793a\u4e86\u5982\u4f55\u5c06OCR\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\uff08\u4e0d\u5305\u62ecLMDB\uff09\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u4ee5\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <p>\u60a8\u4e5f\u53ef\u4ee5\u53c2\u8003 <code>convert_datasets.sh</code>\u3002\u8fd9\u662f\u5c06\u7ed9\u5b9a\u76ee\u5f55\u4e0b\u6240\u6709\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6587\u4ef6\u8f6c\u6362\u4e3a\u901a\u7528\u683c\u5f0f\u7684Shell \u811a\u672c\u3002</p> \u8981\u4e0b\u8f7dOCR\u6570\u636e\u96c6\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u6240\u9700\u7684\u6570\u636e\u683c\u5f0f\uff0c\u8bf7\u53c2\u9605\u4ee5\u4e0b\u4ecb\u7ecd\u3002 <ul> <li>Born-Digital Images</li> <li>CASIA-10K</li> <li>CCPD</li> <li>Chinese text recognition</li> <li>COCO-Text</li> <li>CTW</li> <li>ICDAR2015</li> <li>ICDAR2019 ArT</li> <li>LSVT</li> <li>MLT2017</li> <li>MSRA-TD500</li> <li>MTWI-2018</li> <li>RCTW-17</li> <li>ReCTS</li> <li>SCUT-CTW1500</li> <li>SROIE</li> <li>SVT</li> <li>SynText150k</li> <li>SynthText</li> <li>TextOCR</li> <li>Total-Text</li> </ul>"},{"location":"cn/datasets/converters/#_1","title":"\u6587\u672c\u68c0\u6d4b/\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b","text":"<p>\u8f6c\u6362\u540e\u7684\u6807\u6ce8\u6587\u4ef6\u683c\u5f0f\u5e94\u4e3a\uff1a <pre><code>img_61.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre></p> <p>\u4ee5ICDAR2015\uff08ic15\uff09\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u8981\u5c06ic15\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6240\u9700\u7684\u683c\u5f0f\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/train/ch4_training_images \\\n        --label_dir /path/to/ic15/det/train/ch4_training_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/train/det_gt.txt\n</code></pre> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task det \\\n        --image_dir /path/to/ic15/det/test/ch4_test_images \\\n        --label_dir /path/to/ic15/det/test/ch4_test_localization_transcription_gt \\\n        --output_path /path/to/ic15/det/test/det_gt.txt\n</code></pre>"},{"location":"cn/datasets/converters/#_2","title":"\u6587\u672c\u8bc6\u522b","text":""},{"location":"cn/datasets/converters/#_3","title":"\u901a\u7528\u6570\u636e\u683c\u5f0f","text":"<p>\u6587\u672c\u8bc6\u522b\u6570\u636e\u96c6\u7684\u6807\u6ce8\u683c\u5f0f\u5982\u4e0b\uff1a</p> <p><pre><code>word_7.png  fusionopolis\nword_8.png  fusionopolis\nword_9.png  Reserve\nword_10.png CAUTION\nword_11.png citi\n</code></pre> \u8bf7\u6ce8\u610f\uff0c\u56fe\u50cf\u540d\u79f0\u548c\u6587\u672c\u6807\u7b7e\u4ee5<code>\\t</code>\u5206\u9694\u3002</p> <p>\u8981\u8f6c\u6362\u6807\u6ce8\u6587\u4ef6\uff0c\u8bf7\u8fd0\u884c\uff1a <pre><code># convert training anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_training_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/train/ch4_training_word_images_gt/rec_gt.txt\n</code></pre></p> <pre><code># convert testing anotation\npython tools/dataset_converters/convert.py \\\n        --dataset_name  ic15 \\\n        --task rec \\\n        --label_dir /path/to/ic15/rec/ch4_test_word_images_gt/gt.txt\n        --output_path /path/to/ic15/rec/ch4_test_word_images_gt/rec_gt.txt\n</code></pre>"},{"location":"cn/datasets/converters/#lmdb","title":"LMDB\u6570\u636e\u683c\u5f0f","text":"<p>\u90e8\u5206\u6570\u636e\u652f\u6301\u8f6c\u6362\u6210LMDB\u683c\u5f0f\uff0c\u76ee\u524d\u4ec5\u652f\u6301<code>SynthText</code>\u548c<code>SynthAdd</code>\u6570\u636e\u96c6\u3002</p> <p>\u8981\u8f6c\u6362\u6210LMDB\u683c\u5f0f\uff0c\u8bf7\u8fd0\u884c\uff1a <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name synthtext \\\n    --task rec_lmdb \\\n    --image_dir /path/to/SynthText \\\n    --label_dir /path/to/SynthText_gt.mat \\\n    --output_path ST_full\n</code></pre></p>"},{"location":"cn/datasets/ctw/","title":"CTW \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/ctw/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>COCO-Text\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u60a8\u9700\u8981\u586b\u5199\u8868\u683c\u624d\u80fd\u4e0b\u8f7d\u6b64\u6570\u636e\u96c6\u3002</p> <p>\u56fe\u50cf\u5206\u4e3a26\u6279\uff0c\u537326\u4e2a\u4e0d\u540c\u7684.tar\u5b58\u6863\u6587\u4ef6\uff0c\u683c\u5f0f\u4e3a<code>images-trainval/ctw-trainval*.tar</code>\u3002\u6240\u670926\u6279\u90fd\u9700\u8981\u4e0b\u8f7d\u3002 \u6ce8\u91ca\u5b58\u6863\u6587\u4ef6\u540d\u4e3a<code>ctw-annotations.tar.gz</code>\u3002</p> <p>\u4e0b\u8f7d\u538b\u7f29\u540e\u7684\u56fe\u50cf\u540e\uff0c\u89e3\u538b\u540e\u5c06\u6240\u6709\u56fe\u50cf\u6536\u96c6\u5230\u5355\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982<code>train_val/</code>\uff0c\u6ce8\u91ca\u4e5f\u8fdb\u884c\u76f8\u5e94\u89e3\u538b\u3002\u6700\u7ec8\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a</p> <pre><code>CTW\n  |--- train_val\n  |    |--- 0000172.jpg\n  |    |--- 0000174.jpg\n  |    |--- ...\n  |--- train.jsonl\n  |--- val.jsonl\n  |--- test_cls.jsonl\n  |--- info.json\n</code></pre>"},{"location":"cn/datasets/ctw/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ctw/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw --task det \\\n    --image_dir path/to/CTW/train_val/ \\\n    --label_dir path/to/CTW/train.jsonl \\\n    --output_path path/to/CTW/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>CTW/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u53ef\u4ee5\u66f4\u6539<code>label_dir</code>\u4ee5\u51c6\u5907\u9a8c\u8bc1\u96c6\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/ctw1500/","title":"SCUT-CTW1500 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/ctw1500/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08SCUT-CTW1500\uff09\u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>ctw1500\n \u251c\u2500\u2500 ctw1500_train_labels\n \u2502   \u251c\u2500\u2500 0001.xml\n \u2502   \u251c\u2500\u2500 0002.xml\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 gt_ctw_1500\n \u2502   \u251c\u2500\u2500 0001001.txt\n \u2502   \u251c\u2500\u2500 0001002.txt\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test_images\n \u2502   \u251c\u2500\u2500 1001.jpg\n \u2502   \u251c\u2500\u2500 1002.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train_images\n \u2502   \u251c\u2500\u2500 0001.jpg\n \u2502   \u251c\u2500\u2500 0002.jpg\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/ctw1500/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ctw1500/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/train_images/ \\\n    --label_dir path/to/ctw1500/ctw_1500_train_labels \\\n    --output_path path/to/ctw1500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ctw1500 --task det \\\n    --image_dir path/to/ctw1500/test_images/ \\\n    --label_dir path/to/ctw1500/gt_ctw_1500 \\\n    --output_path path/to/ctw1500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>ctw1500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/ic19_art/","title":"ICDAR2019 ArT \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/ic19_art/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>ICDAR2019 ArT\u6570\u636e\u96c6 \u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u56fe\u50cf\u9700\u8981\u4e0b\u8f7d\u201c\u4efb\u52a11\u548c\u4efb\u52a13\u201d\u90e8\u5206\u4e2d\u7684\u5b58\u6863\u6587\u4ef6<code>train_images.tar.gz</code>\u3002\u6ce8\u91ca\u9700\u8981\u4e0b\u8f7d\u540c\u4e00\u8282\u4e2d\u7684.JSON\u6587\u4ef6<code>train_labels.json</code>\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a <pre><code>ICDAR2019-ArT\n  |--- train_images\n  |    |--- train_images\n  |    |    |--- gt_0.jpg\n  |    |    |--- gt_1.jpg\n  |    |    |--- ...\n  |--- train_labels.json\n</code></pre></p>"},{"location":"cn/datasets/ic19_art/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/ic19_art/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name ic19_art --task det \\\n    --image_dir path/to/ICDAR2019-ArT/train_images/train_images/ \\\n    --label_dir path/to/ICDAR2019-ArT/train_labels.json \\\n    --output_path path/to/ICDAR2019-ArT/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>ICDAR2019-ArT/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/icdar2015/","title":"ICDAR 2015 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/icdar2015/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>ICDAR 2015 \u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f\uff1a\u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>ICDAR 2015 \u6311\u6218\u8d5b\u5206\u4e3a\u4e09\u4e2a\u4efb\u52a1\u3002\u4efb\u52a11\u662f\u6587\u672c\u5b9a\u4f4d\u3002\u4efb\u52a13\u662f\u5355\u8bcd\u8bc6\u522b\u3002\u4efb\u52a14\u662f\u7aef\u5230\u7aef\u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u3002\u4efb\u52a12\u6587\u672c\u5206\u5272\u7684\u6570\u636e\u4e0d\u53ef\u7528\u3002</p>"},{"location":"cn/datasets/icdar2015/#text-localization","title":"Text Localization","text":"<p>\u6709\u56db\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a</p> <pre><code>ch4_training_images.zip\nch4_training_localization_transcription_gt.zip\nch4_test_images.zip\nChallenge4_Test_Task1_GT.zip\n</code></pre>"},{"location":"cn/datasets/icdar2015/#word-recognition","title":"Word Recognition","text":"<p>\u6709\u4e09\u4e2a\u4e0e\u4efb\u52a13\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff0c \u5b83\u4eec\u5206\u522b\u662f\uff1a</p> <pre><code>ch4_training_word_images_gt.zip\nch4_test_word_images_gt.zip\nChallenge4_Test_Task3_GT.txt\n</code></pre> <p>\u8fd9\u4e09\u4e2a\u6587\u4ef6\u4ec5\u7528\u4e8e\u8bad\u7ec3\u5355\u8bcd\u8bc6\u522b\u6a21\u578b\u3002\u8bad\u7ec3\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u4e0d\u9700\u8981\u8fd9\u4e09\u4e2a\u6587\u4ef6\u3002</p>"},{"location":"cn/datasets/icdar2015/#e2e","title":"E2E","text":"<p>\u6709\u4e5d\u4e2a\u4e0e\u4efb\u52a14\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\u3002\u5176\u4e2d\u5305\u62ec\u4efb\u52a11\u4e2d\u7684\u56db\u4e2a\u6587\u4ef6\uff0c \u8fd8\u6709\u4e94\u4e2a\u8bcd\u6c47\u6587\u4ef6\u3002</p> <pre><code>ch4_training_vocabulary.txt\nch4_training_vocabularies_per_image.zip\nch4_test_vocabulary.txt\nch4_test_vocabularies_per_image.zip\nGenericVocabulary.txt\n</code></pre> <p>\u5982\u679c\u60a8\u4e0b\u8f7d\u4e86\u4e00\u4e2a\u540d\u4e3a <code>Challenge4_Test_Task4_GT.zip</code> \u7684\u6587\u4ef6\uff0c\u8bf7\u6ce8\u610f\u5b83\u4e0e <code>Challenge4_Test_Task1_GT.zip</code> \u662f\u76f8\u540c\u7684\u6587\u4ef6\uff0c\u9664\u4e86\u540d\u79f0\u4e0d\u540c\u3002\u5728\u8fd9\u4e2arepo\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 <code>Challenge4_Test_Task4_GT.zip</code> \u6765\u4ee3\u66ff ICDAR2015 \u6570\u636e\u96c6\u7684\u6587\u4ef6 <code>Challenge4_Test_Task1_GT.zip</code>\u3002</p> <p>\u5728 icdar2015 \u4e0b\u8f7d\u5b8c\u6210\u4ee5\u540e, \u8bf7\u628a\u6240\u6709\u7684\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  ic15/\n    ch4_test_images.zip\n    ch4_test_vocabularies_per_image.zip\n    ch4_test_vocabulary.txt\n    ch4_training_images.zip\n    ch4_training_localization_transcription_gt.zip\n    ch4_training_vocabularies_per_image.zip\n    ch4_training_vocabulary.txt\n    Challenge4_Test_Task4_GT.zip\n    GenericVocabulary.txt\n    ch4_test_word_images_gt.zip\n    ch4_training_word_images_gt.zip\n    Challenge4_Test_Task3_GT.zip\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/lsvt/","title":"LSVT \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/lsvt/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>LSVT\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u56fe\u50cf\u9700\u8981\u4e0b\u8f7d<code>train_full_images_0.tar.gz</code>\u548c<code>train_full_images_1.tar.gz</code>\u4e24\u4e2a\u538b\u7f29\u6587\u4ef6\uff0c\u6ce8\u91ca\u9700\u8981\u4e0b\u8f7d<code>train_full_labels.json</code>\u6587\u4ef6\u3002</p> <p>\u56fe\u50cf\u4e0b\u8f7d\u89e3\u538b\u7f29\u540e\uff0c\u8bf7\u5408\u5e76\u5230\u540c\u4e00\u4e2a\u6587\u4ef6\u4e2d\uff0c\u4f8b\u5982<code>train_images</code>\uff0c\u6700\u7ec8\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a <pre><code>LSVT\n  |--- train_images\n  |    |--- gt_0.jpg\n  |    |--- gt_1.jpg\n  |    |--- ...\n  |--- train_full_labels.json\n</code></pre></p>"},{"location":"cn/datasets/lsvt/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/lsvt/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name lsvt --task det \\\n    --image_dir path/to/LSVT/train_images/ \\\n    --label_dir path/to/LSVT/train_full_labels.json \\\n    --output_path path/to/LSVT/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>LSVT/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/mlt2017/","title":"MLT-2017 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/mlt2017/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>MLT (Multi-Lingual) 2017 \u8bba\u6587 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f\uff1a\u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>MLT 2017 \u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1a\u4efb\u52a1 1 \u662f\u6587\u672c\u68c0\u6d4b (\u591a\u8bed\u8a00\u6587\u672c)\u3002 \u4efb\u52a12\u662f\u6587\u672c\u8bc6\u522b\u3002</p>"},{"location":"cn/datasets/mlt2017/#_2","title":"\u6587\u672c\u68c0\u6d4b","text":"<p>\u670911\u4e2a\u4e0e\u4efb\u52a11\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff0c\u5b83\u4eec\u5206\u522b\u662f\uff1a</p> <pre><code>ch8_training_images_x.zip(x from 1 to 8)\nch8_validation_images.zip\nch8_training_localization_transcription_gt_v2.zip\nch8_validation_localization_transcription_gt_v2.zip\n</code></pre> <p>\u6d4b\u8bd5\u96c6\u4e0d\u9700\u8981\u4e0b\u8f7d\u3002</p>"},{"location":"cn/datasets/mlt2017/#_3","title":"\u6587\u672c\u8bc6\u522b","text":"<p>\u67096\u4e2a\u4e0e\u4efb\u52a12\u76f8\u5173\u7684\u6587\u4ef6\u9700\u8981\u4e0b\u8f7d\uff0c\u5b83\u4eec\u5206\u522b\u662f\uff1a <pre><code> ch8_training_word_images_gt_part_x.zip (x from 1 to 3)\n ch8_validation_word_images_gt.zip\n ch8_training_word_gt_v2.zip\n ch8_validation_word_gt_v2.zip\n ```\n&lt;/details&gt;\n\n\n\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e, \u5c06\u6587\u4ef6\u653e\u4e8e `[path-to-data-dir]` \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a:\n</code></pre> path-to-data-dir/   mlt2017/     # text detection     ch8_training_images_1.zip     ch8_training_images_2.zip     ch8_training_images_3.zip     ch8_training_images_4.zip     ch8_training_images_5.zip     ch8_training_images_6.zip     ch8_training_images_7.zip     ch8_training_images_8.zip     ch8_training_localization_transcription_gt_v2.zip     ch8_validation_images.zip     ch8_validation_localization_transcription_gt_v2.zip     # word recognition     ch8_training_word_images_gt_part_1.zip     ch8_training_word_images_gt_part_2.zip     ch8_training_word_images_gt_part_3.zip     ch8_training_word_gt_v2.zip     ch8_validation_word_images_gt.zip     ch8_validation_word_gt_v2.zip</p> <p>```</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/mtwi2018/","title":"ICPR MTWI-2018 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/mtwi2018/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>LSVT\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u8be5\u6570\u636e\u96c6\u5171\u5206\u4e3a3\u4e2a\u4efb\u52a1\uff1a\u4efb\u52a11\u4e3a\u7f51\u9875\u56fe\u50cf\u7684\u6587\u672c\u884c\uff08\u5217\uff09\u8bc6\u522b, \u4efb\u52a12\u4e3a\u7f51\u9875\u56fe\u50cf\u6587\u672c\u68c0\u6d4b, \u4efb\u52a13\u4e3a\u7aef\u5230\u7aef\u7684\u7f51\u9875\u56fe\u50cf\u6587\u672c\u68c0\u6d4b\u548c\u8bc6\u522b\uff0c\u8fd9\u4e09\u4e2a\u4efb\u52a1\u5171\u4eab\u76f8\u540c\u7684\u8bad\u7ec3\u6570\u636e\uff1a<code>mtwi_train.zip</code>\uff1b\u4efb\u52a11\u4f7f\u7528<code>mtwi_task1.zip</code>\u4f5c\u4e3a\u6d4b\u8bd5\u96c6, \u4efb\u52a12\u548c3\u4f7f\u7528<code>mtwi_task2_3.zip</code>\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\u3002\u8fd9\u91cc\u6211\u4eec\u4ec5\u4e0b\u8f7d\u548c\u4f7f\u7528<code>mtw_train.zip</code>\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a <pre><code>MTWI-2018\n  |--- image_train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- txt_train\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"cn/datasets/mtwi2018/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/mtwi2018/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name mtwi2018 --task det \\\n    --image_dir path/to/MTWI-2018/image_train/ \\\n    --label_dir path/to/MTWI-2018/txt_train.json \\\n    --output_path path/to/MTWI-2018/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>MTWI-2018/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/pubtabnet/","title":"PubTabNet \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/pubtabnet/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>PubTabNet\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u5730\u5740</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>pubtabnet\n  |--- train\n  |    |--- PMC1064074_007_00.png\n  |    |--- PMC1064076_003_00.png\n  |    |--- ...\n  |--- test\n  |    |--- PMC1064127_003_00.png\n  |    |--- PMC1065052_003_00.png\n  |    |--- ...\n  |--- val\n  |    |--- PMC1064865_002_00.png\n  |    |--- PMC1079806_002_00.png\n  |    |--- ...\n  |--- PubTabNet_2.0.0.jsonl\n</code></pre>"},{"location":"cn/datasets/pubtabnet/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/pubtabnet/#_3","title":"\u8868\u683c\u8bc6\u522b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u8868\u683c\u8bc6\u522b\u7684\u6807\u6ce8\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <ul> <li>\u5212\u5206training set\u6807\u6ce8</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/train/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_train.jsonl \\\n    --split train\n</code></pre> <ul> <li>\u5212\u5206validation set\u6807\u6ce8</li> </ul> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name pubtabnet --task table \\\n    --image_dir path/to/pubtabnet/val/ \\\n    --label_dir path/to/pubtabnet/PubTabNet_2.0.0.jsonl \\\n    --output_path path/to/pubtabnet/pubtab_val.jsonl \\\n    --split val\n</code></pre> <ul> <li>\u6ce8\uff1a\u539f\u6570\u636e\u96c6\u672a\u63d0\u4f9btesting set\u6807\u6ce8</li> </ul> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>pubtabnet/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>pubtab_train.jsonl</code>\u548c<code>pubtab_val.jsonl</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/rctw17/","title":"RCTW-17 \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/rctw17/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>RCTW-17\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u56fe\u50cf\u8bad\u7ec3\u96c6\u5206\u4e3a\u4e24\u4e2a\u96c6\u5408<code>train_images.zip.001</code>\u548c<code>train_images.zip.002</code>\uff0c\u6ce8\u91ca\u6587\u4ef6\u4e3a<code>*_gts.zip</code>\u3002</p> <p>\u56fe\u50cf\u4e0b\u8f7d\u89e3\u538b\u7f29\u540e\uff0c\u8bf7\u5408\u5e76\u5230\u540c\u4e00\u4e2a\u6587\u4ef6\u4e2d\uff0c\u4f8b\u5982<code>train_images</code>\uff0c\u6700\u7ec8\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a <pre><code>RCTW-17\n  |--- train_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- train_gts\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"cn/datasets/rctw17/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/rctw17/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rctw17 --task det \\\n    --image_dir path/to/RCTW-17/train_images/ \\\n    --label_dir path/to/RCTW-17/train_gts \\\n    --output_path path/to/RCTW-17/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>RCTW-17/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>Back to dataset converters</p>"},{"location":"cn/datasets/rects/","title":"ReCTS \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/rects/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>ReCTS\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a <pre><code>ReCTS\n  |--- img\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- gt\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n  |--- gt_unicode\n  |    |--- &lt;image_name&gt;.json\n  |    |--- &lt;image_name&gt;.json\n  |    |--- ...\n</code></pre></p>"},{"location":"cn/datasets/rects/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/rects/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name rects --task det \\\n    --image_dir path/to/ReCTS/img/ \\\n    --label_dir path/to/ReCTS/gt_unicode.json \\\n    --output_path path/to/ReCTS/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>ReCTS/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/sroie/","title":"SROIE \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/sroie/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>SROIE\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u6ce8\u610f: \u5728\u4e0b\u8f7d\u4e4b\u524d\uff0c\u8bf7\u5148\u6ce8\u518c\u4e00\u4e2a\u8d26\u53f7\u3002</p> <p>\u8be5\u6570\u636e\u96c6\u51713\u4e2a\u4efb\u52a1\uff1a\u4efb\u52a11\u4e3a\u6587\u672c\u68c0\u6d4b\uff0c\u4efb\u52a12\u4e3aOCR\uff0c\u4efb\u52a13\u4e3a\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u4ec5\u4e0b\u8f7d\u548c\u4f7f\u7528\u4efb\u52a11\u6570\u636e\u96c6\u3002</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a <pre><code>SROIE\n  |--- train\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.txt\n  |    |--- ...\n</code></pre></p>"},{"location":"cn/datasets/sroie/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/sroie/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name sroie --task det \\\n    --image_dir path/to/SROIE/train/ \\\n    --label_dir path/to/SROIE/train \\\n    --output_path path/to/SROIE/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>SROIE/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/svt/","title":"The Street View Text(SVT) \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/svt/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u8857\u666f\u6587\u672c\u6570\u636e\u96c6\uff08SVT\uff09\u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>svt1\n \u251c\u2500\u2500 img\n \u2502   \u251c\u2500\u2500 00_00.jpg\n \u2502   \u251c\u2500\u2500 00_01.jpg\n \u2502   \u251c\u2500\u2500 00_02.jpg\n \u2502   \u251c\u2500\u2500 00_03.jpg\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 test.xml\n \u2514\u2500\u2500 train.xml\n</code></pre>"},{"location":"cn/datasets/svt/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/svt/#_3","title":"\u8bc6\u522b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u8bc6\u522b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name  svt --task rec \\\n    --image_dir path/to/svt1/ \\\n    --label_dir path/to/svt1/train.xml \\\n    --output_path path/to/svt1/rec_train_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>svt1/</code> \u4e0b\u6709\u4e00\u4e2a\u6587\u4ef6\u5939 <code>cropped_images/</code> \u548c\u4e00\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>rec_train_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/syntext150k/","title":"Syntext-150k \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/syntext150k/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>SynText150k \u8bba\u6587</p> <p>\u4e0b\u8f7d Syntext-150k - Part1: 54,327 [\u56fe\u50cf][\u6807\u6ce8] - Part2: 94,723 [\u56fe\u50cf][\u6807\u6ce8]</p> <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  syntext150k/\n    syntext1/\n      images.zip\n      annotations/\n        ecms_v1_maxlen25.json\n    syntext2/\n      images.zip\n      annotations/\n        syntext_word_eng.json\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/synthtext/","title":"SynthText \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/synthtext/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>SynthText\u662f\u4e00\u4e2a\u5408\u6210\u751f\u6210\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5355\u8bcd\u5b9e\u4f8b\u88ab\u653e\u7f6e\u5728\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\uff0c\u5e76\u8003\u8651\u4e86\u573a\u666f\u5e03\u5c40\u3002</p> <p>\u8bba\u6587 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u4e0b\u8f7d<code>SynthText.zip</code>\u6587\u4ef6\u5e76\u89e3\u538b\u7f29\u5230<code>[path-to-data-dir]</code>\u6587\u4ef6\u5939\u4e2d\uff1a <pre><code>path-to-data-dir/\n \u251c\u2500\u2500 SynthText/\n \u2502   \u251c\u2500\u2500 1/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_1_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 2/\n \u2502   \u2502   \u251c\u2500\u2500 ant+hill_4_0.jpg\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 gt.mat\n</code></pre></p> <p>:warning: \u53e6\u5916, \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u5728\u4f7f\u7528 <code>SynthText</code> \u6570\u636e\u96c6\u4e4b\u524d\u5148\u8fdb\u884c\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e00\u4e9b\u9519\u8bef\u7684\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u4e0b\u5217\u7684\u65b9\u5f0f\u8fdb\u884c\u6821\u6b63: <pre><code>python tools/dataset_converters/convert.py --dataset_name=synthtext --task=det --label_dir=/path-to-data-dir/SynthText/gt.mat --output_path=/path-to-data-dir/SynthText/gt_processed.mat --image_dir=/path-to-data-dir/SynthText\n</code></pre> \u4ee5\u4e0a\u7684\u64cd\u4f5c\u4f1a\u4ea7\u751f\u4e0e<code>SynthText</code>\u539f\u59cb\u6807\u6ce8\u683c\u5f0f\u76f8\u540c\u4f46\u662f\u662f\u7ecf\u8fc7\u8fc7\u6ee4\u540e\u7684\u6807\u6ce8\u6570\u636e.</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/td500/","title":"MSRA Text Detection 500(MSRA-TD500) \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/td500/#_1","title":"\u6570\u636e\u4e0b\u8f7d","text":"<p>\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff08MSRA-TD500\uff09\u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>MSRA-TD500\n \u251c\u2500\u2500 test\n \u2502   \u251c\u2500\u2500 IMG_0059.gt\n \u2502   \u251c\u2500\u2500 IMG_0059.JPG\n \u2502   \u251c\u2500\u2500 IMG_0080.gt\n \u2502   \u251c\u2500\u2500 IMG_0080.JPG\n \u2502   \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 train\n \u2502   \u251c\u2500\u2500 IMG_0030.gt\n \u2502   \u251c\u2500\u2500 IMG_0030.JPG\n \u2502   \u251c\u2500\u2500 IMG_0063.gt\n \u2502   \u251c\u2500\u2500 IMG_0063.JPG\n \u2502   \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"cn/datasets/td500/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/td500/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <p><pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/train/ \\\n    --label_dir path/to/MSRA-TD500/train \\\n    --output_path path/to/MSRA-TD500/train_det_gt.txt\n</code></pre> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name td500 --task det \\\n    --image_dir path/to/MSRA-TD500/test/ \\\n    --label_dir path/to/MSRA-TD500/test \\\n    --output_path path/to/MSRA-TD500/test_det_gt.txt\n</code></pre></p> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939 <code>MSRA-TD500/</code> \u4e0b\u6709\u4e24\u4e2a\u6ce8\u91ca\u6587\u4ef6 <code>train_det_gt.txt</code> \u548c <code>test_det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/textocr/","title":"TextOCR \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/textocr/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<p>TextOCR\u6570\u636e\u96c6\u5b98\u7f51 | \u4e0b\u8f7d\u94fe\u63a5</p> <p>\u8bf7\u4ece\u4e0a\u8ff0\u7f51\u7ad9\u4e0b\u8f7d\u6570\u636e\u5e76\u89e3\u538b\u7f29\u6587\u4ef6\u3002\u89e3\u538b\u6587\u4ef6\u540e\uff0c\u6570\u636e\u7ed3\u6784\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a <pre><code>TextOCR\n  |--- train_val_images\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- &lt;image_name&gt;.jpg\n  |    |--- ...\n  |--- TextOCR_0.1_train.json\n  |--- TextOCR_0.1_val.json\n</code></pre></p>"},{"location":"cn/datasets/textocr/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"cn/datasets/textocr/#_3","title":"\u68c0\u6d4b\u4efb\u52a1","text":"<p>\u8981\u51c6\u5907\u7528\u4e8e\u6587\u672c\u68c0\u6d4b\u7684\u6570\u636e\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>python tools/dataset_converters/convert.py \\\n    --dataset_name textocr --task det \\\n    --image_dir path/to/TextOCR/train_val_images/ \\\n    --label_dir path/to/TextOCR/TextOCR_0.1_train.json \\\n    --output_path path/to/TextOCR/det_gt.txt\n</code></pre> <p>\u8fd0\u884c\u540e\uff0c\u5728\u6587\u4ef6\u5939<code>TextOCR/</code>\u4e0b\u4f1a\u751f\u6210\u6ce8\u91ca\u6587\u4ef6<code>det_gt.txt</code>\u3002</p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/datasets/totaltext/","title":"Total-Text \u6570\u636e\u96c6","text":""},{"location":"cn/datasets/totaltext/#_1","title":"\u6570\u636e\u96c6\u4e0b\u8f7d","text":"<ul> <li> <p>Total-Text \u8bba\u6587</p> </li> <li> <p>github\u4ed3\u5e93\u5730\u5740\uff1a</p> </li> <li>\u56fe\u50cf</li> <li> <p>\u6807\u6ce8\u6587\u4ef6</p> </li> <li> <p>\u4e0b\u8f7d\u94fe\u63a5\uff1a</p> </li> <li>\u56fe\u50cf (size = 441Mb)</li> <li>\u6807\u6ce8\u6587\u4ef6 (.txt\u683c\u5f0f)</li> </ul> <p>\u5728\u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u628a\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5728 <code>[path-to-data-dir]</code> \u6587\u4ef6\u5939\u5185\uff0c\u5982\u4e0b\u6240\u793a: <pre><code>path-to-data-dir/\n  totaltext/\n    totaltext.zip\n    txt_format.zip\n</code></pre></p> <p>\u8fd4\u56dedataset converters</p>"},{"location":"cn/inference/convert_dynamic/#-shape","title":"\u63a8\u7406 - \u52a8\u6001Shape\u5206\u6863","text":""},{"location":"cn/inference/convert_dynamic/#1","title":"1. \u7b80\u4ecb","text":"<p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff0c\u5982\u68c0\u6d4b\u51fa\u76ee\u6807\u540e\u518d\u8fdb\u884c\u76ee\u6807\u8bc6\u522b\uff0c\u7531\u4e8e\u76ee\u6807\u4e2a\u6570\u548c\u76ee\u6807\u5927\u5c0f\u4e0d\u56fa\u5b9a\uff0c\u5bfc\u81f4\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\u8f93\u5165BatchSize\u548cImageSize\u4e0d\u56fa\u5b9a\u3002\u5982\u679c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u6240\u4ee5\uff0c\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u53ef\u4ee5\u8bbe\u7f6e\u4e00\u4e9b\u5019\u9009\u503c\uff0c\u63a8\u7406\u7684\u65f6\u5019Resize\u5230\u6700\u5339\u914d\u7684\u5019\u9009\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002\u7528\u6237\u53ef\u4ee5\u51ed\u7ecf\u9a8c\u624b\u52a8\u9009\u62e9\u8fd9\u4e9b\u5019\u9009\u503c\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u672c\u5de5\u5177\u96c6\u6210\u4e86\u6570\u636e\u96c6\u7edf\u8ba1\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u7edf\u8ba1\u51fa\u5408\u9002\u7684<code>batch size</code>\u3001<code>height</code>\u548c<code>width</code>\u7ec4\u5408\u4f5c\u4e3a\u5019\u9009\u503c\uff0c\u5e76\u5c01\u88c5\u4e86\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u5206\u6863\u529f\u80fd\u3002</p>"},{"location":"cn/inference/convert_dynamic/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u73af\u5883\u5b89\u88c5\uff0c\u5b89\u88c5ACL\u6216MindSpore Lite\u73af\u5883\u3002</p>"},{"location":"cn/inference/convert_dynamic/#3","title":"3. \u6a21\u578b\u51c6\u5907","text":"<p>\u5f53\u524d\u652f\u6301\u8f93\u5165ONNX\u6a21\u578b\u6587\u4ef6\uff0c\u901a\u8fc7\u9009\u62e9\u4e0d\u540c\u540e\u7aef\uff0c\u81ea\u52a8\u5206\u6863\u5e76\u8f6c\u6362\u4e3aOM\u6216MIndIR\u6a21\u578b\u6587\u4ef6\u3002</p> <p>\u8bf7\u786e\u4fdd\u8f93\u5165\u6a21\u578b\u4e3a\u52a8\u6001Shape\u7248\u7684\u3002\u4f8b\u5982\uff0c\u6587\u672c\u68c0\u6d4b\u6a21\u578b\u5982\u679c\u9700\u8981\u5bf9H\u548cW\u5206\u6863\uff0c\u8981\u786e\u4fdd\u81f3\u5c11H\u548cW\u8f74\u662f\u52a8\u6001\u7684\uff0cShape\u53ef\u4ee5\u4e3a<code>(1,3,-1,-1)</code>\u548c<code>(-1,3,-1,-1)</code>\u7b49\u3002</p>"},{"location":"cn/inference/convert_dynamic/#4","title":"4. \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff1a</p> <ol> <li>\u56fe\u50cf\u6587\u4ef6\u5939</li> </ol> <ul> <li> <p>\u8be5\u5de5\u5177\u4f1a\u8bfb\u53d6\u6587\u4ef6\u5939\u4e0b\u7684\u6240\u6709\u56fe\u50cf\uff0c\u8bb0\u5f55<code>height</code>\u548c<code>width</code>\uff0c\u7edf\u8ba1\u51fa\u5408\u9002\u7684\u5019\u9009\u503c</p> </li> <li> <p>\u9002\u5408\u6587\u672c\u68c0\u6d4b\u548c\u6587\u672c\u8bc6\u522b\u6a21\u578b</p> </li> </ul> <ol> <li>\u6587\u672c\u68c0\u6d4b\u7684\u6807\u6ce8\u6587\u4ef6</li> </ol> <ul> <li> <p>\u53ef\u53c2\u8003converter\uff0c\u5b83\u662f\u53c2\u6570<code>task</code>\u4e3a<code>det</code>\u65f6\u8f93\u51fa\u7684\u6807\u6ce8\u6587\u4ef6</p> </li> <li> <p>\u8be5\u5de5\u5177\u4f1a\u8bfb\u53d6\u6bcf\u5f20\u56fe\u50cf\u4e0b\u6807\u6ce8\u7684\u6587\u672c\u6846\u5750\u6807\uff0c\u8bb0\u5f55<code>height</code>\u548c<code>width</code>\uff0c\u4ee5\u53ca\u6846\u7684\u6570\u91cf\u4f5c\u4e3a<code>batch size</code>\uff0c\u7edf\u8ba1\u51fa\u5408\u9002\u7684\u5019\u9009\u503c</p> </li> <li> <p>\u9002\u5408\u6587\u672c\u8bc6\u522b\u6a21\u578b</p> </li> </ul>"},{"location":"cn/inference/convert_dynamic/#5","title":"5. \u7528\u6cd5","text":"<p><code>cd deploy/models_utils/auto_scaling</code></p>"},{"location":"cn/inference/convert_dynamic/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u5bf9batch size\u8fdb\u884c\u5206\u6863</li> </ul> <p><pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/det_gt.txt\n    --input_shape=-1,3,48,192 \\\n    --output_path=output\n</code></pre>   \u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_dynamic_bs.om</code></p> <ul> <li>\u5bf9height\u548cwidth\u8fdb\u884c\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_dynamic_hw.om</code></p> <ul> <li>\u5bf9batch szie\u3001height\u548cwidth\u8fdb\u884c\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --dataset_path=/path/to/images \\\n    --input_shape=-1,3,-1,-1 \\\n    --output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u591a\u4e2aOM\u6a21\u578b\uff0c\u7ec4\u5408\u4e86\u591a\u4e2a\u4e0d\u540cBatch Size\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\uff1a<code>model_dynamic_bs1_hw.om</code>, <code>model_dynamic_bs4_hw.om</code>, ......</p> <ul> <li>\u4e0d\u505a\u5206\u6863</li> </ul> <pre><code>python converter.py \\\n    --backend=atc \\\n    --model_path=/path/to/model.onnx \\\n    --input_shape=4,3,48,192 \\\n    --output_path=output\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e3a\u5355\u4e2aOM\u6a21\u578b\uff1a<code>model_static.om</code></p>"},{"location":"cn/inference/convert_dynamic/#52","title":"5.2 \u8be6\u7ec6\u53c2\u6570","text":"\u540d\u79f0 \u9ed8\u8ba4\u503c \u5fc5\u9700 \u542b\u4e49 model_path \u65e0 \u662f \u6a21\u578b\u6587\u4ef6\u8def\u5f84 input_shape \u65e0 \u662f \u6a21\u578b\u8f93\u5165shape\uff0cNCHW\u683c\u5f0f data_path \u65e0 \u5426 \u6570\u636e\u96c6\u6216\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84 input_name x \u5426 \u6a21\u578b\u7684\u8f93\u5165\u540d backend atc \u5426 \u8f6c\u6362\u5de5\u5177\uff0catc\u6216lite output_path ./output \u5426 \u8f93\u51fa\u6a21\u578b\u4fdd\u5b58\u6587\u4ef6\u5939 soc_version Ascend310P3 \u5426 Ascend\u7684soc\u578b\u53f7\uff0cAscend310P3\u6216Ascend310"},{"location":"cn/inference/convert_dynamic/#53","title":"5.3 \u914d\u7f6e\u6587\u4ef6","text":"<p>\u9664\u4e86\u4e0a\u8ff0\u547d\u4ee4\u884c\u53c2\u6570\u5916\uff0c\u5728auto_scaling.yaml\u4e2d\u8fd8\u6709\u4e00\u4e9b\u53c2\u6570\uff0c\u7528\u4ee5\u63cf\u8ff0\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u65b9\u5f0f\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\uff1a</p> <ul> <li>limit_side_len</li> </ul> <p>\u539f\u59cb\u8f93\u5165\u6570\u636e\u7684<code>height</code>\u548c<code>width</code>\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u8303\u56f4\u6309\u7167\u6bd4\u4f8b\u8fdb\u884c\u538b\u7f29\uff0c\u53ef\u4ee5\u8c03\u6574\u6570\u636e\u7684\u79bb\u6563\u7a0b\u5ea6\u3002</p> <ul> <li>strategy</li> </ul> <p>\u6570\u636e\u7edf\u8ba1\u7b97\u6cd5\u7b56\u7565\uff0c\u652f\u6301<code>mean_std</code>\u548c<code>max_min</code>\u4e24\u79cd\u7b97\u6cd5\uff0c\u9ed8\u8ba4\uff1a<code>mean_std</code>\u3002</p> <ul> <li> <p>mean_std</p> <p><pre><code>mean_std = [mean - n_std * sigma\uff0cmean + n_std * sigma]\n</code></pre>    - max_min <pre><code>max_min = [min - (max - min) * expand_ratio / 2\uff0cmax + (max - min) * expand_ratio / 2]\n</code></pre></p> </li> <li> <p>width_range/height_range</p> </li> </ul> <p>\u5bf9\u79bb\u6563\u7edf\u8ba1\u4e4b\u540e\u7684width/height\u5927\u5c0f\u9650\u5236\uff0c\u8d85\u51fa\u5c06\u88ab\u8fc7\u6ee4\u3002</p> <ul> <li>interval</li> </ul> <p>\u95f4\u9694\u5927\u5c0f\uff0c\u5982\u67d0\u4e9b\u7f51\u7edc\u53ef\u80fd\u8981\u6c42\u8f93\u5165\u5c3a\u5bf8\u5fc5\u987b\u662f32\u7684\u500d\u6570\u3002</p> <ul> <li>max_scaling_num</li> </ul> <p>\u5206\u6863\u6570\u91cf\u7684\u4e0a\u9650\u3002</p> <ul> <li>batch_choices</li> </ul> <p>\u9ed8\u8ba4\u7684batch size\u503c\uff0c\u5982\u679cdata_path\u4f20\u5165\u7684\u662f\u56fe\u50cf\u6587\u4ef6\u5939\uff0c\u5219\u65e0\u6cd5\u7edf\u8ba1\u51fabatch size\u4fe1\u606f\uff0c\u5c31\u4f1a\u4f7f\u7528\u8be5\u9ed8\u8ba4\u503c\u3002</p> <ul> <li>default_scaling</li> </ul> <p>\u7528\u6237\u4e0d\u4f20\u5165data_path\u6570\u636e\u65f6\uff0c\u63d0\u4f9b\u9ed8\u8ba4\u7684<code>height</code>\u548c<code>width</code>\u5206\u6863\u503c\u3002</p>"},{"location":"cn/inference/convert_tutorial/#-","title":"\u63a8\u7406 - \u6a21\u578b\u8f6c\u6362\u6559\u7a0b","text":"<p>MindOCR\u652f\u6301MindOCR\u8bad\u7ec3\u6a21\u578b\u4ee5\u53ca\u4e09\u65b9\u6a21\u578b(PaddelOCR\u548cmmOCR)\u7684\u63a8\u7406\u3002</p>"},{"location":"cn/inference/convert_tutorial/#1-mindocr","title":"1. MindOCR\u6a21\u578b","text":"<p>MindOCR\u6a21\u578b\u7684\u63a8\u7406\u4f7f\u7528MindSpore Lite\u540e\u7aef\u3002</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];</code></pre>"},{"location":"cn/inference/convert_tutorial/#11","title":"1.1 \u6a21\u578b\u5bfc\u51fa","text":"<p>\u5728\u63a8\u7406\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u8bad\u7ec3\u7aef\u7684ckpt\u6587\u4ef6\u5bfc\u51fa\u4e3aMindIR\u6587\u4ef6\uff0c\u8bf7\u6267\u884c<code>tools/export.py</code>\uff1a</p> <pre><code># \u5728\u7ebf\u4e0b\u8f7d\u6a21\u578b\u53c2\u6570\uff0c\u5bfc\u51fa`dbnet_resnet50` \u6a21\u578b\u7684MindIR\npython tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280\n\n# \u4f7f\u7528\u672c\u5730ckpt\u6587\u4ef6\uff0c\u5bfc\u51fa`dbnet_resnet50` \u6a21\u578b\u7684MindIR\npython tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt\n\n# \u4f7f\u7528\u672c\u5730ckpt\u6587\u4ef6\u548c\u53c2\u6570yaml\u6587\u4ef6\uff0c\u5bfc\u51fa`dbnet_resnet50` \u6a21\u578b\u7684MindIR\npython tools/export.py --model_name_or_config configs/rec/crnn/crnn_resnet34.yaml --local_ckpt_path ~/.mindspore/models/crnn_resnet34-83f37f07.ckpt --data_shape 32 100\n\n\u66f4\u591a\u53c2\u6570\u4f7f\u7528\u8be6\u60c5\uff0c\u8bf7\u6267\u884c `python tools/export.py -h`.\n</code></pre> <p>\u90e8\u5206\u6a21\u578b\u63d0\u4f9b\u4e86MIndIR\u5bfc\u51fa\u6587\u4ef6\u7684\u4e0b\u8f7d\u94fe\u63a5\uff0c\u89c1\u6a21\u578b\u5217\u8868\uff0c\u53ef\u8df3\u8f6c\u5230\u5bf9\u5e94\u6a21\u578b\u7684\u4ecb\u7ecd\u9875\u9762\u8fdb\u884c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/inference/convert_tutorial/#12","title":"1.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u9700\u8981\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\uff0c\u5c06\u4e0a\u8ff0\u5bfc\u51fa\u7684MindIR\u6587\u4ef6\u8fdb\u884c\u79bb\u7ebf\u8f6c\u6362\uff0c\u4ece\u800c\u7528\u4e8eMindSpore Lite\u7684\u63a8\u7406\u3002</p> <p><code>converter_lite</code>\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1\u63a8\u7406\u6a21\u578b\u79bb\u7ebf\u8f6c\u6362\u3002</p> <p>\u5047\u8bbe\u8f93\u5165\u6a21\u578b\u4e3ainput.mindir\uff0c\u7ecf\u8fc7<code>converter_lite</code>\u5de5\u5177\u8f6c\u6362\u540e\u7684\u8f93\u51fa\u6a21\u578b\u4e3aoutput.mindir\uff0c\u5219\u6a21\u578b\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=MINDIR \\\n    --optimize=ascend_oriented \\\n    --modelFile=input.mindir \\\n    --outputFile=output \\\n    --configFile=config.txt\n</code></pre> <p>\u5176\u4e2d\uff0c<code>config.txt</code>\u53ef\u4ee5\u8bbe\u7f6e\u8f6c\u6362\u6a21\u578b\u7684Shape\u548c\u63a8\u7406\u7cbe\u5ea6\u3002</p>"},{"location":"cn/inference/convert_tutorial/#121-shape","title":"1.2.1 \u6a21\u578bShape\u914d\u7f6e","text":"<ul> <li> <p>\u9759\u6001Shape</p> <p>\u5982\u679c\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u540d\u4e3a<code>x</code>\uff0c\u8f93\u5165Shape\u4e3a<code>(1,3,736,1280)</code>\uff0c\u5219config.txt\u5982\u4e0b\uff1a <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre> \u8f6c\u6362\u751f\u6210\u7684output.mindir\u4e3a\u9759\u6001shape\u7248\uff0c\u63a8\u7406\u65f6\u7684\u8f93\u5165\u56fe\u50cf\u9700\u8981Resize\u5230\u8be5input_shape\u4ee5\u6ee1\u8db3\u8f93\u5165\u8981\u6c42\u3002</p> </li> <li> <p>\u52a8\u6001Shape(\u5206\u6863)</p> <p>\u5728\u67d0\u4e9b\u63a8\u7406\u573a\u666f\uff08\u5982\u68c0\u6d4b\u51fa\u6587\u672c\u533a\u57df\u540e\uff0c\u518d\u6267\u884c\u6587\u672c\u8bc6\u522b\u7f51\u7edc\uff09\uff0c\u7531\u4e8e\u6587\u672c\u533a\u57df\u7684\u4e2a\u6570\u548c\u5206\u8fa8\u7387\u4e0d\u56fa\u5b9a\uff0c\u6bcf\u6b21\u63a8\u7406\u90fd\u6309\u7167\u6700\u5927\u7684BatchSize\u6216\u6700\u5927ImageSize\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f1a\u9020\u6210\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</p> <p>\u5047\u8bbe\u5bfc\u51fa\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3, -1, -1)\uff0cNHW\u8fd93\u4e2a\u8f74\u662f\u52a8\u6001\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\u3002</p> <p><code>converter_lite</code>\u901a\u8fc7<code>configFile</code>\u914d\u7f6e<code>[ascend_context]</code>\u4e2d<code>dynamic_dims</code>\u53c2\u6570\u6765\u5b9e\u73b0\uff0c\u8be6\u7ec6\u4fe1\u606f\u53ef\u53c2\u8003\u52a8\u6001shape\u914d\u7f6e\uff0c\u4e0b\u6587\u7b80\u79f0\u201d\u5206\u6863\u201c\u3002</p> <p>\u6240\u4ee5\uff0c\u8f6c\u6362\u65f6\u67092\u79cd\u9009\u62e9\uff0c\u901a\u8fc7\u8bbe\u7f6e\u4e0d\u540c\u7684config.txt\u5b9e\u73b0\uff1a</p> <ul> <li> <p>\u52a8\u6001Image Size</p> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre> </li> <li> <p>\u52a8\u6001Batch Size</p> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cconfig.txt\u5982\u4e0b\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[-1,3,736,1280]\ndynamic_dims=[1],[4],[8],[16],[32]\n</code></pre> </li> </ul> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u9009\u62e9\u52a8\u6001\u503c\u548c\u6a21\u578b\u8f6c\u6362\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> </li> <li> <p>\u52a8\u6001Shape</p> <p>\u8fd9\u79cd<code>\u52a8\u6001Shape</code>\u4e0e<code>\u52a8\u6001Shape(\u5206\u6863)</code>\u7684\u533a\u522b\u5728\u4e8e\u4ed6\u7075\u6d3b\u9002\u914d\u5404\u79cdBatch Size\u548cShape\u7684\u8f93\u5165\u3002\u5176\u914d\u7f6e\u7684config.txt\u5982\u4e0b <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></p> </li> </ul> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#122","title":"1.2.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>converter_lite</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003Ascend\u8f6c\u6362\u5de5\u5177\u529f\u80fd\u8bf4\u660e\uff0c\u5728\u914d\u7f6e\u6587\u4ef6\u7684\u8868\u683c\u4e2d\u63cf\u8ff0\u4e86<code>precision_mode</code>\u53c2\u6570\u7684\u4f7f\u7528\u65b9\u6cd5\uff0c\u53ef\u9009\u62e9<code>enforce_fp16</code>\u3001<code>enforce_fp32</code>\u3001<code>preferred_fp32</code>\u548c<code>enforce_origin</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>config.txt</code>\u7684<code>[ascend_context]</code>\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\nprecision_mode=enforce_fp32\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>enforce_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial/#2-paddleocr","title":"2. PaddleOCR\u6a21\u578b","text":"<p>PaddleOCR\u6a21\u578b\u7684\u63a8\u7406\u53ef\u4ee5\u4f7f\u7528MindSpore Lite\u548cACL\u4e24\u79cd\u540e\u7aef\uff0c\u5206\u522b\u5bf9\u5e94MindSpore Lite MindIR\u6a21\u578b\u548cOM\u6a21\u578b\u3002</p> <pre><code>graph LR;\n    PaddleOCR\u8bad\u7ec3\u6a21\u578b -- export --&gt; PaddleOCR\u63a8\u7406\u6a21\u578b -- paddle2onnx --&gt; ONNX;\n    ONNX -- converter_lite --&gt; o2(MindSpore Lite MindIR);\n    ONNX -- atc --&gt; o1(OM);</code></pre> <p>\u6d89\u53ca\u52302\u79cd\u683c\u5f0f\u7684Paddle\u6a21\u578b\uff0c\u8bad\u7ec3\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\uff0c\u533a\u522b\u5982\u4e0b\uff1a</p> \u6a21\u578b\u7c7b\u578b \u6a21\u578b\u683c\u5f0f \u7b80\u4ecb \u8bad\u7ec3\u6a21\u578b .pdparams\u3001.pdopt\u3001.states PaddlePaddle\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4fdd\u5b58\u7684\u6a21\u578b\u7684\u6743\u91cd\u3001\u4f18\u5316\u5668\u72b6\u6001\u7b49\u4fe1\u606f \u63a8\u7406\u6a21\u578b inference.pdmodel\u3001inference.pdiparams PaddlePaddle\u63a8\u7406\u6a21\u578b\uff0c\u53ef\u7531\u5176\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa\u5f97\u5230\uff0c\u4fdd\u5b58\u4e86\u6a21\u578b\u7684\u7ed3\u6784\u548c\u53c2\u6570 <p>\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\u5e76\u89e3\u538b\uff0c\u8bf7\u6839\u636e\u6a21\u578b\u683c\u5f0f\u6765\u533a\u522b\u662f\u8bad\u7ec3\u6a21\u578b\u8fd8\u662f\u63a8\u7406\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#21-","title":"2.1 \u8bad\u7ec3\u6a21\u578b -&gt; \u63a8\u7406\u6a21\u578b","text":"<p>\u5728PaddleOCR\u6a21\u578b\u7684\u4e0b\u8f7d\u94fe\u63a5\u4e2d\uff0c\u6709\u8bad\u7ec3\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u4e24\u79cd\u683c\u5f0f\uff0c\u5982\u679c\u63d0\u4f9b\u7684\u662f\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u683c\u5f0f\u3002</p> <p>\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u539fPaddleOCR\u4ecb\u7ecd\u9875\u9762\uff0c\u4e00\u822c\u4f1a\u6709\u8f6c\u6362\u811a\u672c\u6837\u4f8b\uff0c\u53ea\u9700\u8981\u4f20\u5165\u8bad\u7ec3\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u3001\u6a21\u578b\u6587\u4ef6\u548c\u4fdd\u5b58\u8def\u5f84\u5373\u53ef\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code># git clone https://github.com/PaddlePaddle/PaddleOCR.git\n# cd PaddleOCR\npython tools/export_model.py \\\n    -c configs/det/det_r50_vd_db.yml \\\n    -o Global.pretrained_model=./det_r50_vd_db_v2.0_train/best_accuracy  \\\n    Global.save_inference_dir=./det_db\n</code></pre>"},{"location":"cn/inference/convert_tutorial/#22-onnx","title":"2.2 \u63a8\u7406\u6a21\u578b -&gt; ONNX","text":"<p>\u5b89\u88c5\u6a21\u578b\u8f6c\u6362\u5de5\u5177paddle2onnx\uff1a<code>pip install paddle2onnx==0.9.5</code></p> <p>\u8be6\u7ec6\u4f7f\u7528\u6559\u7a0b\u8bf7\u53c2\u8003Paddle2ONNX\u6a21\u578b\u8f6c\u5316\u4e0e\u9884\u6d4b\u3002</p> <p>\u6267\u884c\u8f6c\u6362\u547d\u4ee4\uff0c\u751f\u6210onnx\u6a21\u578b\uff1a</p> <pre><code>paddle2onnx \\\n    --model_dir det_db \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file det_db.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>\u53c2\u6570\u4e2dinput_shape_dict\u7684\u503c\uff0c\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7Netron\u5de5\u5177\u6253\u5f00\u63a8\u7406\u6a21\u578b\u67e5\u770b\uff0c\u6216\u8005\u5728\u4e0a\u8ff0tools/export_model.py\u7684\u4ee3\u7801\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/inference/convert_tutorial/#23-onnx-mindir","title":"2.3 ONNX -&gt; MindIR","text":"<p>\u4f7f\u7528converter_lite\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aMindSpore Lite MindIR\u6a21\u578b\u3002\u5de5\u5177\u7684\u8be6\u7ec6\u6559\u7a0b\u89c1MindSpore Lite\u4e91\u4fa7\u63a8\u7406\u79bb\u7ebf\u6a21\u578b\u8f6c\u6362\u3002</p> <p>\u8f6c\u6362\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=det_db.onnx \\\n    --outputFile=det_db_output \\\n    --configFile=config.txt\n</code></pre> <p>\u8f6c\u6362\u6d41\u7a0b\u548cMindOCR\u6a21\u578b\u5b8c\u5168\u76f8\u540c\uff0c\u4ec5\u6709\u533a\u522b\u662f<code>--fmk</code>\u9700\u6307\u5b9a\u8f93\u5165\u662fONNX\u6a21\u578b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002</p>"},{"location":"cn/inference/convert_tutorial/#24-onnx-om","title":"2.4 ONNX -&gt; OM","text":"<p>\u4f7f\u7528ATC\u5de5\u5177\u53ef\u4ee5\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u4e3aOM\u6a21\u578b\u3002</p> <p>\u6607\u817e\u5f20\u91cf\u7f16\u8bd1\u5668\uff08Ascend Tensor Compiler\uff0c\u7b80\u79f0ATC\uff09\u662f\u5f02\u6784\u8ba1\u7b97\u67b6\u6784CANN\u4f53\u7cfb\u4e0b\u7684\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff0c\u5b83\u53ef\u4ee5\u5c06\u5f00\u6e90\u6846\u67b6\u7684\u7f51\u7edc\u6a21\u578b\u8f6c\u6362\u4e3a\u6607\u817eAI\u5904\u7406\u5668\u652f\u6301\u7684.om\u683c\u5f0f\u79bb\u7ebf\u6a21\u578b\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1ATC\u6a21\u578b\u8f6c\u6362\u3002</p>"},{"location":"cn/inference/convert_tutorial/#241-shape","title":"2.4.1 \u6a21\u578bShape\u914d\u7f6e","text":"<p>\u4e0a\u8ff0\u793a\u4f8b\u4e2d\u5bfc\u51fa\u7684ONNX\u6a21\u578b\u8f93\u5165Shape\u4e3a(-1, 3,-1,-1)\u3002</p> <ul> <li>\u9759\u6001Shape</li> </ul> <p>\u53ef\u4ee5\u8f6c\u6362\u4e3a\u9759\u6001Shape\u7248\u7684\u6a21\u578b\uff0cNHW\u90fd\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <ul> <li> <p>\u52a8\u6001Shape(\u5206\u6863)</p> <p>ATC\u5de5\u5177\u901a\u8fc7\u8bbe\u7f6e\u53c2\u6570 dynamic_dims\u6765\u652f\u6301Shape\u7684**\u5206\u6863**\uff0c\u53ef\u4ee5\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\u4e00\u4e9b\u53ef\u9009\u503c\uff0c\u4ee5\u9002\u5e94\u63a8\u7406\u65f6\u5404\u79cdShape\u5927\u5c0f\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u5982\u4e0b\u4e24\u79cd\u9009\u62e9\uff1a</p> <ul> <li> <p>\u52a8\u6001Image Size</p> <p>N\u4f7f\u7528\u56fa\u5b9a\u503c\uff0cHW\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,-1,-1\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"736,1280;768,1280;896,1280;1024,1280\" \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_dynamic_bs \\\n    --log=error\n</code></pre> </li> <li> <p>\u52a8\u6001Batch Size</p> <p>N\u4f7f\u7528\u591a\u4e2a\u53ef\u9009\u503c\uff0cHW\u4f7f\u7528\u56fa\u5b9a\u503c\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:-1,3,736,1280\" \\\n    --input_format=ND \\\n    --dynamic_dims=\"1;4;8;16;32\" \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_dynamic_bs \\\n    --log=error\n</code></pre> </li> </ul> <p>\u5728\u8f6c\u6362\u52a8\u6001Batch Size/Image Size\u6a21\u578b\u65f6\uff0cNHW\u503c\u7684\u9009\u62e9\u53ef\u4ee5\u7531\u7528\u6237\u6839\u636e\u7ecf\u9a8c\u503c\u8bbe\u5b9a\uff0c\u4e5f\u53ef\u4ee5\u4ece\u6570\u636e\u96c6\u4e2d\u7edf\u8ba1\u800c\u6765\u3002</p> <p>\u5982\u679c\u6a21\u578b\u8f6c\u6362\u65f6\u9700\u8981\u540c\u65f6\u652f\u6301\u52a8\u6001Batch Size\u548c\u52a8\u6001Image Size\uff0c\u53ef\u4ee5\u7ec4\u5408\u591a\u4e2a\u4e0d\u540cBatch Size\u7684\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u52a8\u6001Image Size\u3002</p> <p>\u4e3a\u4e86\u7b80\u5316\u6a21\u578b\u8f6c\u6362\u6d41\u7a0b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86**\u81ea\u52a8\u5206\u6863\u5de5\u5177**\uff0c\u53ef\u4ee5\u4e00\u952e\u5f0f\u5b8c\u6210\u52a8\u6001\u503c\u9009\u62e9\u548c\u6a21\u578b\u8f6c\u6362\u8fc7\u7a0b\uff0c\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003\u6a21\u578bShape\u5206\u6863\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5982\u679c\u5bfc\u51fa\u7684\u6a21\u578b\u662f\u9759\u6001Shape\u7248\u7684\uff0c\u5219\u65e0\u6cd5\u5206\u6863\uff0c\u9700\u786e\u4fdd\u5bfc\u51fa\u52a8\u6001Shape\u7248\u7684\u6a21\u578b\u3002</p> </li> </ul>"},{"location":"cn/inference/convert_tutorial/#242","title":"2.4.2 \u6a21\u578b\u7cbe\u5ea6\u6a21\u5f0f\u914d\u7f6e","text":"<p>\u5bf9\u4e8e\u6a21\u578b\u63a8\u7406\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5728\u8f6c\u6362\u6a21\u578b\u65f6\u901a\u8fc7<code>ATC</code>\u8bbe\u7f6e\u3002</p> <p>\u8bf7\u53c2\u8003\u53c2\u6570precision_mode\u7684\u8bf4\u660e\uff0c\u53ef\u9009\u62e9<code>force_fp16</code>\u3001<code>force_fp32</code>\u3001<code>allow_fp32_to_fp16</code>\u3001<code>must_keep_origin_dtype</code>\u548c<code>allow_mix_precision</code>\u7b49\u3002</p> <p>\u6545\u800c\uff0c\u53ef\u4ee5\u5728\u4e0a\u8ff0<code>atc</code>\u547d\u4ee4\u4e2d\u589e\u52a0<code>precision_mode</code>\u53c2\u6570\u6765\u8bbe\u7f6e\u7cbe\u5ea6\uff1a</p> <pre><code>atc --model=det_db.onnx \\\n    --framework=5 \\\n    --input_shape=\"x:1,3,736,1280\" \\\n    --input_format=ND \\\n    --precision_mode=force_fp32 \\\n    --soc_version=Ascend310P3 \\\n    --output=det_db_static \\\n    --log=error\n</code></pre> <p>\u5982\u4e0d\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u4e3a<code>force_fp16</code>\u3002</p>"},{"location":"cn/inference/convert_tutorial/#3-mmocr","title":"3. MMOCR\u6a21\u578b","text":"<p>MMOCR\u4f7f\u7528Pytorch\uff0c\u5176\u6a21\u578b\u6587\u4ef6\u4e00\u822c\u662fpth\u683c\u5f0f\u3002</p> <p>\u9700\u8981\u5148\u628a\u5b83\u5bfc\u51fa\u4e3aONNX\u683c\u5f0f\uff0c\u518d\u8f6c\u6362\u4e3aACL/MindSpore Lite\u652f\u6301\u7684OM/MindIR\u683c\u5f0f\u3002</p> <pre><code>graph LR;\n    MMOCR_pth -- export --&gt;  ONNX;\n    ONNX -- converter_lite --&gt; o2(MindSpore Lite MindIR);\n    ONNX -- atc --&gt; o1(OM);</code></pre>"},{"location":"cn/inference/convert_tutorial/#31-mmocr-onnx","title":"3.1 MMOCR\u6a21\u578b -&gt; ONNX","text":"<p>MMDeploy\u63d0\u4f9b\u4e86MMOCR\u6a21\u578b\u5bfc\u51faONNX\u7684\u547d\u4ee4\uff0c\u8be6\u7ec6\u6559\u7a0b\u89c1\u5982\u4f55\u8f6c\u6362\u6a21\u578b\u3002</p> <p>\u5bf9\u4e8e\u53c2\u6570<code>deploy_cfg</code>\u9700\u9009\u62e9\u76ee\u5f55mmdeploy/configs/mmocr\u4e0b\u7684<code>*_onnxruntime_dynamic.py</code>\u6587\u4ef6\uff0c\u4ece\u800c\u5bfc\u51fa\u4e3a\u52a8\u6001Shape\u7248ONNX\u6a21\u578b\u3002</p>"},{"location":"cn/inference/convert_tutorial/#32-onnx-mindir","title":"3.2 ONNX -&gt; MindIR","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; MIndIR\u3002</p>"},{"location":"cn/inference/convert_tutorial/#33-onnx-om","title":"3.3 ONNX -&gt; OM","text":"<p>\u8bf7\u53c2\u8003\u4e0a\u6587PaddleOCR\u5c0f\u8282\u7684ONNX -&gt; OM\u3002</p>"},{"location":"cn/inference/environment/#-","title":"\u63a8\u7406 - \u8fd0\u884c\u73af\u5883\u5b89\u88c5","text":"<p>MindOCR\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\u7684\u63a8\u7406\u3002</p> <p>\u8bf7\u786e\u4fdd\u7cfb\u7edf\u6b63\u786e\u5b89\u88c5\u4e86\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\uff0c\u5982\u679c\u6ca1\u6709\u5b89\u88c5\uff0c\u8bf7\u5148\u53c2\u8003\u5b89\u88c5\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\u5c0f\u8282\u8fdb\u884c\u5b89\u88c5\u3002</p> <p>MindOCR\u540e\u7aef\u652f\u6301ACL\u548cMindSpore Lite\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u4f7f\u7528ACL\u6a21\u5f0f\u63a8\u7406\u524d\u9700\u4f7f\u7528ATC\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210om\u683c\u5f0f\uff0c\u4f7f\u7528MindSpore Lite\u63a8\u7406\u524d\u9700\u4f7f\u7528converter_lite\u5de5\u5177\u5c06\u6a21\u578b\u8f6c\u6362\u6210MindIR\u683c\u5f0f\uff0c\u5177\u4f53\u533a\u522b\u5982\u4e0b\uff1a</p> ACL Mindspore Lite \u8f6c\u6362\u5de5\u5177 ATC converter_lite \u63a8\u7406\u6a21\u578b\u683c\u5f0f om MindIR"},{"location":"cn/inference/environment/#1-acl","title":"1. ACL\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684ACL\u65b9\u5f0f\u63a8\u7406\uff0c\u76ee\u524dPython\u4fa7\u4f9d\u8d56\u4e8eMindX\u7684Python API\u63a5\u53e3\uff0c\u8be5\u63a5\u53e3\u6682\u53ea\u652f\u6301Python3.9\u3002</p> \u73af\u5883 \u7248\u672c Python 3.9 MindX 3.0.0 <p>\u5728Python3.9\u73af\u5883\u57fa\u7840\u4e0a\uff0c\u4e0b\u8f7dMindX\u7684mxVision SDK\u5b89\u88c5\u5305\uff0c\u53c2\u8003\u6307\u5bfc\u6559\u7a0b\u8fdb\u884c\u5b89\u88c5\uff0c\u4e3b\u8981\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <p><pre><code># \u589e\u52a0\u53ef\u6267\u884c\u6743\u9650\nchmod +x Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run\n# \u6267\u884c\u5b89\u88c5\u547d\u4ee4\uff0c\u5982\u679c\u63d0\u793a\u9700\u6307\u5b9acann\u5305\u8def\u5f84\uff0c\u5219\u589e\u52a0\u53c2\u6570\u5982:--cann-path=/usr/local/Ascend/latest\n./Ascend-mindxsdk-mxvision_{version}_linux-{arch}.run --install\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nsource mxVision/set_env.sh\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c \u5b89\u88c5\u5b8c\u6bd5\u4e4b\u540e\u6d4b\u8bd5\u4e00\u4e0bmindx\u662f\u5426\u53ef\u4ee5\u6b63\u5e38\u5bfc\u5165\uff1a<code>python -c \"import mindx\"</code></p> <p>\u5982\u679c\u63d0\u793a\u627e\u4e0d\u5230mindx\uff0c\u5219\u8f6c\u5230mxVision/python\u76ee\u5f55\u4e0b\uff0c\u5b89\u88c5\u5bf9\u5e94\u7684whl\u5305\uff1a</p> <p><pre><code>cd mxVision/python\npip install *.whl\n</code></pre> \u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\u5219\u65e0\u9700\u6267\u884c\u4e0a\u8ff0\u6b65\u9aa4\u3002</p>"},{"location":"cn/inference/environment/#2-mindspore-lite","title":"2. MindSpore Lite\u63a8\u7406","text":"<p>\u5bf9\u4e8eMindOCR\u7684MindSpore Lite\u63a8\u7406\uff0c\u9700\u8981\u5b89\u88c52.2.0\u6216\u4ee5\u4e0a\u7248\u672c\u7684MindSpore Lite\u7684**\u4e91\u4fa7**\u63a8\u7406\u5de5\u5177\u5305\u3002</p> <p>\u5148\u4e0b\u8f7dAscend\u7248\u7684\u4e91\u4fa7\u7248\u672c\u7684\u63a8\u7406\u5de5\u5177\u5305tar.gz\u6587\u4ef6\uff0c\u4ee5\u53caPython\u63a5\u53e3Wheel\u5305\u3002\u4f8b\u5982\u5bf9\u4e8eAscend\u786c\u4ef6\u5e73\u53f0\uff0cLinux-x86_64\u64cd\u4f5c\u7cfb\u7edf\uff0cPython\u7248\u672c\u4e3a3.7\uff0cmindspore-lite\u7248\u672c\u4e3a2.2.0\uff0c\u5e94\u4e0b\u8f7d\u5e76\u5b89\u88c5\u4ee5\u4e0b\u4e24\u4e2a\u5305</p> <ul> <li> <p>\u63a8\u7406\u5de5\u5177\u5305tar.gz</p> </li> <li> <p>Wheel\u5305</p> </li> </ul> <p>\u5982\u9700\u5176\u5b83\u7248\u672c\u53ef\u53c2\u8003\u7f16\u8bd1\u6559\u7a0b\u3002</p> <p>\u63a8\u7406\u5de5\u5177\u5305\u5b89\u88c5\u65f6\u76f4\u63a5\u89e3\u538b\u5373\u53ef\uff0c\u5e76\u6ce8\u610f\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff1a</p> <p><pre><code>export LITE_HOME=/your_path_to/mindspore-lite\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/runtime/third_party/dnnl:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre> \u5982\u679c\u4f7f\u7528python\u63a5\u53e3\uff0c\u4f7f\u7528pip\u5b89\u88c5\u6240\u9700\u7684whl\u5305 <pre><code>pip install mindspore_lite-{version}-{python_version}-linux_{arch}.whl\n</code></pre></p> <p>\u5982\u679c\u4f7f\u7528C++\u63a5\u53e3\uff0c\u5219\u65e0\u9700\u5b89\u88c5\u3002</p>"},{"location":"cn/inference/inference_quickstart/#mindocr-","title":"MindOCR\u539f\u751f\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb","text":""},{"location":"cn/inference/inference_quickstart/#1-mindocr","title":"1. MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868","text":""},{"location":"cn/inference/inference_quickstart/#11","title":"1.1 \u6587\u672c\u68c0\u6d4b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u8bed\u8a00 \u6570\u636e\u96c6 F-score(%) FPS data shape (NCHW) \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d DBNet MobileNetV3 en IC15 76.96 26.19 (1,3,736,1280) yaml ckpt | mindir ResNet-18 en IC15 81.73 24.04 (1,3,736,1280) yaml ckpt | mindir ResNet-50 en IC15 85.00 21.69 (1,3,736,1280) yaml ckpt | mindir ResNet-50 ch + en 12\u4e2a\u6570\u636e\u96c6 83.41 21.69 (1,3,736,1280) yaml ckpt | mindir DBNet++ ResNet-50 en IC15 86.79 8.46 (1,3,1152,2048) yaml ckpt | mindir ResNet-50 ch + en 12\u4e2a\u6570\u636e\u96c6 84.30 8.46 (1,3,1152,2048) yaml ckpt | mindir EAST ResNet-50 en IC15 86.86 6.72 (1,3,720,1280) yaml ckpt | mindir MobileNetV3 en IC15 75.32 26.77 (1,3,720,1280) yaml ckpt | mindir PSENet ResNet-152 en IC15 82.50 2.52 (1,3,1472,2624) yaml ckpt | mindir ResNet-50 en IC15 81.37 10.16 (1,3,736,1312) yaml ckpt | mindir MobileNetV3 en IC15 70.56 10.38 (1,3,736,1312) yaml ckpt | mindir FCENet ResNet50 en IC15 78.94 14.59 (1,3,736,1280) yaml ckpt | mindir"},{"location":"cn/inference/inference_quickstart/#12","title":"1.2 \u6587\u672c\u8bc6\u522b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u5b57\u5178\u6587\u4ef6 \u6570\u636e\u96c6 Acc(%) FPS data shape (NCHW) \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d CRNN VGG7 Default IC15 66.01 465.64 (1,3,32,100) yaml ckpt | mindir ResNet34_vd Default IC15 69.67 397.29 (1,3,32,100) yaml ckpt | mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) yaml ckpt | mindir SVTR Tiny Default IC15 79.92 338.04 (1,3,64,256) yaml ckpt | mindir Rare ResNet34_vd Default IC15 69.47 273.23 (1,3,32,100) yaml ckpt | mindir ResNet34_vd ch_dict.txt / / / (1,3,32,320) yaml ckpt | mindir RobustScanner ResNet-31 en_dict90.txt IC15 73.71 22.30 (1,3,48,160) yaml ckpt | mindir VisionLAN ResNet-45 Default IC15 80.07 321.37 (1,3,64,256) yaml(LA) ckpt(LA) | mindir(LA)"},{"location":"cn/inference/inference_quickstart/#13","title":"1.3 \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"\u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6570\u636e\u96c6 Acc(%) FPS data shape (NCHW) \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d MobileNetV3 MobileNetV3 / / / (1,3,48,192) yaml ckpt"},{"location":"cn/inference/inference_quickstart/#2-mindocr","title":"2. MindOCR\u63a8\u7406\u6d41\u7a0b","text":"<p><pre><code>graph LR;\n    subgraph Step 1\n        A[ckpt] -- export.py --&gt; B[MindIR]\n    end\n\n    subgraph Step 2\n        B -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    end\n\n    subgraph Step 3\n        C -- input --&gt; D[infer.py];\n    end\n\n    subgraph Step 4\n        D -- outputs --&gt; E[eval_rec.py/eval_det.py];\n    end\n\n    F[images] -- input --&gt; D;</code></pre> \u5982\u4e0a\u56fe\u6240\u793a\uff0c\u63a8\u7406\u6d41\u7a0b\u5206\u4e3a\u4ee5\u4e0b\u51e0\u6b65\uff1a</p> <ol> <li>\u5c06MindOCR\u8bad\u7ec3\u5b8c\u6210\u540e\u7684ckpt\u6a21\u578b\uff0c\u4f7f\u7528<code>tools/export.py</code>\u5bfc\u51fa\u6210MindIR\u6a21\u578b\uff1b</li> <li>\u4e0b\u8f7d\u5e76\u914d\u7f6e\u6a21\u578b\u8f6c\u6362\u5de5\u5177\uff08\u5373**converter_lite**\uff09\uff0c\u4f7f\u7528converter_lite\u5de5\u5177\u5c06MindIR\u6a21\u578b\u5bfc\u51fa\u6210MindSpore Lite MindIR\u6a21\u578b\uff1b</li> <li>\u51c6\u5907\u597dMindSpore Lite MindIR\u6a21\u578b\u548c\u8f93\u5165\u56fe\u7247\u540e\uff0c\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u8fdb\u884c\u63a8\u7406\uff1b</li> <li>\u6839\u636e\u6a21\u578b\u79cd\u7c7b\u4e0d\u540c\uff0c\u4f7f\u7528<code>deploy/eval_utils/eval_det.py</code>\u5bf9\u6587\u672c\u68c0\u6d4b\u7c7b\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u679c\u8fdb\u884c\u8bc4\u4f30\uff0c\u6216\u8005\u4f7f\u7528<code>deploy/eval_utils/eval_rec.py</code>\u5bf9\u6587\u672c\u8bc6\u522b\u7c7b\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u679c\u8fdb\u884c\u8bc4\u4f30\u3002</li> </ol> <p>\u6ce8\u610f\uff1aStep 1\u5728Ascend910\u3001GPU\u6216CPU\u4e0a\u6267\u884c\u3002Step 2,3,4\u5728Ascend310\u6216310P\u4e0a\u6267\u884c\u3002 </p>"},{"location":"cn/inference/inference_quickstart/#3-mindocr","title":"3. MindOCR\u63a8\u7406\u65b9\u6cd5","text":""},{"location":"cn/inference/inference_quickstart/#31","title":"3.1 \u6587\u672c\u68c0\u6d4b","text":"<p>\u4e0b\u9762\u4ee5\u6a21\u578b\u8868\u683c\u4e2d\u7684<code>DBNet ResNet-50 en</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5\uff1a - \u4e0b\u8f7d\u6a21\u578b\u8868\u683c\u4e2d\u7684ckpt\u6587\u4ef6\u5e76\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u51fa\u4e3aMindIR\uff0c\u6216\u8005\u76f4\u63a5\u4ece\u6a21\u578b\u8868\u683c\u4e0b\u8f7d\u5df2\u7ecf\u5bfc\u51fa\u5b8c\u6210\u7684MindIR\u6587\u4ef6:</p> <pre><code>``` shell\n# \u4f7f\u7528\u672c\u5730ckpt\u6587\u4ef6\uff0c\u5bfc\u51fa`DBNet ResNet-50 en` \u6a21\u578b\u7684MindIR\n# \u66f4\u591a\u53c2\u6570\u4f7f\u7528\u8be6\u60c5\uff0c\u8bf7\u6267\u884c `python tools/export.py -h`\npython tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/dbnet.ckpt\n```\n\n\u4e0a\u8ff0\u547d\u4ee4\u4e2d```--model_name_or_config```\u4e3aMindOCR\u4e2d\u7684\u6a21\u578b\u540d\u79f0\u6216\u4f20\u5165yaml\u76ee\u5f55(\u4f8b\u5982```--model_name_or_config configs/rec/crnn/crnn_resnet34.yaml```)\uff1b\n\n```--data_shape 736 1280```\u53c2\u6570\u8868\u660e\u6a21\u578b\u8f93\u5165\u56fe\u7247\u7684\u5927\u5c0f\u4e3a[736, 1280]\uff0c\u6bcf\u4e2aMindOCR\u6a21\u578b\u90fd\u5bf9\u5e94\u7740\u4e00\u4e2a\u56fa\u5b9a\u5bfc\u51fadata shape\uff0c\u8be6\u7ec6\u8bf7\u89c1\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\u7684**data shape**\u5217\uff1b\n\n```--local_ckpt_path /path/to/dbnet.ckpt```\u53c2\u6570\u8868\u660e\u9700\u8981\u5bfc\u51fa\u7684\u6a21\u578b\u6587\u4ef6\u4e3a```/path/to/dbnet.ckpt```\n</code></pre> <ul> <li> <p>\u5728Ascend310\u6216310P\u4e0a\u4f7f\u7528converter_lite\u5de5\u5177\u5c06MindIR\u8f6c\u6362\u4e3aMindSpore Lite MindIR\uff1a</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=MINDIR \\\n    --optimize=ascend_oriented \\\n    --modelFile=dbnet_resnet50-c3a4aa24-fbf95c82.mindir \\\n    --outputFile=dbnet_resnet50_lite\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u4e2d\uff1a</p> <p><code>--fmk=MINDIR</code>\u8868\u660e\u8f93\u5165\u6a21\u578b\u7684\u539f\u59cb\u683c\u5f0f\u4e3aMindIR\uff0c\u540c\u65f6<code>\u2014fmk</code>\u53c2\u6570\u8fd8\u652f\u6301ONNX\u7b49\uff1b</p> <p><code>--saveType=MINDIR</code>\u8868\u660e\u8f93\u51fa\u6a21\u578b\u683c\u5f0f\u4e3aMindIR\u683c\u5f0f\uff1b</p> <p><code>--optimize=ascend_oriented</code>\u8868\u660e\u9488\u5bf9Ascend\u8bbe\u5907\u505a\u4f18\u5316\uff1b</p> <p><code>--modelFile=dbnet_resnet50-c3a4aa24-fbf95c82.mindir</code>\u8868\u660e\u5f53\u524d\u9700\u8981\u8f6c\u6362\u7684\u6a21\u578b\u8def\u5f84\u4e3a<code>dbnet_resnet50-c3a4aa24-fbf95c82.mindir</code>\uff1b</p> <p><code>--outputFile=dbnet_resnet50_lite</code>\u8868\u660e\u8f93\u51fa\u6a21\u578b\u7684\u8def\u5f84\u4e3a<code>dbnet_resnet50_lite</code>\uff0c\u4e0d\u9700\u8981\u52a0.mindir\u540e\u7f00\uff0c\u53ef\u81ea\u52a8\u751f\u6210\uff1b</p> <p>\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>dbnet_resnet50_lite.mindir</code>\u6a21\u578b\u6587\u4ef6;</p> <p>\u4e86\u89e3\u66f4\u591aconverter_lite</p> <p>\u4e86\u89e3\u66f4\u591a\u6a21\u578b\u8f6c\u6362\u6559\u7a0b</p> </li> <li> <p>\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u811a\u672c\u548c<code>dbnet_resnet50_lite.mindir</code>\u6587\u4ef6\u6267\u884c\u63a8\u7406\uff1a</p> <p><pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50_lite.mindir \\\n    --det_model_name_or_config=en_ms_det_dbnet_resnet50 \\\n    --res_save_dir=/path/to/dbnet_resnet50_results\n</code></pre> \u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u53c2\u6570<code>--res_save_dir</code>\u6240\u6307\u76ee\u5f55\u4e0b\u751f\u6210\u9884\u6d4b\u6587\u4ef6<code>det_results.txt</code>\uff1b</p> <p>\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u53ef\u4f7f\u7528<code>--vis_det_save_dir</code>\u53c2\u6570\u8fdb\u884c\u7ed3\u679c\u53ef\u89c6\u5316\u3002</p> <p>\u4e86\u89e3\u66f4\u591ainfer.py\u63a8\u7406\u53c2\u6570</p> </li> <li> <p>\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u7ed3\u679c\uff1a</p> <pre><code>python deploy/eval_utils/eval_det.py \\\n        --gt_path=/path/to/ic15/test_det_gt.txt \\\n        --pred_path=/path/to/dbnet_resnet50_results/det_results.txt\n</code></pre> </li> </ul>"},{"location":"cn/inference/inference_quickstart/#32","title":"3.2 \u6587\u672c\u8bc6\u522b","text":"<p>\u4e0b\u9762\u4ee5\u6a21\u578b\u8868\u683c\u4e2d\u7684<code>CRNN ResNet34_vd en</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5\uff1a</p> <ul> <li> <p>\u4e0b\u8f7d\u6a21\u578b\u8868\u683c\u4e2d\u7684MindIR\u6587\u4ef6\uff1b</p> </li> <li> <p>\u5728Ascend310\u6216310P\u4e0a\u4f7f\u7528converter_lite\u5de5\u5177\u5c06MindIR\u8f6c\u6362\u4e3aMindSpore Lite MindIR\uff1a</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=MINDIR \\\n    --optimize=ascend_oriented \\\n    --modelFile=crnn_resnet34-83f37f07-eb10a0c9.mindir \\\n    --outputFile=crnn_resnet34vd_lite\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>crnn_resnet34vd_lite.mindir</code>\u6a21\u578b\u6587\u4ef6\uff1b</p> <p>converter_lite\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u8bf7\u89c1\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b\u3002</p> <p>\u4e86\u89e3\u66f4\u591aconverter_lite</p> <p>\u4e86\u89e3\u66f4\u591a\u6a21\u578b\u8f6c\u6362\u6559\u7a0b</p> </li> <li> <p>\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u811a\u672c\u548c<code>crnn_resnet34vd_lite.mindir</code>\u6587\u4ef6\u6267\u884c\u63a8\u7406\uff1a</p> <p><pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_word_images \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34vd_lite.mindir \\\n    --rec_model_name_or_config=configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=/path/to/rec_infer_results\n</code></pre> \u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u53c2\u6570<code>--res_save_dir</code>\u6240\u6307\u76ee\u5f55\u4e0b\u751f\u6210\u9884\u6d4b\u6587\u4ef6<code>rec_results.txt</code>\u3002</p> <p>\u4e86\u89e3\u66f4\u591ainfer.py\u63a8\u7406\u53c2\u6570</p> </li> <li> <p>\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u7ed3\u679c\uff1a</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/ic15/rec_gt.txt \\\n    --pred_path=/path/to/rec_infer_results/rec_results.txt\n</code></pre> </li> </ul>"},{"location":"cn/inference/inference_quickstart/#33","title":"3.3 \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"<p>\u4e0b\u9762\u4ee5\u6a21\u578b\u8868\u683c\u4e2d\u7684<code>MobileNet</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5\uff1a</p> <ul> <li>\u4e0b\u8f7d\u6a21\u578b\u8868\u683c\u4e2d\u7684ckpt\u6587\u4ef6\uff1b</li> <li>\u4f7f\u7528<code>export.py</code>\u5de5\u5177\u5c06ckpt\u8f6c\u6362\u4e3amindIR<ul> <li>\u8f6c\u6362\u4e3a\u52a8\u6001mindIR     <pre><code>python tools/export.py \\\n    --model_name_or_config configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --save_dir /path/to/save/cls_mv3 \\\n    --is_dynamic_shape True \\\n    --model_type cls\n</code></pre></li> <li>\u8f6c\u6362\u4e3a\u9759\u6001mindIR     <pre><code>python tools/export.py \\\n    --model_name_or_config configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --save_dir /path/to/save/cls_mv3 \\\n    --is_dynamic_shape False \\\n    --data_shape 48 192\n</code></pre></li> </ul> </li> <li>\u5728Ascend310\u6216310P\u4e0a\u4f7f\u7528converter_lite\u5de5\u5177\u5c06MindIR\u8f6c\u6362\u4e3aMindSpore Lite MindIR\uff1a</li> </ul> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=MINDIR \\\n    --optimize=ascend_oriented \\\n    --modelFile=/path/to/save/cls_mv3.mindir \\\n    --outputFile=cls_mv3_lite\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>cls_mv3_lite.mindir</code>\u6a21\u578b\u6587\u4ef6\uff1b</p> <p>\u4e86\u89e3\u66f4\u591aconverter_lite</p> <p>\u4e86\u89e3\u66f4\u591a\u6a21\u578b\u8f6c\u6362\u6559\u7a0b</p>"},{"location":"cn/inference/inference_quickstart/#34","title":"3.4 \u7aef\u5230\u7aef\u63a8\u7406","text":"<p>\u6839\u636e\u6587\u672c\u68c0\u6d4b, \u6587\u672c\u8bc6\u522b, \u6587\u5b57\u65b9\u5411\u8bc6\u522b, \u51c6\u5907\u597d\u7528\u4e8e\u63a8\u7406\u7684mindIR\u6587\u4ef6\u3002\u6267\u884c\u4e0b\u5217\u547d\u4ee4\u8fdb\u884c\u7aef\u5230\u7aef\u63a8\u7406 <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50_lite.mindir \\\n    --det_model_name_or_config=en_ms_det_dbnet_resnet50 \\\n    --cls_model_path=/path/to/mindir/cls_mv3_lite.mindir \\\n    --cls_model_name_or_config=configs/cls/mobilenetv3/cls_mv3.yaml \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34vd_lite.mindir \\\n    --rec_model_name_or_config=configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=/path/to/infer_results\n</code></pre></p>"},{"location":"cn/inference/inference_quickstart/#4","title":"4.\u8f6c\u6362\u3001\u63a8\u7406\u5e38\u89c1\u95ee\u9898","text":"<p>\u8f6c\u6362\u4e0e\u63a8\u7406\u76f8\u5173\u95ee\u9898\u53ef\u53c2\u8003FAQ\u3002</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#-","title":"\u7b2c\u4e09\u65b9\u6a21\u578b\u79bb\u7ebf\u63a8\u7406 - \u5feb\u901f\u5f00\u59cb","text":""},{"location":"cn/inference/inference_thirdparty_quickstart/#1","title":"1. \u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868","text":"<p>MindOCR\u53ef\u4ee5\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\uff08\u5982PaddleOCR\u3001MMOCR\u7b49\uff09\u7684\u63a8\u7406\uff0c\u672c\u6587\u6863\u5c55\u793a\u4e86\u5df2\u9002\u914d\u7684\u6a21\u578b\u5217\u8868\u3002 \u6027\u80fd\u6d4b\u8bd5\u57fa\u4e8eAscend310P\uff0c\u90e8\u5206\u6a21\u578b\u6682\u65e0\u6d4b\u8bd5\u6570\u636e\u96c6\u3002</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#11","title":"1.1 \u6587\u672c\u68c0\u6d4b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6570\u636e\u96c6 F-score(%) FPS \u6765\u6e90 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 ch_pp_det_OCRv4 DBNet MobileNetV3 / / / PaddleOCR yaml infer model ch_PP-OCRv4_det ch_pp_server_det_v2.0 DBNet ResNet18_vd MLT17 46.22 21.65 PaddleOCR yaml infer model ch_ppocr_server_v2.0_det ch_pp_det_OCRv3 DBNet MobileNetV3 MLT17 33.89 22.40 PaddleOCR yaml infer model ch_PP-OCRv3_det ch_pp_det_OCRv2 DBNet MobileNetV3 MLT17 42.99 21.90 PaddleOCR yaml infer model ch_PP-OCRv2_det ch_pp_mobile_det_v2.0_slim DBNet MobileNetV3 MLT17 31.66 19.88 PaddleOCR yaml infer model ch_ppocr_mobile_slim_v2.0_det ch_pp_mobile_det_v2.0 DBNet MobileNetV3 MLT17 31.56 21.96 PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_det en_pp_det_OCRv3 DBNet MobileNetV3 IC15 42.14 55.55 PaddleOCR yaml infer model en_PP-OCRv3_det ml_pp_det_OCRv3 DBNet MobileNetV3 MLT17 66.01 22.48 PaddleOCR yaml infer model ml_PP-OCRv3_det en_pp_det_dbnet_resnet50vd DBNet ResNet50_vd IC15 79.89 21.17 PaddleOCR yaml infer model DBNet en_pp_det_psenet_resnet50vd PSE ResNet50_vd IC15 80.44 7.75 PaddleOCR yaml train model PSE en_pp_det_east_resnet50vd EAST ResNet50_vd IC15 85.58 20.70 PaddleOCR yaml train model EAST en_pp_det_sast_resnet50vd SAST ResNet50_vd IC15 81.77 22.14 PaddleOCR yaml train model SAST en_mm_det_dbnetpp_resnet50 DBNet++ ResNet50 IC15 81.36 10.66 MMOCR yaml train model DBNetpp en_mm_det_fcenet_resnet50 FCENet ResNet50 IC15 83.67 3.34 MMOCR yaml train model FCENet <p>\u6ce8\u610f\uff1a\u5728\u4f7f\u7528en_pp_det_psenet_resnet50vd\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u4fee\u6539onnx\u6587\u4ef6</p> <pre><code>python deploy/models_utils/onnx_optim/insert_pse_postprocess.py \\\n      --model_path=./pse_r50vd.onnx \\\n      --binary_thresh=0.0 \\\n      --scale=1.0\n</code></pre>"},{"location":"cn/inference/inference_thirdparty_quickstart/#12","title":"1.2 \u6587\u672c\u8bc6\u522b","text":"\u540d\u79f0 \u6a21\u578b \u9aa8\u5e72\u7f51\u7edc \u6570\u636e\u96c6 Acc(%) FPS \u6765\u6e90 \u5b57\u5178\u6587\u4ef6 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 ch_pp_rec_OCRv4 CRNN MobileNetV1Enhance / / / PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv4_rec ch_pp_server_rec_v2.0 CRNN ResNet34 MLT17 (ch) 49.91 154.16 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_server_v2.0_rec ch_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (ch) 49.91 408.38 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv3_rec ch_pp_rec_OCRv2 CRNN MobileNetV1Enhance MLT17 (ch) 44.59 203.34 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_PP-OCRv2_rec ch_pp_mobile_rec_v2.0 CRNN MobileNetV3 MLT17 (ch) 24.59 167.67 PaddleOCR ppocr_keys_v1.txt yaml infer model ch_ppocr_mobile_v2.0_rec en_pp_rec_OCRv3 SVTR MobileNetV1Enhance MLT17 (en) 79.79 917.01 PaddleOCR en_dict.txt yaml infer model en_PP-OCRv3_rec en_pp_mobile_rec_number_v2.0_slim CRNN MobileNetV3 / / / PaddleOCR en_dict.txt yaml infer model en_number_mobile_slim_v2.0_rec en_pp_mobile_rec_number_v2.0 CRNN MobileNetV3 / / / PaddleOCR en_dict.txt yaml infer model en_number_mobile_v2.0_rec korean_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR korean_dict.txt yaml infer model korean_PP-OCRv3_rec japan_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR japan_dict.txt yaml infer model japan_PP-OCRv3_rec chinese_cht_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR chinese_cht_dict.txt yaml infer model chinese_cht_PP-OCRv3_rec te_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR te_dict.txt yaml infer model te_PP-OCRv3_rec ka_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR ka_dict.txt yaml infer model ka_PP-OCRv3_rec ta_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR ta_dict.txt yaml infer model ta_PP-OCRv3_rec latin_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR latin_dict.txt yaml infer model latin_PP-OCRv3_rec arabic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR arabic_dict.txt yaml infer model arabic_PP-OCRv3_rec cyrillic_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR cyrillic_dict.txt yaml infer model cyrillic_PP-OCRv3_rec devanagari_pp_rec_OCRv3 SVTR MobileNetV1Enhance / / / PaddleOCR devanagari_dict.txt yaml infer model devanagari_PP-OCRv3_rec en_pp_rec_crnn_resnet34vd CRNN ResNet34_vd IC15 66.35 420.80 PaddleOCR ic15_dict.txt yaml infer model CRNN en_pp_rec_rosetta_resnet34vd Rosetta Resnet34_vd IC15 64.28 552.40 PaddleOCR ic15_dict.txt yaml infer model Rosetta en_pp_rec_vitstr_vitstr ViTSTR ViTSTR IC15 68.42 364.67 PaddleOCR EN_symbol_dict.txt yaml train model ViTSTR en_mm_rec_nrtr_resnet31 NRTR ResNet31 IC15 67.26 32.63 MMOCR english_digits_symbols.txt yaml train model NRTR en_mm_rec_satrn_shallowcnn SATRN ShallowCNN IC15 73.52 32.14 MMOCR english_digits_symbols.txt yaml train model SATRN"},{"location":"cn/inference/inference_thirdparty_quickstart/#13","title":"1.3 \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"\u540d\u79f0 \u6a21\u578b \u6570\u636e\u96c6 Acc(%) FPS \u6765\u6e90 \u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7d \u53c2\u8003\u94fe\u63a5 ch_pp_mobile_cls_v2.0 MobileNetV3 / / / PaddleOCR yaml infer model ch_ppocr_mobile_v2.0_cls"},{"location":"cn/inference/inference_thirdparty_quickstart/#2","title":"2. \u7b2c\u4e09\u65b9\u63a8\u7406\u6d41\u7a0b\u603b\u89c8","text":"<pre><code>graph LR;\n    A[ThirdParty models] -- xx2onnx --&gt; B[ONNX] -- converter_lite --&gt; C[MindIR];\n    C --input --&gt; D[infer.py] -- outputs --&gt; eval_rec.py/eval_det.py;\n    H[images] --input --&gt; D[infer.py];</code></pre>"},{"location":"cn/inference/inference_thirdparty_quickstart/#3","title":"3. \u7b2c\u4e09\u65b9\u6a21\u578b\u63a8\u7406\u65b9\u6cd5","text":""},{"location":"cn/inference/inference_thirdparty_quickstart/#31","title":"3.1 \u6587\u672c\u68c0\u6d4b","text":"<p>\u4e0b\u9762\u4e3b\u8981\u4ee5\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\u7684<code>ch_pp_det_OCRv4</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5\uff1a</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#311","title":"3.1.1 \u4e0b\u8f7d\u7b2c\u4e09\u65b9\u6a21\u578b\u6587\u4ef6","text":"<ul> <li>\u4e0b\u8f7d\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\uff0c<code>infer model</code>\u8868\u793a\u8be5\u6a21\u578b\u4e3a\u63a8\u7406\u6a21\u578b\u6587\u4ef6\uff1b<code>train model</code>\u8868\u793a\u8be5\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b\u6587\u4ef6\uff0c\u9700\u8981\u5148\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b</li> <li>\u5f53\u6a21\u578b\u4e3a\u63a8\u7406\u6a21\u578b\u6587\u4ef6\uff0c\u4ee5<code>ch_pp_det_OCRv4</code>\u4e3a\u4f8b\uff0c\u4eceinfer model\u4e0b\u8f7d\u5e76\u89e3\u538b\uff0c\u5f97\u5230\u4ee5\u4e0b\u76ee\u5f55     <pre><code>ch_PP-OCRv4_det_infer/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre></li> <li>\u5f53\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b\u6587\u4ef6\uff0c\u4ee5<code>en_pp_det_psenet_resnet50vd</code>\u4e3a\u4f8b, \u4ecetrain model\u4e0b\u8f7d\u540e\u89e3\u538b\uff0c\u5f97\u5230\u4ee5\u4e0b\u76ee\u5f55     <pre><code>det_r50_vd_pse_v2.0_train/\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 best_accuracy.pdparams\n</code></pre>     \u9700\u8981\u5148\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\uff1a     <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npython tools/export_model.py \\\n    -c configs/det/det_r50_vd_pse.yml \\\n    -o Global.pretrained_model=./det_r50_vd_pse_v2.0_train/best_accuracy  \\\n    Global.save_inference_dir=./det_db\n</code></pre>     \u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210\u4ee5\u4e0b\u5185\u5bb9\uff1a     <pre><code>det_db/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre></li> </ul>"},{"location":"cn/inference/inference_thirdparty_quickstart/#312-onnx","title":"3.1.2 \u4e09\u65b9\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u4e3aonnx\u6587\u4ef6","text":"<p>\u4e0b\u8f7d\u5e76\u4f7f\u7528<code>paddle2onnx</code>\u5de5\u5177 <pre><code>pip install paddle2onnx\n</code></pre> \u4f7f\u7528\u8be5\u5de5\u5177\uff0c\u5c06\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u6210onnx\u6587\u4ef6\uff1a</p> <p><pre><code>paddle2onnx \\\n    --model_dir det_db \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file det_db.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <code>paddle2onnx</code>\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u5982\u4e0b\uff1a</p> \u53c2\u6570 \u53c2\u6570\u8bf4\u660e --model_dir \u914d\u7f6e\u5305\u542bPaddle\u6a21\u578b\u7684\u76ee\u5f55\u8def\u5f84 --model_filename [\u53ef\u9009] \u914d\u7f6e\u4f4d\u4e8e <code>--model_dir</code> \u4e0b\u5b58\u50a8\u7f51\u7edc\u7ed3\u6784\u7684\u6587\u4ef6\u540d --params_filename [\u53ef\u9009] \u914d\u7f6e\u4f4d\u4e8e <code>--model_dir</code> \u4e0b\u5b58\u50a8\u6a21\u578b\u53c2\u6570\u7684\u6587\u4ef6\u540d\u79f0 --save_file \u6307\u5b9a\u8f6c\u6362\u540e\u7684\u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\u8def\u5f84 --opset_version [\u53ef\u9009] \u914d\u7f6e\u8f6c\u6362\u4e3aONNX\u7684OpSet\u7248\u672c\uff0c\u76ee\u524d\u652f\u6301 7~16 \u7b49\u591a\u4e2a\u7248\u672c\uff0c\u9ed8\u8ba4\u4e3a 9 --input_shape_dict \u8f93\u5165Tensor\u7684\u5f62\u72b6\uff0c\u7528\u4e8e\u751f\u6210\u52a8\u6001ONNX\u6a21\u578b\uff0c\u683c\u5f0f\u4e3a \"{'x':[N,C,H,W]}\"\uff0c-1\u8868\u793a\u52a8\u6001shape --enable_onnx_checker [\u53ef\u9009]  \u914d\u7f6e\u662f\u5426\u68c0\u67e5\u5bfc\u51fa\u4e3a ONNX \u6a21\u578b\u7684\u6b63\u786e\u6027\uff0c \u5efa\u8bae\u6253\u5f00\u6b64\u5f00\u5173\uff0c \u9ed8\u8ba4\u4e3a False <p>\u53c2\u6570\u4e2d<code>--input_shape_dict</code>\u7684\u503c\uff0c\u53ef\u4ee5\u901a\u8fc7Netron\u5de5\u5177\u6253\u5f00\u63a8\u7406\u6a21\u578b\u67e5\u770b\u3002</p> <p>\u4e86\u89e3\u66f4\u591apaddle2onnx</p> <p>\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>det_db.onnx</code>\u6587\u4ef6;</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#313-onnxlite-mindir","title":"3.1.3 \u5c06onnx\u6587\u4ef6\u8f6c\u6362\u4e3aLite MindIR\u6587\u4ef6","text":"<p>\u5728Ascend310/310P\u4e0a\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\u5c06onnx\u6587\u4ef6\u8f6c\u6362\u4e3aMindIR\uff1a</p> <p>\u521b\u5efa\u8f6c\u6362\u914d\u7f6e\u6587\u4ef6<code>config.txt</code>  - \u82e5\u8981\u8f6c\u5316\u4e3a\u9759\u6001shape\u6a21\u578b\uff0c\u4f8b\u5982\u56fa\u5b9a\u8f93\u5165shape\u4e3a<code>[1,3,736,1280]</code>\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,736,1280]\n</code></pre>  - \u82e5\u8981\u8f6c\u5316\u4e3a\u52a8\u6001shape\u5206\u6863\u6a21\u578b\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[736,1280],[768,1280],[896,1280],[1024,1280]\n</code></pre>  - \u82e5\u8981\u8f6c\u5316\u4e3a\u52a8\u6001shape\u6a21\u578b\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></p> <p>\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u5982\u4e0b\uff1a</p> \u53c2\u6570 \u5c5e\u6027 \u529f\u80fd\u63cf\u8ff0 \u53c2\u6570\u7c7b\u578b \u53d6\u503c\u8bf4\u660e input_format \u53ef\u9009 \u6307\u5b9a\u6a21\u578b\u8f93\u5165format String \u53ef\u9009\u6709\"NCHW\"\u3001\"NHWC\"\u3001\"ND\" input_shape \u53ef\u9009 \u6307\u5b9a\u6a21\u578b\u8f93\u5165Shape\uff0cinput_name\u5fc5\u987b\u662f\u8f6c\u6362\u524d\u7684\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u540d\u79f0\uff0c\u6309\u8f93\u5165\u6b21\u5e8f\u6392\u5217\uff0c\u7528\uff1b\u9694\u5f00 String \u4f8b\u5982\uff1a\"input1:[1,64,64,3];input2:[1,256,256,3]\" dynamic_dims \u53ef\u9009 \u5728\u52a8\u6001batch\u5206\u6863\u65f6\u4f7f\u7528\uff0c\u6307\u5b9abatchsize\u548c\u5206\u8fa8\u7387\u6863\u4f4d String \u4f8b\u5982\uff1a\"dynamic_dims=[48,520],[48,320],[48,384]\" <p>\u4e86\u89e3\u66f4\u591a\u914d\u7f6e\u53c2\u6570</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=det_db.onnx \\\n    --outputFile=det_db_lite \\\n    --configFile=config.txt\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>det_db_lite.mindir</code>\u6a21\u578b\u6587\u4ef6;</p> <p><code>converter_lite</code>\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u5982\u4e0b\uff1a</p> \u53c2\u6570 \u662f\u5426\u5fc5\u9009 \u53c2\u6570\u8bf4\u660e \u53d6\u503c\u8303\u56f4 \u9ed8\u8ba4\u503c \u5907\u6ce8 fmk \u662f \u8f93\u5165\u6a21\u578b\u7684\u539f\u59cb\u683c\u5f0f MINDIR\u3001CAFFE\u3001TFLITE\u3001TF\u3001ONNX - - saveType \u5426 \u8bbe\u5b9a\u5bfc\u51fa\u7684\u6a21\u578b\u4e3aMindIR\u6a21\u578b\u6216\u8005ms\u6a21\u578b MINDIR\u3001MINDIR_LITE MINDIR \u4e91\u4fa7\u63a8\u7406\u7248\u672c\u53ea\u6709\u8bbe\u7f6e\u4e3aMINDIR\u8f6c\u51fa\u7684\u6a21\u578b\u624d\u53ef\u4ee5\u63a8\u7406 modelFile \u662f \u8f93\u5165\u6a21\u578b\u7684\u8def\u5f84 - - - outputFile \u662f \u8f93\u51fa\u6a21\u578b\u7684\u8def\u5f84\uff0c\u4e0d\u9700\u52a0\u540e\u7f00\uff0c\u53ef\u81ea\u52a8\u751f\u6210.mindir\u540e\u7f00 - - - configFile \u5426 1\uff09\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u540e\u91cf\u5316\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff1b2\uff09\u53ef\u4f5c\u4e3a\u6269\u5c55\u529f\u80fd\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 - - - optimize \u5426 \u8bbe\u7f6e\u9488\u5bf9\u8bbe\u5907\u7684\u6a21\u578b\u4f18\u5316\u7c7b\u578b\u3002\u82e5\u672a\u8bbe\u7f6e\uff0c\u5219\u4e0d\u505a\u4f18\u5316 none\u3001general\u3001gpu_oriented\u3001ascend_oriented - - <p>\u4e86\u89e3\u66f4\u591aconverter_lite</p> <p>\u4e86\u89e3\u66f4\u591a\u6a21\u578b\u8f6c\u6362\u6559\u7a0b</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#314-lite-mindir","title":"3.1.4 \u7528Lite MindIR\u8fdb\u884c\u63a8\u7406","text":"<p>\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u811a\u672c\u548c<code>det_db_lite.mindir</code>\u6587\u4ef6\u6267\u884c\u63a8\u7406\uff1a</p> <p><pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/det_db_lite.mindir \\\n    --det_model_name_or_config=ch_pp_det_OCRv4 \\\n    --res_save_dir=/path/to/ch_pp_det_OCRv4_results\n</code></pre> \u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u53c2\u6570<code>--res_save_dir</code>\u6240\u6307\u76ee\u5f55\u4e0b\u751f\u6210\u9884\u6d4b\u6587\u4ef6<code>det_results.txt</code>\uff1b</p> <p>\u5728\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u53ef\u4f7f\u7528<code>--vis_det_save_dir</code>\u53c2\u6570\u8fdb\u884c\u7ed3\u679c\u53ef\u89c6\u5316</p> <p>\u4e86\u89e3\u66f4\u591ainfer.py\u63a8\u7406\u53c2\u6570</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#315","title":"3.1.5 \u8bc4\u4f30\u68c0\u6d4b\u7ed3\u679c","text":"<pre><code>python deploy/eval_utils/eval_det.py \\\n    --gt_path=/path/to/ic15/test_det_gt.txt \\\n    --pred_path=/path/to/ch_pp_det_OCRv4_results/det_results.txt\n</code></pre>"},{"location":"cn/inference/inference_thirdparty_quickstart/#32","title":"3.2 \u6587\u672c\u8bc6\u522b","text":"<p>\u4e0b\u9762\u4e3b\u8981\u4ee5\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\u7684<code>ch_pp_rec_OCRv4</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5\uff1a</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#321","title":"3.2.1 \u4e0b\u8f7d\u7b2c\u4e09\u65b9\u6a21\u578b\u6587\u4ef6","text":"<ul> <li>\u4e0b\u8f7d\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\uff0c<code>infer model</code>\u8868\u793a\u8be5\u6a21\u578b\u4e3a\u63a8\u7406\u6a21\u578b\u6587\u4ef6\uff1b<code>train model</code>\u8868\u793a\u8be5\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b\u6587\u4ef6\uff0c\u9700\u8981\u5148\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b</li> <li> <p>\u5f53\u6a21\u578b\u4e3a\u63a8\u7406\u6a21\u578b\u6587\u4ef6\uff0c\u4ee5<code>ch_pp_rec_OCRv4</code>\u4e3a\u4f8b\uff0c\u4eceinfer model\u4e0b\u8f7d\u5e76\u89e3\u538b\uff0c\u5f97\u5230\u4ee5\u4e0b\u76ee\u5f55</p> <pre><code>ch_PP-OCRv4_det_infer/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre> </li> <li> <p>\u5f53\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b\u6587\u4ef6\uff0c\u4ee5<code>en_pp_rec_vitstr_vitstr</code>\u4e3a\u4f8b, \u4ecetrain model\u4e0b\u8f7d\u540e\u89e3\u538b\uff0c\u5f97\u5230\u4ee5\u4e0b\u76ee\u5f55     <pre><code>rec_vitstr_none_ce_train/\n\u251c\u2500\u2500 train.log\n\u251c\u2500\u2500 best_accuracy.pdopt\n\u251c\u2500\u2500 best_accuracy.states\n\u251c\u2500\u2500 best_accuracy.pdparams\n</code></pre>     \u9700\u8981\u5148\u8f6c\u6362\u4e3a\u63a8\u7406\u6a21\u578b\uff1a     <pre><code>git clone https://github.com/PaddlePaddle/PaddleOCR.git\ncd PaddleOCR\npython tools/export_model.py \\\n    -c configs/rec/rec_vitstr_none_ce.yml \\\n    -o Global.pretrained_model=./rec_vitstr_none_ce_train/best_accuracy  \\\n    Global.save_inference_dir=./rec_vitstr\n</code></pre>     \u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210\u4ee5\u4e0b\u5185\u5bb9\uff1a     <pre><code>rec_vitstr/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre></p> </li> </ul>"},{"location":"cn/inference/inference_thirdparty_quickstart/#322-onnx","title":"3.2.2 \u4e09\u65b9\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u4e3aonnx\u6587\u4ef6","text":"<p>\u4e0b\u8f7d\u5e76\u4f7f\u7528<code>paddle2onnx</code>\u5de5\u5177 <pre><code>pip install paddle2onnx\n</code></pre> \u4f7f\u7528\u8be5\u5de5\u5177\uff0c\u5c06\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u6210onnx\u6587\u4ef6\uff1a</p> <pre><code>paddle2onnx \\\n    --model_dir ch_PP-OCRv4_rec_infer \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file rec_crnn.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,48,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> <p>\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>rec_crnn.onnx</code>\u6587\u4ef6</p> <p><code>paddle2onnx</code>\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u8bf7\u89c1\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#323-onnxlite-mindir","title":"3.2.3 \u5c06onnx\u6587\u4ef6\u8f6c\u6362\u4e3aLite MindIR\u6587\u4ef6","text":"<p>\u5728Ascend310/310P\u4e0a\u4f7f\u7528<code>converter_lite</code>\u5de5\u5177\u5c06onnx\u6587\u4ef6\u8f6c\u6362\u4e3aMindIR\uff1a \u521b\u5efa\u8f6c\u6362\u914d\u7f6e\u6587\u4ef6<code>config.txt</code></p> <ul> <li>\u82e5\u8981\u8f6c\u5316\u4e3a**\u9759\u6001shape**\u6a21\u578b\uff0c\u4f8b\u5982\u56fa\u5b9a\u8f93\u5165shape\u4e3a<code>[1,3,48,320]</code>\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,48,320]\n</code></pre></li> <li>\u82e5\u8981\u8f6c\u5316\u4e3a**\u52a8\u6001shape\u5206\u6863**\u6a21\u578b\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=x:[1,3,-1,-1]\ndynamic_dims=[48,520],[48,320],[48,384],[48,360],[48,394],[48,321],[48,336],[48,368],[48,328],[48,685],[48,347]\n</code></pre></li> <li>\u82e5\u8981\u8f6c\u5316\u4e3a**\u52a8\u6001shape**\u6a21\u578b\uff0c\u5219\u53ef\u53c2\u8003\u4ee5\u4e0b\u914d\u7f6e     <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></li> </ul> <p>\u914d\u7f6e\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u8bf7\u89c1\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b\u3002</p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=rec_crnn.onnx \\\n    --outputFile=rec_crnn_lite \\\n    --configFile=config.txt\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>rec_crnn_lite.mindir</code>\u6a21\u578b\u6587\u4ef6\uff1b</p> <p><code>converter_lite</code>\u53c2\u6570\u7b80\u8981\u8bf4\u660e\u8bf7\u89c1\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b\u3002</p> <p>\u4e86\u89e3\u66f4\u591aconverter_lite</p> <p>\u4e86\u89e3\u66f4\u591a\u6a21\u578b\u8f6c\u6362\u6559\u7a0b</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#324","title":"3.2.4 \u4e0b\u8f7d\u8bc6\u522b\u5b57\u5178\u6587\u4ef6","text":"<p>\u6839\u636e\u4e0a\u9762\u7684\u8868\u683c\uff0c\u4e0b\u8f7d<code>ch_pp_rec_OCRv4</code>\u5bf9\u5e94\u7684\u5b57\u5178\u6587\u4ef6ppocr_keys_v1.txt</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#325-lite-mindir","title":"3.2.5 \u7528Lite MindIR\u8fdb\u884c\u63a8\u7406","text":"<p>\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u811a\u672c\u548c<code>rec_crnn_lite.mindir</code>\u6587\u4ef6\u6267\u884c\u63a8\u7406\uff1a</p> <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/mlt17_ch \\\n    --rec_model_path=/path/to/mindir/rec_crnn_lite.mindir \\\n    --rec_model_name_or_config=ch_pp_rec_OCRv4 \\\n    --character_dict_path=/path/to/ppocr_keys_v1.txt \\\n    --res_save_dir=/path/to/ch_rec_infer_results\n</code></pre> <p>\u6267\u884c\u5b8c\u6210\u540e\uff0c\u5728\u53c2\u6570<code>--res_save_dir</code>\u6240\u6307\u76ee\u5f55\u4e0b\u751f\u6210\u9884\u6d4b\u6587\u4ef6<code>rec_results.txt</code>\u3002</p> <p>\u4e86\u89e3\u66f4\u591ainfer.py\u63a8\u7406\u53c2\u6570</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#326","title":"3.2.6 \u8bc4\u4f30\u68c0\u6d4b\u7ed3\u679c","text":"<p>\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u7ed3\u679c\uff1a <pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/mlt17_ch/chinese_gt.txt \\\n    --pred_path=/path/to/en_rec_infer_results/rec_results.txt\n</code></pre> \u5173\u4e8e\u6570\u636e\u96c6\u51c6\u5907\u8bf7\u53c2\u8003\u6570\u636e\u96c6\u8f6c\u6362</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#33","title":"3.3 \u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"<p>\u4e0b\u9762\u4e3b\u8981\u4ee5\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4e2d\u7684<code>ch_pp_mobile_cls_v2</code>\u4e3a\u4f8b\u4ecb\u7ecd\u63a8\u7406\u65b9\u6cd5</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#331","title":"3.3.1 \u4e0b\u8f7d\u7b2c\u4e09\u65b9\u6a21\u578b\u6587\u4ef6","text":"<p>\u53c2\u7167\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b\uff0c\u7531\u4e8e<code>ch_pp_mobile_cls_v2.0</code>\u4e3ainfer model\uff0c\u4e0b\u8f7d\u5e76\u89e3\u538b\u5373\u53ef\u5f97\u5230\u4ee5\u4e0b\u76ee\u5f55</p> <pre><code>ch_ppocr_mobile_v2.0_cls_infer/\n\u251c\u2500\u2500 inference.pdmodel\n\u251c\u2500\u2500 inference.pdiparams\n\u251c\u2500\u2500 inference.pdiparams.info\n</code></pre>"},{"location":"cn/inference/inference_thirdparty_quickstart/#332-onnx","title":"3.3.2 \u4e09\u65b9\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u4e3aonnx\u6587\u4ef6","text":"<p>\u4e0b\u8f7d\u5e76\u4f7f\u7528<code>paddle2onnx</code>\u5de5\u5177 <pre><code>pip install paddle2onnx\n</code></pre> \u4f7f\u7528\u8be5\u5de5\u5177\uff0c\u5c06\u63a8\u7406\u6a21\u578b\u8f6c\u6362\u6210onnx\u6587\u4ef6\uff1a</p> <p><pre><code>paddle2onnx \\\n    --model_dir cls_mv3 \\\n    --model_filename inference.pdmodel \\\n    --params_filename inference.pdiparams \\\n    --save_file cls_mv3.onnx \\\n    --opset_version 11 \\\n    --input_shape_dict=\"{'x':[-1,3,-1,-1]}\" \\\n    --enable_onnx_checker True\n</code></pre> \u8be6\u60c5\u53c2\u8003\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#333-onnxlite-mindir","title":"3.3.3 \u5c06onnx\u6587\u4ef6\u8f6c\u6362\u4e3aLite MindIR\u6587\u4ef6","text":"<p>\u53c2\u8003\u4e0a\u8ff0\u6587\u672c\u68c0\u6d4b\u6837\u4f8b\uff0c\u521b\u5efa\u8f6c\u6362\u914d\u7f6e\u6587\u4ef6<code>config.txt</code>\u3002\u4ee5\u52a8\u6001shape\u6a21\u578b\u4e3a\u4f8b <pre><code>[acl_build_options]\ninput_format=NCHW\ninput_shape_range=x:[-1,3,-1,-1]\n</code></pre></p> <p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>converter_lite \\\n    --saveType=MINDIR \\\n    --fmk=ONNX \\\n    --optimize=ascend_oriented \\\n    --modelFile=cls_mv3.onnx \\\n    --outputFile=cls_mv3_lite \\\n    --configFile=config.txt\n</code></pre> \u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u5b8c\u6210\u540e\u4f1a\u751f\u6210<code>cls_mv3_lite.mindir</code>\u6a21\u578b\u6587\u4ef6</p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#34","title":"3.4 \u7aef\u5230\u7aef\u63a8\u7406","text":"<p>\u6839\u636e\u6587\u672c\u68c0\u6d4b, \u6587\u672c\u8bc6\u522b, \u6587\u5b57\u65b9\u5411\u8bc6\u522b, \u51c6\u5907\u7528\u4e8e\u63a8\u7406\u7684MindIR\u6587\u4ef6\u3002\u6267\u884c\u4e0b\u5217\u547d\u4ee4\u8fdb\u884c\u7aef\u5230\u7aef\u63a8\u7406 <pre><code>python deploy/py_infer/infer.py \\\n    --input_images_dir=/path/to/ic15/ch4_test_images \\\n    --det_model_path=/path/to/mindir/det_db_lite.mindir \\\n    --det_model_name_or_config=ch_pp_det_OCRv4 \\\n    --cls_model_path=/path/to/mindir/cls_mv3_lite.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --rec_model_path=/path/to/mindir/rec_crnn_lite.mindir \\\n    --rec_model_name_or_config=ch_pp_rec_OCRv4 \\\n    --character_dict_path=/path/to/ppocr_keys_v1.txt \\\n    --res_save_dir=/path/to/infer_results\n</code></pre></p>"},{"location":"cn/inference/inference_thirdparty_quickstart/#4-faq","title":"4 FAQ","text":"<p>\u8f6c\u6362\u4e0e\u63a8\u7406\u76f8\u5173\u95ee\u9898\u53ef\u53c2\u8003FAQ</p>"},{"location":"cn/inference/inference_tutorial/#pythonc310ocr","title":"\u57fa\u4e8ePython/C++\u548c\u6607\u817e310\u7684OCR\u63a8\u7406","text":""},{"location":"cn/inference/inference_tutorial/#1","title":"1. \u7b80\u4ecb","text":"<p>MindOCR\u7684\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u91c7\u7528MindSpore Lite\u548cACL\u4e24\u79cd\u63a8\u7406\u540e\u7aef\uff0c \u96c6\u6210\u4e86\u6587\u672c\u68c0\u6d4b\u3001\u89d2\u5ea6\u5206\u7c7b\u548c\u6587\u5b57\u8bc6\u522b\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684OCR\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u5e76\u884c\u5316\u65b9\u5f0f\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002</p> <p>MindOCR Lite\u6574\u4f53\u7684\u63a8\u7406\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>graph LR;\n    A[MindOCR models] -- export --&gt; B[MindIR] -- converter_lite --&gt; C[MindSpore Lite MindIR];\n    D[ThirdParty models] -- xx2onnx --&gt; E[ONNX] -- converter_lite --&gt; C;\n    C --input --&gt; F[MindOCR Infer] -- outputs --&gt; G[Evaluation];\n    H[images] --input --&gt; F[MindOCR Infer];</code></pre>"},{"location":"cn/inference/inference_tutorial/#2","title":"2. \u8fd0\u884c\u73af\u5883","text":"<p>\u8bf7\u53c2\u8003\u8fd0\u884c\u73af\u5883\u51c6\u5907\uff0c\u914d\u7f6eMindOCR\u7684\u63a8\u7406\u8fd0\u884c\u73af\u5883\uff0c\u6ce8\u610f\u7ed3\u5408\u6a21\u578b\u7684\u652f\u6301\u60c5\u51b5\u6765\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/inference/inference_tutorial/#3","title":"3. \u6a21\u578b\u8f6c\u6362","text":"<p>MindOCR\u9664\u4e86\u652f\u6301\u81ea\u8eab\u8bad\u7ec3\u7aef\u5bfc\u51fa\u6a21\u578b\u7684\u63a8\u7406\u5916\uff0c\u8fd8\u652f\u6301\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5217\u8868\u89c1MindOCR\u539f\u751f\u6a21\u578b\u79bb\u7ebf\u63a8\u7406\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u79bb\u7ebf\u63a8\u7406\uff08\u5982PaddleOCR\u3001MMOCR\u7b49\uff09\u3002</p> <p>\u8bf7\u53c2\u8003\u6a21\u578b\u8f6c\u6362\u6559\u7a0b\uff0c\u5c06\u5176\u8f6c\u6362\u4e3aMindOCR\u63a8\u7406\u652f\u6301\u7684\u6a21\u578b\u683c\u5f0f\u3002</p>"},{"location":"cn/inference/inference_tutorial/#4-python","title":"4. \u63a8\u7406 (Python)","text":"<p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u4fa7\u76ee\u5f55\u4e0b\uff1a<code>cd deploy/py_infer</code>.</p>"},{"location":"cn/inference/inference_tutorial/#41","title":"4.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_cls_rec \\\n    --vis_pipeline_save_dir=det_cls_rec\n</code></pre> <p>\u53ef\u89c6\u5316\u56fe\u7247\u5b58\u653e\u5728det_cls_rec\u4e2d,\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> <p> \u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u7ed3\u679c\u53ef\u89c6\u5316 </p> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_182.jpg   [{\"transcription\": \"cocoa\", \"points\": [[14.0, 284.0], [222.0, 274.0], [225.0, 325.0], [17.0, 335.0]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=det_rec \\\n    --vis_pipeline_save_dir=det_rec\n</code></pre> <p>\u53ef\u89c6\u5316\u7ed3\u679c\u5b58\u653e\u5728det_rec\u6587\u4ef6\u5939\u4e2d\uff0c\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> <p> \u6587\u672c\u68c0\u6d4b\u8bc6\u522b\u7ed3\u679c\u53ef\u89c6\u5316 </p> <p>\u8bc6\u522b\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_498.jpg   [{\"transcription\": \"keep\", \"points\": [[819.0, 71.0], [888.0, 67.0], [891.0, 104.0], [822.0, 108.0]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --det_model_path=/path/to/mindir/dbnet_resnet50.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --res_save_dir=det \\\n    --vis_det_save_dir=det\n</code></pre> <p>\u53ef\u89c6\u5316\u7ed3\u679c\u5b58\u653e\u5728det\u4e2d\uff0c\u5982\u56fe\u6240\u793a\uff1a</p> <p> </p> <p> \u6587\u672c\u68c0\u6d4b\u7ed3\u679c\u53ef\u89c6\u5316 </p> <p>\u68c0\u6d4b\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_108.jpg   [[[226.0, 442.0], [402.0, 416.0], [404.0, 433.0], [228.0, 459.0]], [...]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code># cls_mv3.mindir is converted from ppocr\npython infer.py \\\n    --input_images_dir=/path/to/images \\\n    --cls_model_path=/path/to/mindir/cls_mv3.mindir \\\n    --cls_model_name_or_config=ch_pp_mobile_cls_v2.0 \\\n    --res_save_dir=cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre> <ul> <li>\u8bc6\u522b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u5b57\u8bc6\u522b</p> <pre><code>python infer.py \\\n    --input_images_dir=/path/to/images \\\n    --backend=lite \\\n    --rec_model_path=/path/to/mindir/crnn_resnet34.mindir \\\n    --rec_model_name_or_config=../../configs/rec/crnn/crnn_resnet34.yaml \\\n    --res_save_dir=rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728rec/rec_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_421.png   \"under\"\nword_1657.png  \"candy\"\nword_1814.png  \"cathay\"\n</code></pre>"},{"location":"cn/inference/inference_tutorial/#42","title":"4.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str lite \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 precision_mode str \u65e0 \u63a8\u7406\u7684\u7cbe\u5ea6\u6a21\u5f0f\uff0c\u6682\u53ea\u652f\u6301\u5728\u6a21\u578b\u8f6c\u6362\u65f6\u8bbe\u7f6e\uff0c\u6b64\u5904\u4e0d\u751f\u6548 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 vis_det_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_pipeline_save_dir str \u65e0 \u7ed8\u5236\u68c0\u6d4b\u6846\u548c\u6587\u672c\u7684\u56fe\u7247\u4fdd\u5b58\u8def\u5f84 vis_font_path str \u65e0 \u7ed8\u5236\u6587\u5b57\u65f6\u7684\u5b57\u4f53\u8def\u5f84 crop_save_dir str \u65e0 \u6587\u672c\u68c0\u6d4b\u540e\u88c1\u526a\u56fe\u7247\u7684\u4fdd\u5b58\u8def\u5f84 show_log bool False \u662f\u5426\u6253\u5370\u65e5\u5fd7 save_log_dir str \u65e0 \u65e5\u5fd7\u4fdd\u5b58\u6587\u4ef6\u5939 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 det_model_name_or_config str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 cls_model_name_or_config str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 rec_model_name_or_config str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u540d\u79f0\u6216\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199 <p>\u8bf4\u660e\uff1a</p> <p><code>*_model_name_or_config</code>\u53ef\u4ee5\u586b\u6a21\u578b\u540d\u6216YAML\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff0c\u53ef\u53c2\u8003MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868\u548c\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\uff08\u5982PaddleOCR\u3001MMOCR\u7b49\uff09\u3002</p>"},{"location":"cn/inference/inference_tutorial/#5-c","title":"5. \u63a8\u7406 (C++)","text":"<p>\u76ee\u524d\u6682\u65f6\u53ea\u652f\u6301pp-ocr\u7cfb\u5217\u7684\u4e2d\u6587DBNET\u3001CRNN\u3001SVTR\u6a21\u578b\u3002</p> <p>\u8fdb\u5165\u5230MindOCR\u63a8\u7406\u6d4b\u76ee\u5f55\u4e0b <code>cd deploy/cpp_infer</code>,\u6267\u884c\u7f16\u8bd1\u811a\u672c <code>bash build.sh</code>, \u6784\u5efa\u5b8c\u6210\u4e4b\u540e\u5728\u5f53\u524d\u8def\u5f84dist\u76ee\u5f55\u4e0b\u751f\u6210\u53ef\u6267\u884c\u6587\u4ef6infer\u3002</p>"},{"location":"cn/inference/inference_tutorial/#51","title":"5.1 \u547d\u4ee4\u793a\u4f8b","text":"<ul> <li>\u68c0\u6d4b+\u5206\u7c7b+\u8bc6\u522b</li> </ul> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --cls_model_path /path/to/mindir/crnn \\\n    --rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n    --character_dict_path /path/to/ppocr_keys_v1.txt \\\n    --res_save_dir det_cls_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_cls_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b+\u8bc6\u522b</li> </ul> <p>\u4e0d\u4f20\u5165\u65b9\u5411\u5206\u7c7b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5c31\u4f1a\u8df3\u8fc7\u65b9\u5411\u5206\u7c7b\u6d41\u7a0b\uff0c\u53ea\u6267\u884c\u68c0\u6d4b+\u8bc6\u522b</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --rec_model_path /path/to/mindir/crnn_resnet34.mindir \\\n    --character_dict_path /path/to/ppocr_keys_v1.txt \\\n    --res_save_dir det_rec\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det_rec/pipeline_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg   [{\"transcription\": \"spa\", \"points\": [[1114, 35], [1200, 0], [1234, 52], [1148, 97]]}, {...}]\n</code></pre> <ul> <li>\u68c0\u6d4b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u68c0\u6d4b</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --det_model_path /path/to/mindir/dbnet_resnet50.mindir \\\n    --res_save_dir det\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728det/det_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>img_478.jpg    [[[1114, 35], [1200, 0], [1234, 52], [1148, 97]], [...]]]\n</code></pre> <ul> <li>\u5206\u7c7b</li> </ul> <p>\u53ef\u4ee5\u5355\u72ec\u8fd0\u884c\u6587\u672c\u65b9\u5411\u5206\u7c7b</p> <pre><code>./dist/infer \\\n    --input_images_dir /path/to/images \\\n    --backend lite \\\n    --cls_model_path /path/to/mindir/crnn \\\n    --res_save_dir cls\n</code></pre> <p>\u7ed3\u679c\u4fdd\u5b58\u5728cls/cls_results.txt\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>word_867.png   [\"180\", 0.5176]\nword_1679.png  [\"180\", 0.6226]\nword_1189.png  [\"0\", 0.9360]\n</code></pre>"},{"location":"cn/inference/inference_tutorial/#52","title":"5.2 \u8be6\u7ec6\u63a8\u7406\u53c2\u6570\u89e3\u91ca","text":"<ul> <li>\u57fa\u672c\u8bbe\u7f6e</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 input_images_dir str \u65e0 \u5355\u5f20\u56fe\u50cf\u6216\u8005\u56fe\u7247\u6587\u4ef6\u5939 device str Ascend \u63a8\u7406\u8bbe\u5907\u540d\u79f0\uff0c\u652f\u6301\uff1aAscend device_id int 0 \u63a8\u7406\u8bbe\u5907id backend str acl \u63a8\u7406\u540e\u7aef\uff0c\u652f\u6301\uff1aacl, lite parallel_num int 1 \u63a8\u7406\u6d41\u6c34\u7ebf\u4e2d\u6bcf\u4e2a\u8282\u70b9\u5e76\u884c\u6570 <ul> <li>\u7ed3\u679c\u4fdd\u5b58</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 res_save_dir str inference_results \u63a8\u7406\u7ed3\u679c\u7684\u4fdd\u5b58\u8def\u5f84 <ul> <li>\u6587\u672c\u68c0\u6d4b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 det_model_path str \u65e0 \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u65b9\u5411\u5206\u7c7b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 cls_model_path str \u65e0 \u6587\u672c\u65b9\u5411\u5206\u7c7b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 <ul> <li>\u6587\u672c\u8bc6\u522b</li> </ul> \u53c2\u6570\u540d\u79f0 \u7c7b\u578b \u9ed8\u8ba4\u503c \u542b\u4e49 rec_model_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84 character_dict_path str \u65e0 \u6587\u672c\u8bc6\u522b\u6a21\u578b\u5bf9\u5e94\u7684\u8bcd\u5178\u6587\u4ef6\u8def\u5f84\uff0c\u9ed8\u8ba4\u503c\u53ea\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199"},{"location":"cn/inference/model_evaluation/#_1","title":"\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u8bc4\u4f30","text":""},{"location":"cn/inference/model_evaluation/#1","title":"1. \u6587\u672c\u68c0\u6d4b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u68c0\u6d4b\u7ed3\u679c\uff1a <pre><code>python deploy/eval_utils/eval_det.py \\\n    --gt_path=/path/to/det_gt.txt \\\n    --pred_path=/path/to/prediction/det_results.txt\n</code></pre></p>"},{"location":"cn/inference/model_evaluation/#2","title":"2. \u6587\u672c\u8bc6\u522b","text":"<p>\u5b8c\u6210\u63a8\u7406\u540e\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bc4\u4f30\u8bc6\u522b\u7ed3\u679c\uff1a</p> <pre><code>python deploy/eval_utils/eval_rec.py \\\n    --gt_path=/path/to/rec_gt.txt \\\n    --pred_path=/path/to/prediction/rec_results.txt \\\n    --character_dict_path=/path/to/xxx_dict.txt\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0ccharacter_dict_path\u662f\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u5b57\u5178\u4ec5\u652f\u6301\u6570\u5b57\u548c\u82f1\u6587\u5c0f\u5199\u3002</p> <p>\u5728\u8bc4\u4f30PaddleOCR\u6216MMOCR\u7cfb\u5217\u6a21\u578b\u65f6\uff0c\u8bf7\u53c2\u7167\u7b2c\u4e09\u65b9\u6a21\u578b\u652f\u6301\u5217\u8868\u4f7f\u7528\u5bf9\u5e94\u5b57\u5178\u3002</p>"},{"location":"cn/inference/windows_infer/","title":"Windows infer.md","text":""},{"location":"cn/inference/windows_infer/#windows-c","title":"Windows C++\u63a8\u7406","text":""},{"location":"cn/inference/windows_infer/#_1","title":"\u73af\u5883\u914d\u7f6e","text":"<ol> <li>\u4e0b\u8f7dGCC\u5e76\u89e3\u538b\uff1b</li> <li>\u5c06GCC\u89e3\u538b\u540e\u7684\u76ee\u5f55<code>mingw64/bin</code>\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cfPath\u91cc\uff1b</li> <li>\u4e0b\u8f7dCMake\u5b89\u88c5\uff0c\u5728\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u52fe\u9009Add CMake to the system PATH for the current user\uff0c\u5c06cmake\u6dfb\u52a0\u5230Path\u73af\u5883\u53d8\u91cf\uff1a</li> </ol> <ol> <li>\u4e0b\u8f7dMindSpore Lite\u5e76\u89e3\u538b\uff1b</li> <li>\u4e0b\u8f7dopencv-mingw\u9884\u7f16\u8bd13.4.8 x64\u7248\u672c\uff0c\u5e76\u89e3\u538b\uff1b</li> <li>\u5c06OpenCV\u89e3\u538b\u540e\u7684<code>x64/mingw/bin</code>\u8def\u5f84\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cfPath\u91cc\uff1b</li> <li>\u4e0b\u8f7dMindOCR\u4ee3\u7801\u5e76\u89e3\u538b\uff1b</li> <li>\u4e0b\u8f7dClipper\u5e76\u89e3\u538b\uff0c\u5c06<code>cpp</code>\u76ee\u5f55\u4e0b\u7684<code>clipper.cpp</code>\u548c<code>clipper.hpp</code>\u6587\u4ef6\u62f7\u8d1d\u5230MindOCR\u4ee3\u7801\u76ee\u5f55<code>deploy/cpp_infer_ddl/src/data_process/postprocess</code>\u91cc:</li> </ol> <ol> <li>\u6253\u5f00\u4e0b\u8f7d\u9875\u9762\uff1a</li> </ol> <p>\u4e0b\u8f7d\u6570\u636e\u96c6ic15.zip\uff0c\u6587\u672c\u68c0\u6d4b\u6a21\u578b\uff0c\u6587\u672c\u8bc6\u522b\u6a21\u578b\u4ee5\u53ca\u5b57\u5178\u6587\u4ef6\u3002 </p>"},{"location":"cn/inference/windows_infer/#_2","title":"\u63a8\u7406\u65b9\u6cd5","text":"<ol> <li> <p>\u8fdb\u5165MindOCR\u4ee3\u7801\u76ee\u5f55<code>deploy/cpp_infer_ddl/src/</code>\uff0c\u4fee\u6539<code>build.bat</code>\u4e2d\u7684MindSpore Lite\u8def\u5f84\u4ee5\u53caOpenCV\u8def\u5f84\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a <pre><code>set LITE_HOME=D:\\mindocr_windows\\mindspore-lite-2.0.0-win-x64\nset OPENCV_DIR=D:\\mindocr_windows\\OpenCV-MinGW-Build-OpenCV-3.4.8-x64\n</code></pre> \u6ce8\u610f\uff1a\u5728\u4fee\u6539MindSpore_lite\u8def\u5f84<code>LITE_HOME</code>\u548cOpenCV\u8def\u5f84<code>OPENCV_DIR</code>\u65f6\uff0c\u9700\u8981\u5199\u6210\u4e0a\u8ff0\u6837\u4f8b\u7684\u53cd\u659c\u6760<code>\\</code>\u5199\u6cd5\u3002</p> </li> <li> <p>\u8fd0\u884c<code>build.bat</code>\u6587\u4ef6\uff08\u53cc\u51fb\u6253\u5f00\u6216\u8005\u547d\u4ee4\u884c\u91cc\u8f93\u5165<code>build.bat</code>\u5e76\u56de\u8f66\uff09\uff0c\u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728<code>deploy/cpp_infer_ddl/src/dist</code>\u76ee\u5f55\u4e0b\u4f1a\u751f\u6210<code>infer.exe</code>\u6587\u4ef6\uff1b</p> </li> <li> <p>build\u5b8c\u6210\u540e\u4f7f\u7528<code>deploy/cpp_infer_ddl/src/infer.bat</code>\u8fdb\u884c\u63a8\u7406\uff0c\u6ce8\u610f\u4fee\u6539infer.bat\u91cc\u7684\u4ee5\u4e0b\u53c2\u6570\uff1a <pre><code>LITE_HOME=D:/mindocr_windows/mindspore-lite-2.0.0-win-x64 # mindspore lite\u8def\u5f84\n\nOPENCV_DIR=D:/mindocr_windows/OpenCV-MinGW-Build-OpenCV-3.4.8-x64 # OpenCV\u8def\u5f84\n\n--input_images_dir D:\\ic15\\det\\test\\ch4_test_images # \u6d4b\u8bd5\u56fe\u7247\u76ee\u5f55\n--det_model_path D:\\models\\ch_ppocr_server_v2.0_det_infer_cpu.ms # \u6587\u672c\u68c0\u6d4b\u6a21\u578b\u76ee\u5f55\n--rec_model_path D:\\models\\ch_ppocr_server_v2.0_rec_infer_argmax_cpu.ms # \u6587\u672c\u8bc6\u522b\u6a21\u578b\u76ee\u5f55\n--character_dict_path D:\\dict\\ic15_dict.txt # \u5b57\u5178\u6587\u4ef6\u76ee\u5f55\n</code></pre></p> </li> </ol> <p>\u6ce8\u610f: <code>LITE_HOME</code>\u548c<code>OPENCV_DIR</code>\u9700\u8981\u8bbe\u7f6e\u6210\u6b63\u659c\u6760<code>/</code>\u5199\u6cd5\uff0cinfer.exe\u91cc\u9762\u7684\u8def\u5f84\u53c2\u6570\u90fd\u9700\u8981\u8bbe\u7f6e\u6210\u53cd\u659c\u6760<code>\\</code>\uff0c\u4e0e\u5982\u4e0a\u6837\u4f8b\u4fdd\u6301\u4e00\u81f4\u3002</p> <ol> <li>\u5728<code>deploy/cpp_infer_ddl/src/</code>\u76ee\u5f55\u4e2d\uff0c\u6253\u5f00cmd\u7ec8\u7aef\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6267\u884c\u63a8\u7406: <pre><code>infer.bat\n</code></pre></li> <li>\u63a8\u7406\u7ed3\u679c\u5b58\u5728<code>deploy/cpp_infer_ddl/src/dist/det_rec</code>\u76ee\u5f55\u4e0b\uff1b</li> </ol>"},{"location":"cn/mkdocs/modelzoo_training/#-mindocr","title":"\u8bad\u7ec3 - MindOCR\u6a21\u578b\u652f\u6301\u5217\u8868","text":""},{"location":"cn/mkdocs/modelzoo_training/#_1","title":"\u6587\u672c\u68c0\u6d4b","text":"model dataset bs cards F-score ms/step fps amp config dbnet_mobilenetv3 icdar2015 10 1 77.28 100 100 O0 mindocr_dbnet dbnet_resnet18 icdar2015 20 1 81.73 186 108 O0 mindocr_dbnet dbnet_resnet50 icdar2015 10 1 85.05 133 75.2 O0 mindocr_dbnet dbnet++_resnet50 icdar2015 32 1 86.74 571 56 O0 mindocr_dbnet++ psenet_resnet152 icdar2015 8 8 82.06 769.6 83.16 O0 mindocr_psenet psenet_resnet50 icdar2015 8 8 81.37 304.138 210.43 O0 mindocr_psenet psenet_mobilenetv3 icdar2015 8 8 70.56 173.604 368.66 O0 mindocr_psenet east_mobilenetv3 icdar2015 20 8 75.32 138 1185 O0 mindocr_east east_resnet50 icdar2015 20 8 84.87 256 625 O0 mindocr_east fcenet_resnet50 icdar2015 8 4 84.12 2978.65 10.36 O0 mindocr_fcenet"},{"location":"cn/mkdocs/modelzoo_training/#_2","title":"\u6587\u672c\u8bc6\u522b","text":"model dataset bs cards acc ms/step fps amp config svtr_tiny IC03,13,15,IIIT,etc 512 4 90.23 459 4560 O2 mindocr_svtr crnn_vgg7 IC03,13,15,IIIT,etc 16 8 82.03 22.06 5802.71 O3 mindocr_crnn crnn_resnet34_vd IC03,13,15,IIIT,etc 64 8 84.45 76.48 6694.84 O3 mindocr_crnn rare_resnet34_vd IC03,13,15,IIIT,etc 512 4 85.19 449 4561 O2 mindocr_rare visionlan_resnet45 IC03,13,15,IIIT,etc 192 4 90.61 417 1840 O2 mindocr_visionlan master_resnet31 IC03,13,15,IIIT,etc 512 4 90.37 747 2741 O2 mindocr_master robustscanner_resnet31 IC13,15,IIIT,SVT,etc 256 4 87.86 825 310 O0 mindocr_robustscanner abinet_resnet45 IC03,13,15,IIIT,etc 768 8 91.35 718 628.11 O0 mindocr_abinet"},{"location":"cn/mkdocs/modelzoo_training/#_3","title":"\u6587\u672c\u65b9\u5411\u5206\u7c7b","text":"model dataset bs cards acc ms/step fps amp config mobilenetv3 RCTW17,MTWI,LSVT 256 4 94.59 172.9 5923.5 O0 mindocr_mobilenetv3"},{"location":"cn/reference/api_doc/","title":"API doc","text":"<p>\u656c\u8bf7\u671f\u5f85...</p>"},{"location":"cn/tutorials/advanced_train/","title":"\u8fdb\u9636\u8bad\u7ec3\u7b56\u7565","text":""},{"location":"cn/tutorials/advanced_train/#ema","title":"\u7b56\u7565\uff1a\u68af\u5ea6\u7d2f\u79ef\uff0c\u68af\u5ea6\u88c1\u526a\uff0cEMA","text":"<p>\u8bad\u7ec3\u7b56\u7565\u53ef\u5728\u6a21\u578bYAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u914d\u7f6e\u3002\u8bf7\u5728\u8bbe\u7f6e\u540e\u8fd0\u884c<code>tools/train.py</code>\u811a\u672c\u8fdb\u884c\u8bad\u7ec3</p> <p>Yaml\u914d\u7f6e\u6587\u4ef6\u53c2\u8003\u6837\u4f8b</p> <pre><code>train:\n  gradient_accumulation_steps: 2\n  clip_grad: True\n  clip_norm: 5.0\n  ema: True\n  ema_decay: 0.9999\n</code></pre>"},{"location":"cn/tutorials/advanced_train/#_2","title":"\u68af\u5ea6\u7d2f\u79ef","text":"<p>\u68af\u5ea6\u7d2f\u79ef\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u663e\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u5728\u540c\u7b49\u663e\u5b58\uff0c\u5141\u8bb8**\u4f7f\u7528\u66f4\u5927\u7684\u5168\u5c40batch size\u8fdb\u884c\u8bad\u7ec3**\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u5c06<code>train.gradient_accumulation_steps</code> \u8bbe\u7f6e\u4e3a\u5927\u4e8e1\u7684\u503c\u6765\u542f\u7528\u68af\u5ea6\u7d2f\u79ef\u529f\u80fd\u3002 \u7b49\u4ef7\u7684\u5168\u5c40batch size\u4e3a\uff1a</p> <p><code>global_batch_size = batch_size * num_devices * gradient_accumulation_steps</code></p>"},{"location":"cn/tutorials/advanced_train/#_3","title":"\u68af\u5ea6\u88c1\u526a","text":"<p>\u68af\u5ea6\u88c1\u526a\u901a\u5e38\u7528\u6765\u7f13\u89e3\u68af\u5ea6\u7206\u70b8/\u6ea2\u51fa\u95ee\u9898\uff0c\u4ee5\u4f7f\u6a21\u578b\u6536\u655b\u66f4\u7a33\u5b9a\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train.clip_grad</code>\u4e3a<code>True</code>\u6765\u542f\u7528\u8be5\u529f\u80fd\uff0c\u8c03\u6574<code>train.clip_norm</code>\u7684\u503c\u53ef\u4ee5\u63a7\u5236\u68af\u5ea6\u88c1\u526a\u8303\u6570\u7684\u5927\u5c0f\u3002</p>"},{"location":"cn/tutorials/advanced_train/#ema_1","title":"EMA","text":"<p>Exponential Moving Average\uff08EMA\uff09\u662f\u4e00\u79cd\u5e73\u6ed1\u6a21\u578b\u6743\u91cd\u7684\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u3002\u5b83\u80fd\u5e2e\u52a9\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u7a33\u5b9a\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u4f1a\u5e26\u6765\u66f4\u597d\u7684\u6a21\u578b\u6027\u80fd\u3002 \u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>train.ema</code>\u4e3a<code>True</code>\u6765\u4f7f\u7528\u8be5\u529f\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u8c03\u6574<code>train.ema_decay</code>\u6765\u63a7\u5236\u6743\u91cd\u8870\u51cf\u7387\uff0c\u901a\u5e38\u8bbe\u7f6e\u4e3a\u63a5\u8fd11\u7684\u503c.</p>"},{"location":"cn/tutorials/advanced_train/#_4","title":"\u65ad\u70b9\u7eed\u8bad","text":"<p>\u65ad\u70b9\u7eed\u8bad\u901a\u5e38\u7528\u4e8e\u8bad\u7ec3\u610f\u5916\u4e2d\u65ad\u65f6\uff0c\u6b64\u65f6\u4f7f\u7528\u8be5\u529f\u80fd\u53ef\u4ee5\u7ee7\u7eed\u4ece\u4e2d\u65ad\u5904epoch\u7ee7\u7eed\u8bad\u7ec3\u3002\u53ef\u4ee5\u901a\u8fc7\u5728yaml\u914d\u7f6e\u4e2d\u8bbe\u7f6e<code>model.resume</code>\u4e3a<code>True</code>\u6765\u4f7f\u7528\u8be5\u529f\u80fd\uff0c\u7528\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>model:\n  resume: True\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u4ece<code>train.ckpt_save_dir</code>\u76ee\u5f55\u4e2d\u4fdd\u5b58\u7684<code>train_resume.ckpt</code>\u6062\u590d\u3002</p> <p>\u5982\u679c\u8981\u4f7f\u7528\u5176\u4ed6epoch\u7528\u4e8e\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u5728<code>resume</code>\u4e2d\u6307\u5b9aepoch\u8def\u5f84\uff0c\u7528\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>model:\n  resume: /some/path/to/train_resume.ckpt\n</code></pre>"},{"location":"cn/tutorials/advanced_train/#openi","title":"OpenI\u4e91\u5e73\u53f0\u8bad\u7ec3","text":"<p>\u8bf7\u53c2\u8003MindOCR\u4e91\u4e0a\u8bad\u7ec3\u5feb\u901f\u5165\u95e8</p>"},{"location":"cn/tutorials/distribute_train/","title":"\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u7684\u6559\u7a0b\uff0c\u5728Ascend\u5904\u7406\u5668\u4e0a\u6709\u4e24\u79cd\u65b9\u5f0f\u53ef\u4ee5\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\uff0c\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u6216\u901a\u8fc7\u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002\u5728GPU\u5904\u7406\u5668\u4e0a\u53ef\u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u5355\u673a\u591a\u5361\u8bad\u7ec3\u3002</p> <p>\u8bf7\u786e\u4fdd\u5728\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e4b\u524d\uff0c\u5c06 <code>yaml</code> \u6587\u4ef6\u4e2d\u7684 <code>distribute</code> \u53c2\u6570\u8bbe\u7f6e\u4e3a <code>True</code>\u3002</p> <ul> <li>\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3</li> <li>1. Ascend<ul> <li>1.1 \u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3</li> <li>1.2 \u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u8bad\u7ec3</li> <li>1.2.1 \u4f7f\u7528\u516b\u4e2a\uff08\u5168\u90e8\uff09\u8bbe\u5907\u8fdb\u884c\u8bad\u7ec3</li> <li>1.2.2 \u4f7f\u7528\u56db\u4e2a\uff08\u90e8\u5206\uff09\u8bbe\u5907\u8fdb\u884c\u8bad\u7ec3</li> </ul> </li> <li>2. GPU<ul> <li>2.1 \u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3</li> </ul> </li> </ul>"},{"location":"cn/tutorials/distribute_train/#1-ascend","title":"1. Ascend","text":"<p>\u6ce8:</p> <p>\u5728Ascend\u5e73\u53f0\uff0c \u6709\u4e00\u4e9b\u5e38\u89c1\u7684\u4f7f\u7528\u9650\u5236\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <ul> <li> <p>\u5355\u673a\u573a\u666f\u4e0b\u652f\u63011\u30012\u30014\u30018\u5361\u8bbe\u5907\u96c6\u7fa4\uff0c\u591a\u673a\u573a\u666f\u4e0b\u652f\u63018*N\u5361\u8bbe\u5907\u96c6\u7fa4\u3002</p> </li> <li> <p>\u6bcf\u53f0\u673a\u5668\u76840-3\u5361\u548c4-7\u5361\u5404\u4e3a1\u4e2a\u7ec4\u7f51\uff0c2\u5361\u548c4\u5361\u8bad\u7ec3\u65f6\u5361\u5fc5\u987b\u76f8\u8fde\u4e14\u4e0d\u652f\u6301\u8de8\u7ec4\u7f51\u521b\u5efa\u96c6\u7fa4\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u56db\u5361\u8bad\u7ec3\u65f6\uff0c\u53ea\u80fd\u9009\u62e9<code>{0, 1, 2, 3}</code> \u6216\u8005 <code>{4, 5, 6, 7}</code>. \u8fdb\u884c\u53cc\u5361\u8bad\u7ec3\u65f6, \u8de8\u7ec4\u7f51\u7684\u8bbe\u5907, \u4f8b\u5982 <code>{0, 4}</code>\u4e0d\u652f\u6301\u521b\u5efa\u96c6\u7fa4\u3002 \u4e0d\u8fc7, \u540c\u4e00\u7ec4\u7f51\u5185\u7684\u8bbe\u5907\uff0c\u4f8b\u5982 <code>{0, 1}</code>\u548c <code>{1, 2}</code>\u652f\u6301\u521b\u5efa\u96c6\u7fa4\u3002</p> </li> </ul>"},{"location":"cn/tutorials/distribute_train/#11-openmpi","title":"1.1 \u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u5728 Ascend \u786c\u4ef6\u5e73\u53f0\u4e0a\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528 <code>OpenMPI</code> \u7684 <code>mpirun</code> \u547d\u4ee4\u6765\u8fd0\u884c <code>n</code> \u4e2a\u8bbe\u5907\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u4f8b\u5982\uff0c\u5728 DBNet Readme \u4e2d\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5728\u8bbe\u5907 <code>0</code> \u548c\u8bbe\u5907 <code>1</code> \u4e0a\u8bad\u7ec3\u6a21\u578b\uff1a</p> <pre><code># n \u4ee3\u8868\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f7f\u7528\u7684NPU\u7684\u6570\u91cf\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c<code>mpirun</code> \u5c06\u4ece\u8bbe\u5907 <code>0</code> \u5f00\u59cb\u6309\u987a\u5e8f\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u8bad\u7ec3\u3002\u4f8b\u5982\uff0c<code>mpirun -n 4 python-command</code> \u5c06\u5728\u56db\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u8bad\u7ec3\uff1a<code>{0, 1, 2, 3}</code>\u3002</p>"},{"location":"cn/tutorials/distribute_train/#12-rank_table_file","title":"1.2 \u914d\u7f6eRANK_TABLE_FILE\u8fdb\u884c\u8bad\u7ec3","text":""},{"location":"cn/tutorials/distribute_train/#121","title":"1.2.1 \u4f7f\u7528\u516b\u4e2a\uff08\u5168\u90e8\uff09\u8bbe\u5907\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u6b64\u79cd\u65b9\u6cd5\u5728\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u524d\u9700\u8981\u521b\u5efajson\u683c\u5f0f\u7684HCCL\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u751f\u6210RANK_TABLE_FILE\u6587\u4ef6\uff0c\u4ee5\u4e0b\u4e3a\u751f\u62108\u5361\u76f8\u5e94\u914d\u7f6e\u6587\u4ef6\u547d\u4ee4\uff0c\u66f4\u5177\u4f53\u4fe1\u606f\u53ca\u76f8\u5e94\u811a\u672c\u53c2\u89c1hccl_tools\u4e2d\u7684\u8bf4\u660e\uff0c <pre><code>python hccl_tools.py --device_num \"[0,8)\"\n</code></pre> \u8f93\u51fa\u4e3a\uff1a <pre><code>hccl_8p_10234567_127.0.0.1.json\n</code></pre> \u5176\u4e2d<code>hccl_8p_10234567_127.0.0.1.json</code>\u4e2d\u5185\u5bb9\u793a\u4f8b\u4e3a\uff1a <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"0\",\n                    \"device_ip\": \"192.168.100.101\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"1\",\n                    \"device_ip\": \"192.168.101.101\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"2\",\n                    \"device_ip\": \"192.168.102.101\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"3\",\n                    \"device_ip\": \"192.168.103.101\",\n                    \"rank_id\": \"3\"\n                },\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"4\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"5\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"6\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"7\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre></p> <p>\u4e4b\u540e\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\uff1a</p> <pre><code>bash ascend8p.sh\n</code></pre> <p>\u4ee5CRNN\u8bad\u7ec3\u4e3a\u4f8b\uff0c\u5176<code>ascend8p.sh</code>\u811a\u672c\u4e3a\uff1a <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=8\nexport RANK_TABLE_FILE=\"./hccl_8p_01234567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$i\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre></p> <p>\u5f53\u9700\u8981\u8bad\u7ec3\u5176\u4ed6\u6a21\u578b\u65f6\uff0c\u53ea\u8981\u5c06\u811a\u672c\u4e2d\u7684yaml config\u6587\u4ef6\u8def\u5f84\u66ff\u6362\u5373\u53ef\uff0c\u5373<code>python -u tools/train.py --config path/to/model_config.yaml</code></p> <p>\u6b64\u65f6\u8bad\u7ec3\u5df2\u7ecf\u5f00\u59cb\uff0c\u53ef\u5728<code>train.log</code>\u4e2d\u67e5\u770b\u8bad\u7ec3\u65e5\u5fd7\u3002</p>"},{"location":"cn/tutorials/distribute_train/#122","title":"1.2.2 \u4f7f\u7528\u56db\u4e2a\uff08\u90e8\u5206\uff09\u8bbe\u5907\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u8981\u5728\u56db\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u8bad\u7ec3\uff0c\u4f8b\u5982\uff0c<code>{4, 5, 6, 7}</code>\uff0c<code>RANK_TABLE_FILE</code>\u548c\u8fd0\u884c\u811a\u672c\u4e0e\u5728\u516b\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u4f7f\u7528\u7684\u6587\u4ef6\u6709\u6240\u4e0d\u540c\u3002</p> <p>\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa <code>rank_table.json</code>\uff1a</p> <pre><code>python hccl_tools.py --device_num \"[4,8)\"\n</code></pre> <p>\u8f93\u51fa\u4e3a\uff1a <pre><code>hccl_4p_4567_127.0.0.1.json\n</code></pre></p> <p>\u5176\u4e2d\uff0c <code>hccl_4p_4567_127.0.0.1.json</code> \u7684\u793a\u4f8b\u4e3a:</p> <pre><code>{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"127.0.0.1\",\n            \"device\": [\n                {\n                    \"device_id\": \"4\",\n                    \"device_ip\": \"192.168.100.100\",\n                    \"rank_id\": \"0\"\n                },\n                {\n                    \"device_id\": \"5\",\n                    \"device_ip\": \"192.168.101.100\",\n                    \"rank_id\": \"1\"\n                },\n                {\n                    \"device_id\": \"6\",\n                    \"device_ip\": \"192.168.102.100\",\n                    \"rank_id\": \"2\"\n                },\n                {\n                    \"device_id\": \"7\",\n                    \"device_ip\": \"192.168.103.100\",\n                    \"rank_id\": \"3\"\n                }\n            ],\n            \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre> <p>\u7136\u540e\u8fd0\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3:</p> <pre><code>bash ascend4p.sh\n</code></pre> <p>\u4ee5CRNN\u8bad\u7ec3\u4e3a\u4f8b\uff0c\u5176<code>ascend4p.sh</code>\u811a\u672c\u4e3a\uff1a</p> <pre><code>#!/bin/bash\nexport DEVICE_NUM=8\nexport RANK_SIZE=4\nexport RANK_TABLE_FILE=\"./hccl_4p_4567_127.0.0.1.json\"\n\nfor ((i = 0; i &lt; ${RANK_SIZE}; i++)); do\n    export DEVICE_ID=$((i+4))\n    export RANK_ID=$i\n    echo \"Launching rank: ${RANK_ID}, device: ${DEVICE_ID}\"\n    if [ $i -eq 0 ]; then\n      echo 'i am 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; ./train.log &amp;\n    else\n      echo 'not 0'\n      python -u tools/train.py --config configs/rec/crnn/crnn_resnet34_zh.yaml &amp;&gt; /dev/null &amp;\n    fi\ndone\n</code></pre> <p>\u6ce8\u610f\uff0c <code>DEVICE_ID</code> \u548c <code>RANK_ID</code> \u7684\u7ec4\u5408\u5173\u7cfb\u5e94\u8be5\u8ddf <code>hccl_4p_4567_127.0.0.1.json</code> \u6587\u4ef6\u4e2d\u76f8\u543b\u5408.</p>"},{"location":"cn/tutorials/distribute_train/#21-openmpi","title":"2.1 \u901a\u8fc7OpenMPI\u8fd0\u884c\u811a\u672c\u8fdb\u884c\u8bad\u7ec3","text":"<p>\u5728 GPU \u786c\u4ef6\u5e73\u53f0\u4e0a\uff0cMindSpore\u4e5f\u652f\u6301\u4f7f\u7528 <code>OpenMPI</code> \u7684 <code>mpirun</code> \u547d\u4ee4\u6765\u8fd0\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u4ee5\u4e0b\u547d\u4ee4\u5c06\u5728 <code>device 0</code>\u548c <code>device 1</code> \u4e0a\u8fd0\u884c\u8bad\u7ec3\u3002</p> <pre><code># n \u4ee3\u8868\u8bad\u7ec3\u4f7f\u7528\u5230\u7684GPU\u6570\u91cf\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> <p>\u5982\u679c\u7528\u6237\u60f3\u5728 <code>device 2</code> \u548c <code>device 3</code> \u4e0a\u8fd0\u884c\u8bad\u7ec3\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u8fd0\u884c\u4e0a\u9762\u7684\u547d\u4ee4\u4e4b\u524d\u8fd0\u884c <code>export CUDA_VISIBLE_DEVICES=2,3</code>\uff0c\u6216\u8005\u76f4\u63a5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code># n \u4ee3\u8868\u8bad\u7ec3\u4f7f\u7528\u5230\u7684GPU\u6570\u91cf\nCUDA_VISIBLE_DEVICES=2,3 mpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre>"},{"location":"cn/tutorials/frequently_asked_questions/","title":"\u5e38\u89c1\u95ee\u9898","text":""},{"location":"cn/tutorials/frequently_asked_questions/#_1","title":"\u5e38\u89c1\u95ee\u9898","text":"<ul> <li>\u672a\u5b9a\u4e49\u7b26\u53f7</li> <li>Ascend so\u5e93\u627e\u4e0d\u5230\u76f8\u5173\u9519\u8bef</li> <li>\u5173\u4e8e\u9519\u8bef\u7801<code>a39999</code></li> <li>\u5173\u4e8e\u9519\u8bef<code>acl open device 0 failed</code></li> <li>windows\u5b89\u88c5mindocr\u4f9d\u8d56\u5931\u8d25\u76f8\u5173\u95ee\u9898</li> <li>\u5173\u4e8e<code>RunTimeError:The device address tpe is wrong</code></li> <li>\u6a21\u578b\u8f6c\u6362\u76f8\u5173\u95ee\u9898</li> <li>\u63a8\u7406\u76f8\u5173\u95ee\u9898</li> <li>DBNet\u8bad\u7ec3\u901f\u7387\u4e0d\u53ca\u9884\u671f</li> <li><code>libgomp-d22c30c5.so.1.0.0</code>\u76f8\u5173\u9519\u8bef</li> <li>\u5f53\u5728lmdb dataset\u4e0a\u8bad\u7ec3abinet\u62a5\u6570\u636e\u7ba1\u9053\u9519\u8bef</li> <li>\u5f53\u5728synthtext\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3dbnet\u62a5\u8fd0\u884c\u65f6\u9519\u8bef</li> </ul>"},{"location":"cn/tutorials/frequently_asked_questions/#q1","title":"Q1 \u672a\u5b9a\u4e49\u7b26\u53f7","text":"<ul> <li><code>undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore\n&gt;&gt;&gt; import mindspore lite\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/_init_.py\", line 26, in &lt;module&gt;\n      from mindspore lite.context import Context\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore_lite/context.py\", line 22, in &lt;module&gt;\n      from mindspore lite.lib import-c lite wrapper\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol:_ZN9mindspore5tracel15GetDebugInfostrERKSt10shared_ptrINS_9DebugInfoEERKSsNS_13SourceLineTipE\n</code></pre> <ul> <li><code>undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv</code></li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\". \"credits\" or \"license\" for more infommation.\n&gt;&gt;&gt; import mindspore_lite\n&gt;&gt;&gt; import mindspore\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/ root/miniconda3/envs/xxx/1ib/python3.7/site-packages/mindspore/_init_.py\", line 18, in &lt;module&gt;\n      from mindspore.run check import run check\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspore/ run_check/_init_.py\", line 17, in &lt;module&gt;\n      from . check_version import check_version_and_env_config\n  File \"/root/miniconda3/envs/xxx/lib/python3.7/site-packages/mindspo re/run_check/check version.py\", line 29, in &lt;module&gt;\n      from mindspore._c_expression \"import MSContext, ms_ctx_param\nImportError: xxxx/mindspore-lite-2.2.0-linux-x64/tools/converter/lib/libmindspore_converter.so: undefined symbol: _ZN9mindspore12label_manage23GetGlobalTraceLabelTypeEv\n</code></pre> <ul> <li><code>undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv</code></li> </ul> <pre><code>[WARNING] LITE(20788, 7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.973 [mindspore/lite/tools/opt imizer/common/fommat_utils.cc:385] ConvertAbstractFommatShape] abstract must be a tensor, but got: ValueAny.\n[WARNING] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 10.858.998 [mindspore/lite/tools/optimizer/common/gllo_utils.cc: 1071] GenTransposeNode] Convertabstract failed for node: args0_nh2nc\n[WARNING] LITE(20788,7f897fO04ff40, converter_lite) :2023-10-19-07:24: 11.035.069 [mindspore/lite/src/extendrt/cxx_api/dlutils.h:124] DLSopen] dlopen /xxx/mindspore/to0ls/converter/lib/libascend pass plugin.so failed, error: /xxx/mindspore/tools/converter/1ib/libmslite_shared lib.s0: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n\n[ERROR] LITE(20788,7f897f04ff40, converter_lite) :2023-10-19-07:24: 11.035.121 [mindspore/lite/tools/converter/adapter/acl/plugin/acl_pass_plugin.cc:86] CreateAclPassInner] DLSopen failed, so path: /xxx/mindspore-1ite-2.2.0.20231019-1inux-x64/tools/converter/lib/1ibascend_pass_plugin.so, ret: dlopen /xxx/mindspore/tools/converter/lib/libascend_pass_plugin.so failed, error: /xxx/mindspore/tools/converter/lib/libmslite shared lib.so: undefined symbol: _ZNK9mindspore6kernel15KernelBuildInfo8TostringEv\n</code></pre> <p>\u4ee5\u4e0a\u7f3a\u5c11\u7b26\u53f7\u95ee\u9898\uff0c\u4e3a<code>mindspore</code> python whl\u5305\u3001<code>mindspore_lite</code> python whl\u5305\u3001<code>mindspore_lite</code>tar\u5305\u4e0d\u5339\u914d\u5bfc\u81f4\u3002\u6839\u636e\u4e0b\u8f7dmindspore \u548c \u4e0b\u8f7dMindSpore Lite\uff0c\u9700\u68c0\u67e5</p> <ul> <li><code>mindspore</code>, <code>mindspore_lite</code>\u662f\u5426\u7248\u672c\u4e00\u81f4\uff0c\u4f8b\u5982\u90fd\u4e3a2.2.0\u7248\u672c\uff1b</li> <li><code>mindspore_lite</code>\u7684whl\u5305\u4e0e<code>mindspore_lite</code>\u7684tar\u5305\u662f\u5426\u7248\u672c\u4e00\u81f4\uff0c\u4f8b\u5982\u90fd\u4e3a2.2.0\u7248\u672c\uff1b</li> <li><code>mindspore_lite</code>\u7684whl\u5305\u4e0e<code>mindspore_lite</code>\u7684tar\u5305\u662f\u5426\u90fd\u4e3a\u4e91\u4fa7\u7248\u672c</li> </ul> <p>\u4f8b\u5982\u5e73\u53f0\u4e3alinux x86_64\u4e0b\u7684\u6607\u817e\u73af\u5883\uff0c\u5982\u4e0b\u5305\u7ec4\u5408\u662f\u5408\u9002\u7684</p> <ul> <li><code>mindspore_lite whl</code>: mindspore_lite-2.2.0-cp37-cp37m-linux_x86_64.whl</li> <li><code>mindspore_lite tar.gz</code>: mindspore-lite-2.2.0-linux-x64.tar.gz</li> <li><code>mindspore whl</code>: mindspore-2.2.0-cp37-cp37m-linux_x86_64.whl</li> </ul>"},{"location":"cn/tutorials/frequently_asked_questions/#q2-ascend-so","title":"Q2 Ascend so\u5e93\u627e\u4e0d\u5230\u76f8\u5173\u9519\u8bef","text":"<ul> <li><code>dlopen mindspore_lite/lib/libascend_kernel_plugin.so</code> \uff0cNo such file or directory</li> </ul> <p>\u9519\u8bef\u4fe1\u606f</p> <pre><code>File \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 95, in warpper\n  return func(*args, **kwargs)\nFile \"/home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n  raise RuntimeError(f\"build from_file failed! Error is {ret.Tostring()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[WARNING] ME(15411,7f07f56be100, python) : 2023-10-16-00:51:42.509.780 [mindspore/lite/src/extend rt/cxx_api/dlutils.h:124] DLSopen]\ndlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python) :2023-10-16-00:51:42.509.877 [mindspo re/lite/src/extendrt/kernel/ascend/plugin/ascend_allocator_plugin.cc:70] Register] DLSopen failed, so path: /home/xxx/miniconda3/envs/ yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_kernel_plugin.so , func name: CreateAclAllocator. err: dlopen /home/xxx/miniconda3/envs/yyy/lib/python3.8/site-packages/mindspore_lite/lib/libascend_ kernel_plugin.so failed, error: libacl_cblas.so: cannot open shared object file: No such file or directory\n[ERROR] ME(15411,7f07f56be100, python):2023-10-16-00:51:42.509.893 [mindspore/lite/src/extendrt/infer_session.cc:66] HandleContext] failed register ascend allocator plugin.\n...\nraise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>\u89e3\u51b3\u65b9\u6cd5</p> <p>\u8be5\u9519\u8bef\u662f<code>mindspore_lite</code>tar\u5305\u4e2d\u7684<code>libascend_kernel plugin.so</code>\u672a\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cf<code>LD_LIBRARY_PATH</code>\u5bfc\u81f4\uff0c\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b</p> <ol> <li> <p>\u67e5\u770b\u662f\u5426\u5b89\u88c5\u4e86<code>mindspore_lite</code>\u7684**\u4e91\u4fa7\u63a8\u7406\u5de5\u5177\u5305**\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u8bf7\u4ece \u5de5\u5177\u5305tar.gz\u3001whl\u5305\u4e0b\u8f7d\u94fe\u63a5\uff0c\u4e0b\u8f7dAscend\u7248\u7684\u4e91\u4fa7\u7248\u672c<code>tar.gz</code>\u5305\u4ee5\u53ca<code>whl</code>\u5305\u5b89\u88c5\uff0c\u8be6\u7ec6\u8bf7\u89c1 mindspore lite \u5b89\u88c5\u3002</p> </li> <li> <p>\u627e\u5230<code>mindspore_lite</code>\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u5982\u8def\u5f84\u4e3a<code>/your_path_to/mindspore-lite</code>\uff0ccd\u5230\u8be5\u76ee\u5f55\u4e0b</p> </li> <li> <p>\u67e5\u627e<code>libascend_kernel_plugin.so</code>\uff0c\u547d\u4ee4\u4e3a<code>find ./ -name libascend_kernel_plugin.so</code>\uff0c\u53ef\u4ee5\u627e\u5230\u8be5so\u7684\u8def\u5f84\u4e3a</p> </li> </ol> <pre><code>./runtime/lib/libascend_kernel_plugin.so\n</code></pre> <ol> <li>\u5c06\u8be5\u8def\u5f84\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cf</li> </ol> <pre><code>export LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LD_LIBRARY_PATH\n</code></pre> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>python -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n[WARNING] ME(60105:13981374421 1776, MainProcess):2023-10-25-08: 14:33.640.411 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\nTraceback (most recent call last):\nFile \"&lt;string&gt;\", line 1, in module&gt;\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/_checkparam.py\", line 1313, in wrapper\n  return func(*args, **kwargs)\nFile \"/xxx/py37/1ib/python3.7/site-packages/mindspore/context.py\", line 1456, in set_context\n  ctx.set_device_target(kwargs['device target'])\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 381, in set_device_target\n  self.set_param(ms_ctx_param.device_target, target)\nFile \"/xxx/py37/lib/python3.7/site-packages/mindspore/context.py\", line 175, in set_param\n  self._context_handle.set_param(param, value)\nRuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. liboptiling.so: cannot open shared object file: No such file or directory\nLoad dynamic library: 1ibmindspore ascend.so.1 failed. liboptiling.so: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n...\n</code></pre> <p>\u8be5\u9519\u8bef\u4e3a<code>liboptiling.so</code>\u672a\u52a0\u5165\u73af\u5883\u53d8\u91cf<code>LD_LIBRARY_PATH</code>\u5bfc\u81f4\uff0c\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b</p> <ol> <li> <p>\u67e5\u770b\u662f\u5426\u5b89\u88c5\u4e86<code>CANN</code>\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\uff0c\u5b89\u88c5CANN\u3002</p> </li> <li> <p>\u627e\u5230<code>CANN</code>\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u5982\u8def\u5f84\u4e3a<code>/your_path_to/cann</code>\uff0ccd\u5230\u8be5\u76ee\u5f55\u4e0b</p> </li> <li> <p>\u67e5\u627e<code>liboptiling.so</code>\uff0c\u547d\u4ee4\u4e3a<code>find ./ -name liboptiling.so</code>\uff0c\u53ef\u4ee5\u627e\u5230\u8be5so\u7684\u8def\u5f84\u4e3a</p> <pre><code>./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64/liboptiling.so\n./CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/minios/aarch64/liboptiling.so\n</code></pre> </li> <li> <p>\u5c06\u8be5\u8def\u5f84\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cf\uff0c\u82e5\u4e3a<code>x86_64</code>\uff0c\u5219</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/opp/built-in/op_impl/ai_core/tbe/op_tiling/lib/linux/x86_64/:$LD_LIBRARY_PATH\n</code></pre> <p>\u5f53\u4e0b\u5217\u4fe1\u606f\u51fa\u73b0\u65f6\uff0c\u63d0\u793a\u5b89\u88c5\u6210\u529f</p> <pre><code>The result of multiplication calculation is correct. MindSpore has been installed on platform [Ascend] successfully!\n</code></pre> </li> </ol> <ul> <li><code>Load dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory</code></li> </ul> <pre><code>RuntimeError: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly. and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_ascend.so.1 failed. libaicpu_ascend_engine.so: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n...\n</code></pre> <p>\u8be5\u9519\u8bef\u4e3a<code>libaicpu_ascend_engine.so</code>\u672a\u52a0\u5165\u73af\u5883\u53d8\u91cf<code>LD_LIBRARY_PATH</code>\u5bfc\u81f4\uff0c\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b</p> <ol> <li> <p>\u67e5\u770b\u662f\u5426\u5b89\u88c5\u4e86<code>CANN</code>\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5\u6607\u817eAI\u5904\u7406\u5668\u914d\u5957\u8f6f\u4ef6\u5305\uff0c\u5b89\u88c5CANN\u3002</p> </li> <li> <p>\u627e\u5230<code>CANN</code>\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u5982\u8def\u5f84\u4e3a<code>/your_path_to/cann</code>\uff0ccd\u5230\u8be5\u76ee\u5f55\u4e0b</p> </li> <li> <p>\u67e5\u627e<code>libaicpu_ascend_engine.so</code>\uff0c\u547d\u4ee4\u4e3a<code>find ./ -name libaicpu_ascend_engine.so</code>\uff0c\u53ef\u4ee5\u627e\u5230\u8be5so\u7684\u8def\u5f84\u4e3a</p> <pre><code>./CANN-7.0/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./CANN-7.0/compiler/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n./latest/x86_64-linux/lib64/plugin/opskernel/libaicpu_ascend_engine.so\n</code></pre> </li> <li> <p>\u5c06\u8def\u5f84\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cf\uff0c\u82e5\u4e3a<code>x86_64</code>\uff0c\u5219</p> <pre><code>export LD_LIBRARY_PATH=$ASCEND_HOME/CANN-7.0/compiler/lib64/plugin/opskernel/:$LD_LIBRARY_PATH\n</code></pre> </li> </ol>"},{"location":"cn/tutorials/frequently_asked_questions/#q3-ascend-error-message-a39999","title":"Q3 Ascend Error Message A39999","text":"<ul> <li>\u9519\u8bef1</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n     TraceBack (most recent call last):\n     Start aicpu executor failed, retCode=0x7020009 devId=0[FUNC :DeviceRetain][FILE: runtime.cc][LINE:2698]\n     check param failed, dev can not be NULL![FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2544]\n     Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain][FILE: runtime.cc][LINE:2571]\n     Check param failed, context can not be null.[FUNC:NewDevice][FILE:api impl.cc][LINE:1899]\n     New device failed, retcode=0x70 10006[FUNC:SetDevice][FILE:api_impL-cc][LINE:1922]\n     rtsetDevice execute failed, reason=[device retain error][FUNC:FuncErrorReason][FILE :error message manage.ccl[LINE:50]\n     open device 0 failed runtime result = 507033.[FUNC: ReportCallError][FILE:log_inner.cpp][LINE:161]\n\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <ul> <li>\u9519\u8bef2</li> </ul> <pre><code>----------------------------------------------------\n- Ascend Error Message:\n----------------------------------------------------\nE39999: Inner Error!\nE39999 tsd client wait response fail, device response code[1]. load aicpu ops package failed, device[O], host pid[5653], error stack:\n[TSDaemon] checksum aicpu package failed, ret=103, [tsd_common.cpp:2242:SaveProcessConfig]17580\nCheck head tag failed, ret=279, [package_worker.cpp:537:VerifyAicpuPackage]2369\nVerify Aicpu package failed, srcPathlIhome/HMHiAiuser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz]..[package_worker.cpp:567:DecompressionAicpuPackage]2369\nDecompression AicpuPackage [/home/HwHiAiUser/aicpu_kernels/vf0_5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [package_worker.cpp:218:LoadAICPUPackageForProcessMode]2369\nLoad aicpu package path[/home/HwHiAiUser/hdcd/device0/] fileName[ 5653_Ascend310P-aicpu_syskernels.tar.gz] failed, [inotify_watcher.cpp:311:HandleEvent]2369\n[TSDaemon] load aicpu ops package failed, device[0], host pid[5653], [tsd_common.cpp:2054:CheckAndHandleTimeout]2374\n[FUNC:WaitRsp][FILE:process_mode_manager.cpp][LINE:270]\n  TraceBack (most recent call last):\n  TsdOpen failed. devId=0, tdt error=31[FUNC:PrintfTsdError] [FILE: runtime.cc][LINE:2060]\n  Start aicpu executor failed, retCode=Ox7020009 devId=0[FUNC:DeviceRetain][FILE: runtime.cc] [LINE:2698]\n  Check param failed, dev can not be NULL! [FUNC:PrimaryContextRetain] [FILE: runtime.cc][LINE:2544]\n  Check param failed, ctx can not be NULL! [FUNC:PrimaryContextRetain J[FILE: runtime.cc][LINE:2571]\n  Check param failed, context can not be null. [FUNC:NewDevice] [FILE:api_impl.cc][LINE: 1893]\n  New device failed, retCode=0x7010006[FUNC:SetDevice] [FILE:api impl.cc][LINE:1916]\n  rtSetDevice execute failed, reason=[device retain errorl[FUNC:FuncErrorReason] [FILE:error_message_manage.cc][LINE:50]\n  open device 0 failed, runtime result = 507033. [FUNC:ReportCalLError][FILE:log_inner.cpp]ILINE:161]\n(Please search \"Ascend Error Message\" at https://www.mindspore.cn for error code description)\n</code></pre> <p>\u53ef\u80fd\u539f\u56e0</p> <ul> <li> <p>\u9a71\u52a8\u7248\u672c\u4e0eCANN\u4e0d\u5339\u914d</p> </li> <li> <p>\u73af\u5883\u53d8\u91cf\u672a\u914d\u7f6e\u6210\u529f\uff0c\u5bfc\u81f4aicpu\u542f\u52a8\u5931\u8d25\uff0c\u5c1d\u8bd5\u5c06\u4e0b\u5217\u9879\u52a0\u5165\u73af\u5883\u53d8\u91cf</p> </li> </ul> <pre><code>export ASCEND_OPP_PATH=${ASCEND_HOME}/latest/opp\nexport ASCEND_AICPU_PATH=${ASCEND_OPP_PATH}/..\n</code></pre>"},{"location":"cn/tutorials/frequently_asked_questions/#q4-acl-open-device-0-failed","title":"Q4 <code>acl open device 0 failed</code>","text":"<p>\u63a8\u7406\u65f6\u53ef\u80fd\u89e6\u53d1<code>acl open device 0 failed</code>\uff0c\u4f8b\u5982</p> <pre><code>benchmark --modelFile=dbnet_mobilenetv3_lite.mindir --device=Ascend --inputShapes='1,3,736,1280' --loopCount=100 - -wammUpLoopCount=10\nModelPath = dbnet_mobilenetv3_lite.mindir\nModelType = MindIR\nInDatapath =\nGroupInfoFile =\nConfigFilepath =\nInDataType = bin\nLoopCount = 100\nDeviceType = Ascend\nAccuracyThreshold = 0.5\nCosineDistanceThreshold = -1.1\nWarmUpLoopCount = 10\nNumThreads = 2  InterOpParallelNum = 1\nFpl16Priority = 0   Enableparal\u00cdel = 0\ncalibDataPath =\nEnableGLTexture = 0\ncpuBindMode = HIGHER CPU\nCalibDataType = FLOAT\nResize Dims: 1 3 736 1280\nstart unified benchmark run\nIERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 : 54.833.515 Imindspore/lite/src/extend rt/kernel/ascend/model/model_infer.cc:59] Init] Acl open device 0 failed.\n[ERROR] ME (26748,7f6c73867fc0,benchmark):2023-10-26-09:51:54.833.573 [mindspore/lite/src/extend rt/kernel/ascend/src/custom_ascend_kernel.cc:141] Init] Model i\nnfer init failed.   [ERROR] ME (26748, 7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.604 [mindspore/lite/src/extendrt/session/single_op_session.cc:198] BuildCustomAscendKernelImpl] kernel init failed CustomAscend\n[ERROR] ME (26748,7f6c73867fc0, benchmark) :2023-10-26-09:51 :54.833.669 [mindspore/li te/src/extendrt/session/single_op_sess ion.cc:220] BuildCustomAscendKernel] Build ascend kernel failed for node: custom_0\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51 : 54.833.699 [mindspore/lite/src/extend rt/session/single_op_session.cc:302] CompileGraph] Failed to Build custom ascend kernel\n[ERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.833.727 [mindspore/lite/s rc/extendrt/cxx_api/model/model_impl.cc:413] BuildByBufferImpl] compile graph failed.\n[ERROR] ME (26748, 7f6c73867fc0, benchmark):2023-10-26-09:51:54.835.590 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1256] CompileGraph] ms_model_.Build failed while running\nIERROR] ME (26748,7f6c73867fc0,benchmark) :2023-10-26-09:51:54.835.627 [mindspore/lite/tools/benchma rk/benchmark_unified_api.cc:1325] RunBenchmark] Compile graph failed.\n[ERROR] ME(26748,7f6c73867fc0, benchmark):2023-10-26-09: 51:54.835.662 [mindspore/lite/tools/benchmark/ run_benchmark.cc :78] RunBenchmark] Run Benchmark dbnet_mobilenetv3_lite.mindi r Failed : -1\nms_model_.Build failed while running Run Benchmark dbnet mobilenetv3 lite.mindir Failed : -1\n</code></pre> <p>\u73af\u5883\u53d8\u91cf\u7f3a\u5c11\u914d\u7f6e<code>acllib</code>\u76f8\u5173\u5e93</p> <pre><code>export NPU_HOST_LIB=$ASCEND_HOME/latest/acllib/lib64/stub\nexport DDK_PATH=$ASCEND_HOME/latest\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/acllib/lib64\nexport ASCEND_AICPU_PATH=$ASCEND_HOME/latest/x86_64-linux\nexport LD_LIBRARY_PATH=$ASCEND_HOME/latest/x86_64-linux/lib64:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"cn/tutorials/frequently_asked_questions/#q5-windowsmindocr","title":"Q5 windows\u5b89\u88c5mindocr\u4f9d\u8d56\u5931\u8d25","text":"<p>windows\u4e0b\u6267\u884c</p> <pre><code>git clone git@gitee.com:mindspore-lab/mindocr.git\ncd mindocr\npip install -e .\n</code></pre> <p>\u62a5<code>lanms</code>\u5b89\u88c5\u5931\u8d25\uff0c\u9519\u8bef\u4fe1\u606f\u4e3a</p> <pre><code>FileNotFoundError: [WinError 2] \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6587\u4ef6\u3002\n</code></pre> <p><code>lanma</code>\u4f3c\u4e4e\u4e0d\u652f\u6301windows\uff0chttps://github.com/argman/EAST/ \u4e0a\u4ecd\u5b58\u5728\u9002\u914dwindows\u7684issue\u3002\u63a8\u8350\u4f7f\u7528linux\u3002 \u7528windows\u53ef\u8003\u8651\u7528<code>lanms-neo</code>\u66ff\u6362<code>lanms</code>\u5e76\u5b89\u88c5\u3002\u5b89\u88c5\u8fc7\u7a0b\u53ef\u80fd\u9047\u5230\u4ee5\u4e0b\u9519\u8bef</p> <pre><code>Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting lanms-neo==1.0.2\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7b/fe/beff7e7e4455cb9f69c5734897ca8552a57f6423b062ec86b2ebc1d79c0d/lanms_neo-1.0.2.tar.gz (39 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: lanms-neo\n  Building wheel for lanms-neo (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Building wheel for lanms-neo (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [10 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build\\lib.win-amd64-cpython-37\n      creating build\\lib.win-amd64-cpython-37\\lanms\n      copying lanms\\__init__.py -&gt; build\\lib.win-amd64-cpython-37\\lanms\n      running build_ext\n      building 'lanms._C' extension\n      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for lanms-neo\nFailed to build lanms-neo\nERROR: Could not build wheels for lanms-neo, which is required to install pyproject.toml-based projects\n</code></pre> <p>\u9700 \u5b89\u88c5\u5f00\u53d1\u5de5\u5177\u3002\u5e76\u6267\u884c<code>pip install lanms-neo</code>\u3002</p> <p>\u5220\u9664<code>requirement.txt</code>\u4e2d\u7684<code>lanms</code>\u4f9d\u8d56\u9879\uff0c\u7ee7\u7eed\u6267\u884c<code>pip install -r requirements.txt</code>\u53ef\u5b8c\u6210\u5b89\u88c5\u3002</p>"},{"location":"cn/tutorials/frequently_asked_questions/#q6-runtimeerror-the-device-address-type-is-wrong-type-name-in-addresscpu-type-name-in-contextascend","title":"Q6 <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code>","text":"<ul> <li>export\u65f6\u89e6\u53d1 <code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code>\uff0c\u4f8b\u5982</li> </ul> <pre><code>[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.141.25 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.143.96 [mindspore/run_check/_check_version.py:460] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(18680:139900608063296,MainProcess):2023-10-31-12:31:20.144.71 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\nTraceback (most recent call last):\n  File \"tools/export.py\", line 173, in &lt;module&gt;\n    export(**vars(args))\n  File \"tools/export.py\", line 73, in export\n    net = build_model(model_cfg, pretrained=True, amp_level=amp_level)\n  File \"/xxx/mindocr/mindocr/models/builder.py\", line 52, in build_model\n    network = create_fn(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 122, in svtr_tiny_ch\n    model = SVTR(model_config)\n  File \"/xxx/mindocr/mindocr/models/rec_svtr.py\", line 26, in __init__\n    BaseModel.__init__(self, config)\n  File \"/xxx/mindocr/mindocr/models/base_model.py\", line 34, in __init__\n    self.backbone = build_backbone(backbone_name, **config.backbone)\n  File \"/xxx/mindocr/mindocr/models/backbones/builder.py\", line 48, in build_backbone\n    backbone = backbone_class(**kwargs)\n  File \"/xxx/mindocr/mindocr/models/backbones/rec_svtr.py\", line 486, in __init__\n    ops.zeros((1, num_patches, embed_dim[0]), ms.float32)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/function/array_func.py\", line 1039, in zeros\n    output = zero_op(size, value)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 314, in __call__\n    return _run_op(self, self.name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\", line 913, in _run_op\n    stub = _pynative_executor.run_op_async(obj, op_name, args)\n  File \"/xxx/py37/lib/python3.7/site-packages/mindspore/common/api.py\", line 1186, in run_op_async\n    return self._executor.run_op_async(*args)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <ul> <li>MindSporeAscend\u6a21\u5f0f\u4e0b\u8fdb\u884c\u8ba1\u7b97\u53d1\u751f\u9519\u8bef\u3002\u89e6\u53d1<code>RuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend</code>\uff0c\u4f8b\u5982</li> </ul> <pre><code>Python 3.7.16 (default, Jan 17 2023, 22:20:44)\n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import mindspore as ms\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.384 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n[WARNING] ME(44720:140507814819648,MainProcess):2023-11-01-03:01:38.884.675 [mindspore/run_check/_check_version.py:466] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n&gt;&gt;&gt; import mindspore.ops as ops\n&gt;&gt;&gt; ms.set_context(device_target=\"Ascend\")\n&gt;&gt;&gt; ms.run_check()\nMindSpore version:  2.2.0.20231025\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n&gt;&gt;&gt; x = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; y = ms.Tensor(np.ones([1,3,3,4]).astype(np.float32))\n&gt;&gt;&gt; print(ops.add(x, y))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/_stub_tensor.py\", line 49, in fun\n    return method(*arg, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 493, in __str__\n    return str(self.asnumpy())\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore/common/tensor.py\", line 964, in asnumpy\n    return Tensor_.asnumpy(self)\nRuntimeError: The device address type is wrong: type name in address:CPU, type name in context:Ascend\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/hardware/ge_device_res_manager.cc:72 AllocateMemory\n</code></pre> <p>\u4e0a\u8ff0\u95ee\u9898\uff0c\u4e3a Ascend 310/Ascend 310P3 \u4e0b\u53ef\u80fd\u4e0d\u652f\u6301MindSporeAscend\u6a21\u5f0f\u4e0b\u7684\u8ba1\u7b97\u3002\u8bf7\u5230\u4e0b\u8f7dmindspore\u786e\u8ba4\u5b89\u88c5\u7684\u7248\u672c\u662f\u5426\u4e0emindspore\u7248\u672c\u548c\u786c\u4ef6\u5e73\u53f0\u914d\u5957\uff0c\u6216\u8fd9\u4f60\u53ef\u4ee5</p> <ul> <li>\u5c06MindSpore\u6a21\u5f0f\u8c03\u4e3aCPU</li> <li>\u4f7f\u7528 MindSpore Lite</li> </ul>"},{"location":"cn/tutorials/frequently_asked_questions/#q7","title":"Q7 \u6a21\u578b\u8f6c\u6362\u76f8\u5173\u95ee\u9898","text":"<ul> <li>\u8c03\u7528<code>converter_lite</code>\u8f6c\u6362\u6a21\u578b\u5230<code>mindir</code>\u7aef\u4fa7\u6a21\u578b\u65f6\uff0c\u62a5<code>SetGraphInputShape] Failed to find input xxx in input_shape yyy:xxxxxxxxxxx</code></li> </ul> <p>\u4f8b\u5982\u5c06 dbnet_resnet50.mindir \u8f6c\u4e3a<code>mindir</code>\u7aef\u4fa7\u6a21\u578b\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8bbe\u7f6e\u4e86<code>config.txt</code>\u4e3a</p> <pre><code>[ascend_context]\ninput_format=NCHW\ninput_shape=args0:[1,3,736,1280]\n</code></pre> <p>\u5e76\u8fd0\u884c</p> <pre><code>converter_lite --saveType=MINDIR --fmk=MINDIR --optimize=ascend_oriented --modelFile=dbnet_resnet50.mindir --outputFile=dbnet_resnet50_lite --configFile=config.txt\n</code></pre> <p>\u62a5\u4ee5\u4e0b\u9519\u8bef</p> <pre><code>[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.385 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:756] SetGraphInputShape] Failed to find input x in input_shape args0:1,3,736,1280\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.416 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:773] ConvertGraphToOm] Failed to set graph input shape\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.427 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.439 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.450 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.461 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.476 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.486 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.555 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.005.564 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.006.118 [mindspore/lite/tools/converter/converter.cc:1029] HandleGraphCommon] Optimize func graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.133 [mindspore/lite/tools/converter/converter.cc:979] Convert] Handle graph failed: -2 NULL pointer returned.\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.150 [mindspore/lite/tools/converter/converter.cc:1166] RunConverter] Convert model failed\n[ERROR] LITE(30860,7f579d3f4f40,converter_lite):2023-11-10-03:19:29.013.163 [mindspore/lite/tools/converter/cxx_api/converter.cc:348] Convert] Convert model failed, ret=NULL pointer returned.\nERROR [mindspore/lite/tools/converter/converter_lite/main.cc:104] main] Convert failed. Ret: NULL pointer returned.\nConvert failed. Ret: NULL pointer returned.\n</code></pre> <p>\u8be5\u95ee\u9898\u53d8\u91cf\u540d\u4e0d\u5339\u914d\u5bfc\u81f4\u3002\u6ce8\u610f\u5230</p> <pre><code>Failed to find input x in input_shape args0:1,3,736,1280\n</code></pre> <p>\u53ef\u77e5<code>config.txt</code>\u4e2d\u8f93\u5165\u53d8\u91cf\u540d<code>args0</code>\u4e0e\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u53d8\u91cf\u540d<code>x</code>\u4e0d\u5339\u914d\u3002\u5c06<code>config.txt</code>\u4e2d<code>args0</code>\u6539\u4e3a<code>x</code>\u5373\u53ef</p> <ul> <li>\u63a8\u7406\u8fc7\u7a0b\u8bef\u7528\u4e91\u4fa7<code>mindir</code>\u6a21\u578b\uff0c\u62a5<code>Save ge model to buffer failed.</code></li> </ul> <p>\u4f8b\u5982\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cdet\u6a21\u578b\u8bef\u7528\u4e86\u4e91\u4fa7<code>mindir</code>\u6a21\u578b\uff0c\u5c06\u629b\u51fa\u4ee5\u4e0b\u9519\u8bef</p> <pre><code>[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.120 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:45.206.157 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.253 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:200] operator()] Save ge model to buffer failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:45.277.292 [mindspore/ccsrc/cxx_api/model/model_converter_utils/multi_process.cc:118] ParentProcess] Parent process process failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.224 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.280 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.307 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.332 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.359 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.388 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.430 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.453 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.673 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43138,7f02bddd9740,python3):2023-11-10-03:40:46.235.698 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.216 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.270 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43138,7f02bddd9740,python3):2023-11-10-03:40:46.238.351 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43138:139649752078144,Process-1:18):2023-11-10-03:40:46.255.926 [src/parallel/framework/module_base.py:38] DetInferNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:18:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_infer_node.py\", line 12, in init_self_args\n    self.text_detector.init(preprocess=False, model=True, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.698 [mindspore/ccsrc/cxx_api/model/acl/model_converter.cc:251] LoadMindIR] Convert MindIR model to OM model failed\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.755 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:781] ConvertGraphToOm] Model converter load mindir failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.782 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:862] BuildGraph] Convert graph  to om failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.807 [mindspore/lite/tools/converter/adapter/acl/src/acl_pass_impl.cc:1320] Run] Build graph failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.834 [mindspore/lite/tools/converter/adapter/acl/acl_pass.cc:42] Run] Acl pass impl run failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.864 [mindspore/lite/tools/converter/anf_transform.cc:472] RunConvertPass] Acl pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.904 [mindspore/lite/tools/converter/anf_transform.cc:660] RunPass] Run convert pass failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.305.928 [mindspore/lite/tools/converter/anf_transform.cc:754] TransformFuncGraph] Proc online transform failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.162 [mindspore/lite/tools/converter/anf_transform.cc:855] Transform] optimizer failed.\n[ERROR] LITE(43123,7f02bddd9740,python3):2023-11-10-03:40:46.306.188 [mindspore/lite/tools/converter/converter_funcgraph.cc:471] Optimize] Transform anf graph failed.\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.599 [mindspore/lite/src/extendrt/convert/runtime_convert.cc:214] RuntimeConvert] Convert model failed\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.646 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:507] ConvertGraphOnline] Failed to converter graph\n[ERROR] ME(43123,7f02bddd9740,python3):2023-11-10-03:40:46.308.712 [mindspore/lite/src/extendrt/cxx_api/model/model_impl.cc:395] BuildByBufferImpl] convert graph failed.\n[ERROR] MINDOCR(43123:139649752078144,Process-1:17):2023-11-10-03:40:46.324.506 [src/parallel/framework/module_base.py:38] DetPreNode init failed: build_from_file failed! Error is Common error code.\nProcess Process-1:17:\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/root/miniconda3/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 39, in process_handler\n    raise error\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 34, in process_handler\n    params = self.init_self_args()\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/detection/det_pre_node.py\", line 13, in init_self_args\n    self.text_detector.init(preprocess=True, model=False, postprocess=False)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_base.py\", line 29, in init\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_det.py\", line 22, in _init_model\n    device_id=self.args.device_id,\n  File \"/home/mindocr/deploy/py_infer/src/core/model/model.py\", line 15, in __init__\n    self.model = _INFER_BACKEND_MAP[backend](**kwargs)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 16, in __init__\n    super().__init__(model_path, device, device_id)\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/model_base.py\", line 28, in __init__\n    self._init_model()\n  File \"/home/mindocr/deploy/py_infer/src/core/model/backend/lite_model.py\", line 33, in _init_model\n    self.model.build_from_file(self.model_path, mslite.ModelType.MINDIR, context)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 95, in warpper\n    return func(*args, **kwargs)\n  File \"/root/miniconda3/envs/py37/lib/python3.7/site-packages/mindspore_lite/model.py\", line 235, in build_from_file\n    raise RuntimeError(f\"build_from_file failed! Error is {ret.ToString()}\")\nRuntimeError: build_from_file failed! Error is Common error code.\n</code></pre> <p>\u53ef\u80fd\u539f\u56e0</p> <ul> <li><code>mindir</code>\u4e91\u4fa7\u6a21\u578b\u5728\u672a\u8f6c\u6362\u4e3a<code>mindir</code>\u7aef\u4fa7\u6a21\u578b\u4f7f\u7528</li> <li><code>converter_lite</code>\u8f6c\u6362\u5de5\u5177\u7248\u672c\u4e0e\u63a8\u7406\u65f6<code>mindspore_lite</code>\u7248\u672c\u4e0d\u4e00\u81f4\u3002\u4f8b\u5982\u7528<code>converter_lite 2.2</code>\u8f6c\u6362\u5f97\u5230\u7684<code>mindir</code>\u7aef\u4fa7\u6a21\u578b\uff0c\u7528\u4e8e<code>mindspore_lite 2.1</code>\u4e0b\u63a8\u7406</li> </ul>"},{"location":"cn/tutorials/frequently_asked_questions/#q8","title":"Q8 \u63a8\u7406\u65f6\u76f8\u5173\u95ee\u9898","text":"<ul> <li>\u4f7f\u7528<code>deploy/py_infer/infer.py</code>\u63a8\u7406\u65f6\uff0c\u62a5<code>TypeError: unhashable type: 'numpy.ndarray'</code>\uff0c\u5177\u4f53\u9519\u8bef\u4e3a</li> </ul> <pre><code>[ERROR] MINDOCR(51913:140354674829120,Process-1:28):2023-11-10-06:52:34.304.673 [src/parallel/framework/module_base.py:66] ERROR occurred in RecPostNode module for test.jpg: unhashable type: 'numpy.ndarray'.\nTraceback (most recent call last):\n  File \"/home/mindocr/deploy/py_infer/src/parallel/framework/module_base.py\", line 62, in call_process\n    self.process(send_data)\n  File \"/home/mindocr/deploy/py_infer/src/parallel/module/recognition/rec_post_node.py\", line 24, in process\n    output = self.text_recognizer.postprocess(data[\"pred\"], batch)\n  File \"/home/mindocr/deploy/py_infer/src/infer/infer_rec.py\", line 132, in postprocess\n    return self.postprocess_ops(pred)\n  File \"/home/mindocr/deploy/py_infer/src/data_process/postprocess/builder.py\", line 32, in __call__\n    return self._ops_func(*args, **kwargs)\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in __call__\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\n  File \"/home/mindocr/mindocr/postprocess/rec_postprocess.py\", line 153, in &lt;listcomp&gt;\n    raw_chars = [[self.character[idx] for idx in pred_indices[b]] for b in range(pred_indices.shape[0])]\nTypeError: unhashable type: 'numpy.ndarray'\n</code></pre> <p>\u8be5\u9519\u8bef\u4e3a\u6a21\u578b\u8f93\u51fa\u7684shape\u6709\u8bef\u3002\u8bf7\u68c0\u67e5\u5982\u4e0b\u4e8b\u9879</p> <ul> <li>\u4f7f\u7528\u6070\u5f53\u7684\u6a21\u578b\u3002\u4f8b\u5982\u5728 <code>--rec_model_path</code> \u9519\u8bef\u4f20\u5165\u4e86\u68c0\u6d4b\u6a21\u578b\uff0c\u53ef\u89e6\u53d1\u6b64\u9519\u8bef\uff1b</li> <li>\u4f7f\u7528\u63a8\u7406\u6a21\u578b\uff08\u975e\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u7528<code>converter_lite</code>\u8f6c\u6362\u5de5\u5177\u8f6c\u4e3a\u7aef\u4fa7<code>mindir</code>\u8fdb\u884c\u63a8\u7406\u3002</li> </ul>"},{"location":"cn/tutorials/frequently_asked_questions/#q9-dbnet","title":"Q9 DBNet\u8bad\u7ec3\u901f\u7387\u4e0d\u53ca\u9884\u671f","text":"<p>\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8bad\u7ec3DBNet\u7cfb\u5217\u7f51\u7edc\uff08\u5305\u62ecDBNet MobileNetV3\u3001DBNet ResNet-18\u3001DBNet ResNet-50\u3001DBNet++ ResNet-50\u7b49\uff09\u65f6\uff0c\u8bad\u7ec3\u5e27\u7387\u4e0d\u53ca\u9884\u671f\u3002\u4f8b\u5982\uff0cDBNet MobileNetV3\u5728Ascend 910A\u4e0a\uff0c\u8bad\u7ec3\u901f\u7387\u4ec580fps\uff0c\u4e0d\u53ca\u9884\u671f\u7684100fps\u3002</p> <pre><code>python tools/train.py -c configs/det/dbnet/db_mobilenetv3_icdar15.yaml\n</code></pre> <p>\u7531\u4e8eDBNet\u6570\u636e\u9884\u5904\u7406\u8fc7\u7a0b\u76f8\u5bf9\u590d\u6742\uff0c\u5982\u8bad\u7ec3\u670d\u52a1\u5668CPU\u5355\u6838\u8fd0\u7b97\u80fd\u529b\u8f83\u5f31\uff0c\u5219\u6570\u636e\u9884\u5904\u7406\u53ef\u80fd\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002</p> <p>\u89e3\u51b3\u65b9\u6cd5</p> <ol> <li> <p>\u5c1d\u8bd5\u5c06\u914d\u7f6e\u6587\u4ef6\u4e2d<code>train.dataset.use_minddata</code>\u548c<code>eval.dataset.use_minddata</code>\u7684\u9009\u9879\u8bbe\u7f6e\u4e3a<code>True</code>\u3002MindOCR\u5c06\u91c7\u7528MindSporeMindData\u6267\u884c\u90e8\u5206\u6570\u636e\u9884\u5904\u7406\u6b65\u9aa4\uff1a</p> <pre><code>...\ntrain:\n  ckpt_save_dir: './tmp_det'\n  dataset_sink_mode: True\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/train/ch4_training_images\n    label_file: ic15/det/train/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- \u8bbe\u7f6e\u8be5\u9009\u9879\n...\neval:\n  ckpt_load_path: tmp_det/best.ckpt\n  dataset_sink_mode: False\n  dataset:\n    type: DetDataset\n    dataset_root: /data/ocr_datasets\n    data_dir: ic15/det/test/ch4_test_images\n    label_file: ic15/det/test/det_gt.txt\n    sample_ratio: 1.0\n    use_minddata: True                          &lt;-- \u8bbe\u7f6e\u8be5\u9009\u9879\n...\n</code></pre> </li> <li> <p>\u5982\u8bad\u7ec3\u670d\u52a1\u5668CPU\u6838\u6570\u8f83\u591a\uff0c\u5c1d\u8bd5\u8c03\u9ad8\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>train.loader.num_workers</code>\u9009\u9879\uff0c\u63d0\u5347\u6570\u636e\u9884\u53d6\u7684\u7ebf\u7a0b\u6570\uff1a</p> <pre><code>...\ntrain:\n  ...\n  loader:\n    shuffle: True\n    batch_size: 10\n    drop_remainder: True\n    num_workers: 12                             &lt;-- \u8bbe\u7f6e\u8be5\u9009\u9879\n...\n</code></pre> </li> </ol>"},{"location":"cn/tutorials/frequently_asked_questions/#q10-libgomp-d22c30c5so100","title":"Q10 <code>libgomp-d22c30c5.so.1.0.0</code>\u76f8\u5173\u9519\u8bef","text":"<p>\u8fd0\u884cmindocr\u65f6\uff0c\u53ef\u80fd\u62a5\u4ee5\u4e0b\u9519\u8bef <pre><code>ImportError: /root/mindocr_env/lib/python3.8/site-packages/sklearn/__check_build/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block\n</code></pre> \u53ef\u4ee5\u5c1d\u8bd5\u4ee5\u4e0b\u6b65\u9aa4  - \u5728python\u5b89\u88c5\u8def\u5f84\u4e0b\u67e5\u627e<code>libgomp-d22c30c5.so.1.0.0</code>:    <pre><code>cd /root/mindocr_env/lib/python3.8\nfind ~ -name libgomp-d22c30c5.so.1.0.0\n</code></pre>    \u5c06\u67e5\u627e\u5230\u4ee5\u4e0b\u7ed3\u679c    <pre><code>/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0\n</code></pre>  - \u5c06so\u6587\u4ef6\u8def\u5f84\u52a0\u5165\u5230\u73af\u5883\u53d8\u91cf<code>LD_PRELOAD</code> <pre><code>export LD_PRELOAD=/root/mindocr_env/lib/python3.8/site-packages/scikit_learn.libs/libgomp-d22c30c5.so.1.0.0:$LD_PRELOAD\n</code></pre></p>"},{"location":"cn/tutorials/frequently_asked_questions/#q11-lmdb-datasetabinet","title":"Q11 \u5f53\u5728lmdb dataset\u4e0a\u8bad\u7ec3abinet\u62a5\u6570\u636e\u7ba1\u9053\u9519\u8bef","text":"<p>\u5f53\u5728lmdb dataset\u4e0a\u8bad\u7ec3abinet\u62a5\u4ee5\u4e0b\u6570\u636e\u7ba1\u9053\u9519\u8bef <pre><code>mindocr.data.rec_lmdb_dataset WARNING - Error occurred during preprocess.\n Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'.\n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message:\n------------------------------------------------------------------\n[ERROR] No cast for the specified DataType was found.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers)\n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/py_func_op.cc(143).\n</code></pre> \u53ef\u4ee5\u5c1d\u8bd5\u7528\u5982\u4e0b\u6b65\u9aa4\u4fee\u590d  - \u627e\u5230mindspore\u7684\u5305\u8def\u5f84  - \u6253\u5f00\u6587\u4ef6: <code>mindspore/dataset/transforms/transform.py</code>  - \u8df3\u8f6c\u523093\u884c\uff0c\u53ef\u4ee5\u5f97\u5230\u5982\u4e0b\u5185\u5bb9:   <pre><code>93        if key in EXECUTORS_LIST:\n94           # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor.UpdateOperation(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>  - \u4f7f\u7528<code>executor = cde.Execute(self.parse())</code>\u66ff\u636297\u884c, \u5f97\u5230\u5982\u4e0b\u5185\u5bb9:   <pre><code>93        if key in EXECUTORS_LIST:\n94            # get the executor by process id and thread id\n95            executor = EXECUTORS_LIST[key]\n96            # remove the old transform which in executor and update the new transform\n97            executor = cde.Execute(self.parse())\n98        else:\n99            # create a new executor by process id and thread_id\n100           executor = cde.Execute(self.parse())\n101           # add the executor the global EXECUTORS_LIST\n102           EXECUTORS_LIST[key] = executor\n</code></pre>   - \u4fdd\u5b58\u540e\u518d\u6b21\u5c1d\u8bd5\u8bad\u7ec3\u5373\u53ef</p>"},{"location":"cn/tutorials/frequently_asked_questions/#q12-synthtextdbnet","title":"Q12 \u5f53\u5728synthtext\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3dbnet\u62a5\u8fd0\u884c\u65f6\u9519\u8bef","text":"<p>\u5f53\u5728synthtext\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3dbnet\u62a5\u4ee5\u4e0b\u6570\u636e\u7ba1\u9053\u9519\u8bef <pre><code>Traceback (most recent call last):\n  ...\n  File \"/root/archiconda3/envs/Python380/lib/python3.8/site-packages/mindspore/common/api.py\", line 1608, in _exec_pip\n    return self.graph_executor(args, phase)\nRuntimeError: Run task for graph:kernel_graph_1 error! The details reger to 'Ascend Error Message'\n</code></pre></p> <p>\u8bf7\u5c1d\u8bd5\u5c06CANN\u66f4\u65b0\u52307.1\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u7f51\u7edc","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u6587\u672c\u68c0\u6d4b\u7f51\u7edc\u7684\u6559\u7a0b\u3002</p> <ul> <li>\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u7f51\u7edc</li> <li>1. \u6570\u636e\u96c6\u51c6\u5907<ul> <li>1.1 \u51c6\u5907\u8bad\u7ec3\u6570\u636e</li> <li>1.2 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e</li> </ul> </li> <li>2. \u914d\u7f6e\u6587\u4ef6\u51c6\u5907<ul> <li>2.1 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u6570\u636e\u96c6</li> <li>2.2 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570</li> <li>2.3 \u914d\u7f6e\u6a21\u578b\u67b6\u6784</li> <li>2.4 \u914d\u7f6e\u8bad\u7ec3\u8d85\u53c2\u6570</li> </ul> </li> <li>3. \u6a21\u578b\u8bad\u7ec3, \u6d4b\u8bd5\u548c\u63a8\u7406<ul> <li>3.1 \u8bad\u7ec3</li> <li>3.2 \u8bc4\u4f30</li> <li>3.3 \u63a8\u7406</li> <li>3.3.1 \u73af\u5883\u51c6\u5907</li> <li>3.3.2 \u6a21\u578b\u8f6c\u6362</li> <li>3.3.3 \u63a8\u7406 (Python)</li> </ul> </li> </ul>"},{"location":"cn/tutorials/training_detection_custom_dataset/#1","title":"1. \u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u76ee\u524d\uff0cMindOCR\u68c0\u6d4b\u7f51\u7edc\u652f\u6301\u4e24\u79cd\u8f93\u5165\u683c\u5f0f\uff0c\u5206\u522b\u662f:</p> <ul> <li> <p><code>Common Dataset</code>\uff1a\u4e00\u79cd\u6587\u4ef6\u683c\u5f0f\uff0c\u5b58\u50a8\u56fe\u50cf\u3001\u6587\u672c\u8fb9\u754c\u6846\u548c\u6587\u672c\u6807\u6ce8\u3002\u76ee\u6807\u6587\u4ef6\u683c\u5f0f\u7684\u4e00\u4e2a\u793a\u4f8b\u662f\uff1a <pre><code>img_1.jpg\\t[{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n</code></pre> \u5b83\u7531 DetDataset \u8bfb\u53d6\u3002\u5982\u679c\u60a8\u7684\u6570\u636e\u96c6\u4e0d\u662f\u4e0e\u793a\u4f8b\u683c\u5f0f\u76f8\u540c\u7684\u683c\u5f0f\uff0c\u8bf7\u53c2\u9605 \u8bf4\u660e \uff0c\u4e86\u89e3\u5982\u4f55\u5c06\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6ce8\u91ca\u8f6c\u6362\u4e3a\u652f\u6301\u7684\u683c\u5f0f\u3002</p> </li> <li> <p><code>SynthTextDataset</code>\uff1a\u7531 SynthText800k \u63d0\u4f9b\u7684\u4e00\u79cd\u6587\u4ef6\u683c\u5f0f\u3002 \u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u7ec6\u8282\u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc\u3002\u5b83\u7684\u6807\u6ce8\u6587\u4ef6\u662f\u4e00\u4e2a<code>.mat</code>\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u62ec <code>imnames</code>\uff08\u56fe\u50cf\u540d\u79f0\uff09\u3001<code>wordBB</code>\uff08\u5355\u8bcd\u7ea7\u8fb9\u754c\u6846\uff09\u3001<code>charBB</code>\uff08\u5b57\u7b26\u7ea7\u8fb9\u754c\u6846\uff09\u548c <code>txt</code>\uff08\u6587\u672c\u5b57\u7b26\u4e32\uff09\u3002\u5b83\u7531 SynthTextDataset \u8bfb\u53d6\u3002\u7528\u6237\u53ef\u4ee5\u53c2\u8003 <code>SynthTextDataset</code>\u6765\u7f16\u5199\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\u3002</p> </li> </ul> <p>\u6211\u4eec\u5efa\u8bae\u7528\u6237\u5c06\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\u51c6\u5907\u6210 <code>Common Dataset</code>\u683c\u5f0f\uff0c\u7136\u540e\u4f7f\u7528 <code>DetDataset</code> \u6765\u52a0\u8f7d\u6570\u636e\u3002\u4ee5\u4e0b\u6559\u7a0b\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u8be6\u7ec6\u6b65\u9aa4\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#11","title":"1.1 \u51c6\u5907\u8bad\u7ec3\u6570\u636e","text":"<p>\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u56fe\u50cf\u653e\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u5e76\u5728\u66f4\u9ad8\u7ea7\u522b\u7684\u76ee\u5f55\u4e2d\u6307\u5b9a\u4e00\u4e2a txt \u6587\u4ef6 <code>train_det.txt</code> \uff0c\u6765\u6807\u8bb0\u6240\u6709\u8bad\u7ec3\u56fe\u50cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\u3002txt \u6587\u4ef6\u7684\u4e00\u4e2a\u793a\u4f8b\u5982\u4e0b\uff1a <pre><code># \u6587\u4ef6\u540d   # \u4e00\u4e2a\u5b57\u5178\u5217\u8868\nimg_1.jpg\\t[{\"transcription\": \"Genaxis Theatre\", \"points\": [[377, 117], [463, 117], [465, 130], [378, 130]]}, {\"transcription\": \"[06]\", \"points\": [[493, 115], [519, 115], [519, 131], [493, 131]]}, {...}]\nimg_2.jpg\\t[{\"transcription\": \"guardian\", \"points\": [[642, 250], [769, 230], [775, 255], [648, 275]]}]\n...\n</code></pre></p> <p>\u6ce8\u610f\uff1a\u8bf7\u4f7f\u7528 <code>\\tab</code> \u5206\u9694\u56fe\u50cf\u540d\u79f0\u548c\u6807\u7b7e\uff0c\u907f\u514d\u4f7f\u7528\u7a7a\u683c\u6216\u5176\u4ed6\u5206\u9694\u7b26\u3002</p> <p>\u6700\u7ec8\u7684\u8bad\u7ec3\u96c6\u5c06\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u5b58\u50a8\uff1a</p> <pre><code>|-data\n    |- train_det.txt\n    |- training\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_detection_custom_dataset/#12","title":"1.2 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e","text":"<p>\u7c7b\u4f3c\u5730\uff0c\u8bf7\u5c06\u6240\u6709\u6d4b\u8bd5\u56fe\u50cf\u653e\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u5e76\u5728\u66f4\u9ad8\u7ea7\u522b\u7684\u76ee\u5f55\u4e2d\u6307\u5b9a\u4e00\u4e2a txt \u6587\u4ef6 <code>val_det.txt</code> \uff0c\u6765\u6807\u8bb0\u6240\u6709\u6d4b\u8bd5\u56fe\u50cf\u540d\u79f0\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u6700\u7ec8\uff0c\u6d4b\u8bd5\u96c6\u7684\u6587\u4ef6\u5939\u5c06\u4f1a\u4ee5\u4ee5\u4e0b\u683c\u5f0f\u5b58\u50a8\uff1a <pre><code>|-data\n    |- val_det.txt\n    |- validation\n        |- img_1.jpg\n        |- img_2.jpg\n        |- img_3.jpg\n        | ...\n</code></pre></p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#2","title":"2. \u914d\u7f6e\u6587\u4ef6\u51c6\u5907","text":"<p>\u4e3a\u4e86\u51c6\u5907\u76f8\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u7528\u6237\u5e94\u8be5\u6307\u5b9a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u76ee\u5f55\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#21","title":"2.1 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u6570\u636e\u96c6","text":"<p>\u8bf7\u9009\u62e9 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4f5c\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u4fee\u6539\u5176\u4e2d\u7684<code>train.dataset</code> \u548c <code>eval.dataset</code> \u5b57\u6bb5\u3002</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: DetDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u6cd5\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 `Common Dataset` \u683c\u5f0f\n    dataset_root: dir/to/data/                                        # \u6570\u636e\u7684\u6839\u76ee\u5f55\n    data_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n    label_file: train_det.txt                                       # \u8bad\u7ec3\u6807\u7b7e\u7684\u8def\u5f84\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n...\neval:\n  dataset:\n    type: DetDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u6cd5\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 `Common Dataset` \u683c\u5f0f\n    dataset_root: dir/to/data/                                        # \u6570\u636e\u7684\u6839\u76ee\u5f55\n    data_dir: validation/                                             # \u6d4b\u8bd5\u6570\u636e\u96c6\u76ee\u5f55\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n    label_file: val_det.txt                                     # \u6d4b\u8bd5\u6807\u7b7e\u7684\u8def\u5f84\u3002\u5b83\u5c06\u4e0e `dataset_root` \u62fc\u63a5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u8def\u5f84\u3002\n  ...\n</code></pre>"},{"location":"cn/tutorials/training_detection_custom_dataset/#22","title":"2.2 \u914d\u7f6e\u8bad\u7ec3/\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570","text":"<p>\u4ee5 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e2d\u7684 <code>train.dataset.transform_pipeline</code> \u4e3a\u4f8b\u3002\u5b83\u6307\u5b9a\u4e86\u4e00\u7ec4\u5e94\u7528\u4e8e\u56fe\u50cf\u6216\u6807\u7b7e\u7684\u8f6c\u6362\u51fd\u6570\uff0c\u7528\u4ee5\u751f\u6210\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\u6216\u635f\u5931\u51fd\u6570\u8f93\u5165\u7684\u6570\u636e\u3002\u8fd9\u4e9b\u8f6c\u6362\u51fd\u6570\u5b9a\u4e49\u5728 <code>mindocr/data/transforms</code> \u4e2d\u3002</p> <pre><code>...\ntrain:\n...\n  dataset:\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - RandomColorAdjust:\n          brightness: 0.1255  # 32.0 / 255\n          saturation: 0.5\n      - RandomHorizontalFlip:\n          p: 0.5\n      - RandomRotate:\n          degrees: [ -10, 10 ]\n          expand_canvas: False\n          p: 1.0\n      - RandomScale:\n          scale_range: [ 0.5, 3.0 ]\n          p: 1.0\n      - RandomCropWithBBox:\n          max_tries: 10\n          min_crop_ratio: 0.1\n          crop_size: [ 640, 640 ]\n          p: 1.0\n      - ValidatePolygons:\n      - ShrinkBinaryMap:\n          min_text_size: 8\n          shrink_ratio: 0.4\n      - BorderMap:\n          shrink_ratio: 0.4\n          thresh_min: 0.3\n          thresh_max: 0.7\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n  ...\n</code></pre> <ul> <li> <p><code>DecodeImage</code> \u548c <code>DetLabelEncode</code>\uff1a\u8fd9\u4e24\u4e2a\u8f6c\u6362\u51fd\u6570\u89e3\u6790 <code>train_det.txt</code> \u6587\u4ef6\u4e2d\u7684\u5b57\u7b26\u4e32\uff0c\u52a0\u8f7d\u56fe\u50cf\u548c\u6807\u7b7e\uff0c\u5e76\u5c06\u5b83\u4eec\u4fdd\u5b58\u4e3a\u4e00\u4e2a\u5b57\u5178\uff1b</p> </li> <li> <p><code>RandomColorAdjust</code>\uff0c <code>RandomHorizontalFlip</code>\uff0c <code>RandomRotate</code>\uff0c <code>RandomScale</code> \u548c <code>RandomCropWithBBox</code>\uff1a\u8fd9\u4e9b\u8f6c\u6362\u51fd\u6570\u6267\u884c\u5178\u578b\u7684\u56fe\u50cf\u589e\u5f3a\u64cd\u4f5c\u3002\u9664\u4e86 <code>RandomColorAdjust</code>\u4ee5\u5916\uff0c\u5176\u4ed6\u51fd\u6570\u90fd\u4f1a\u6539\u53d8\u8fb9\u754c\u6846\u6807\u7b7e\u3002</p> </li> <li> <p><code>ValidatePolygons</code>\uff1a\u5b83\u8fc7\u6ee4\u6389\u7531\u4e8e\u4e4b\u524d\u7684\u6570\u636e\u589e\u5f3a\u800c\u5728\u51fa\u73b0\u56fe\u50cf\u5916\u90e8\u7684\u8fb9\u754c\u6846\uff1b</p> </li> <li> <p><code>ShrinkBinaryMap</code>\u548c <code>BorderMap</code>\uff1a\u5b83\u4eec\u751f\u6210 <code>dbnet</code> \u8bad\u7ec3\u6240\u9700\u7684\u4e8c\u8fdb\u5236\u56fe\u548c\u8fb9\u754c\u56fe</p> </li> <li> <p><code>NormalizeImage</code>\uff1a\u5b83\u6839\u636e <code>ImageNet</code> \u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\u5bf9\u56fe\u50cf\u8fdb\u884c\u5f52\u4e00\u5316\uff1b</p> </li> <li> <p><code>ToCHWImage</code>\uff1a\u5b83\u5c06 <code>HWC</code> \u56fe\u50cf\u8f6c\u6362\u4e3a <code>CHW</code> \u56fe\u50cf\u3002</p> </li> </ul> <p>\u5bf9\u4e8e\u6d4b\u8bd5\u8f6c\u6362\u51fd\u6570\uff0c\u6240\u6709\u7684\u56fe\u50cf\u589e\u5f3a\u64cd\u4f5c\u90fd\u88ab\u79fb\u9664\uff0c\u88ab\u66ff\u6362\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u7f29\u653e\u51fd\u6570\u3002</p> <pre><code>eval:\n  dataset\n    transform_pipeline:\n      - DecodeImage:\n          img_mode: RGB\n          to_float32: False\n      - DetLabelEncode:\n      - DetResize:\n          target_size: [ 736, 1280 ]\n          keep_ratio: False\n          force_divisable: True\n      - NormalizeImage:\n          bgr_to_rgb: False\n          is_hwc: True\n          mean: imagenet\n          std: imagenet\n      - ToCHWImage:\n</code></pre> <p>\u66f4\u591a\u5173\u4e8e\u8f6c\u6362\u51fd\u6570\u7684\u6559\u7a0b\u53ef\u4ee5\u5728 \u8f6c\u6362\u6559\u7a0b \u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#23","title":"2.3 \u914d\u7f6e\u6a21\u578b\u67b6\u6784","text":"<p>\u867d\u7136\u4e0d\u540c\u7684\u6a21\u578b\u6709\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u4f46 <code>MindOCR</code> \u5c06\u5b83\u4eec\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u4e09\u9636\u6bb5\u67b6\u6784\uff1a<code>[backbone]-&gt;[neck]-&gt;[head]</code>\u3002\u4ee5 <code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e3a\u4f8b\uff1a</p> <pre><code>model:\n  type: det\n  transform: null\n  backbone:\n    name: det_resnet50  # \u76ee\u524d\u53ea\u652f\u6301 ResNet50\n    pretrained: True    # \u662f\u5426\u4f7f\u7528\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u91cd\n  neck:\n    name: DBFPN         # DBNet \u7684 FPN \u90e8\u5206\n    out_channels: 256\n    bias: False\n    use_asf: False      # DBNet++ \u4e2d\u7684\u81ea\u9002\u5e94\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\uff08\u4ec5\u7528\u4e8e DBNet++\uff09\n  head:\n    name: DBHead\n    k: 50               # \u53ef\u5fae\u5206\u4e8c\u503c\u5316\u7684\u653e\u5927\u56e0\u5b50\n    bias: False\n    adaptive: True      # \u8bad\u7ec3\u65f6\u4e3a True\uff0c\u63a8\u7406\u65f6\u4e3a False\n</code></pre> <p><code>backbone</code>,<code>neck</code>\u548c<code>head</code>\u5b9a\u4e49\u5728 <code>mindocr/models/backbones</code>\u3001<code>mindocr/models/necks</code> \u548c <code>mindocr/models/heads</code> \u4e0b\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#24","title":"2.4 \u914d\u7f6e\u8bad\u7ec3\u8d85\u53c2\u6570","text":"<p><code>configs/det/dbnet/dbnet_r50_icdar15.yaml</code> \u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e9b\u8bad\u7ec3\u8d85\u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\uff1a <pre><code>metric:\n  name: DetMetric\n  main_indicator: f-score\n\nloss:\n  name: DBLoss\n  eps: 1.0e-6\n  l1_scale: 10\n  bce_scale: 5\n  bce_replace: bceloss\n\nscheduler:\n  scheduler: polynomial_decay\n  lr: 0.007\n  num_epochs: 1200\n  decay_rate: 0.9\n  warmup_epochs: 3\n\noptimizer:\n  opt: SGD\n  filter_bias_and_bn: false\n  momentum: 0.9\n  weight_decay: 1.0e-4\n</code></pre> \u5b83\u4f7f\u7528 <code>SGD</code> \u4f18\u5316\u5668\uff08\u5728 <code>mindocr/optim/optim.factory.py</code> \u4e2d\uff09\u548c <code>polynomial_decay</code>\uff08\u5728 <code>mindocr/scheduler/scheduler_factory.py</code> \u4e2d\uff09\u4f5c\u4e3a\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u3002\u635f\u5931\u51fd\u6570\u662f <code>DBLoss</code>\uff08\u5728 <code>mindocr/losses/det_loss.py</code> \u4e2d\uff09\uff0c\u8bc4\u4f30\u6307\u6807\u662f <code>DetMetric</code>\uff08\u5728 <code>mindocr/metrics/det_metrics.py</code> \u4e2d\uff09\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#3","title":"3. \u6a21\u578b\u8bad\u7ec3, \u6d4b\u8bd5\u548c\u63a8\u7406","text":"<p>\u5f53\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u90fd\u5df2\u8bbe\u7f6e\u597d\u540e\uff0c\u7528\u6237\u5c31\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u4ed6\u4eec\u7684\u6a21\u578b\u3002MindOCR\u652f\u6301\u5728\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\u8fdb\u884c\u6d4b\u8bd5\u548c\u63a8\u7406\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#31","title":"3.1 \u8bad\u7ec3","text":"<ul> <li>\u5355\u673a\u8bad\u7ec3</li> </ul> <p>\u5728\u5355\u673a\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u662f\u5728\u5355\u4e2a\u8bbe\u5907\u4e0a\u8bad\u7ec3\u7684\uff08\u9ed8\u8ba4\u4e3a<code>device:0</code>\uff09\u3002\u7528\u6237\u5e94\u8be5\u5728<code>yaml</code>\u914d\u7f6e\u6587\u4ef6\u4e2d\u5c06<code>system.distribute</code>\u8bbe\u7f6e\u4e3a<code>False</code>\u3002\u5982\u679c\u7528\u6237\u60f3\u8981\u5728\u9664device:0\u4ee5\u5916\u7684\u8bbe\u5907\u4e0a\u8fd0\u884c\u8fd9\u4e2a\u6a21\u578b\uff0c\u8fd8\u9700\u8981\u5c06<code>system.device_id</code>\u8bbe\u7f6e\u4e3a\u76ee\u6807\u8bbe\u5907id\u3002</p> <p>\u4ee5<code>configs/det/dbnet/db_r50_icdar15.yaml</code>\u4e3a\u4f8b\uff0c\u8bad\u7ec3\u547d\u4ee4\u662f\uff1a <pre><code>python tools/train.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre></p> <ul> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> </ul> <p>\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\uff0cyaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>system.distribute</code>\u5e94\u8be5\u4e3a<code>True</code>\u3002\u5728GPU\u548cAscend\u8bbe\u5907\u4e0a\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>mpirun</code>\u6765\u542f\u52a8\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u4f8b\u5982\uff0c\u4f7f\u7528<code>device:0</code>\u548c<code>device:1</code>\u8fdb\u884c\u8bad\u7ec3\uff1a</p> <p><pre><code># n\u662fGPU/NPU\u7684\u6570\u91cf\nmpirun --allow-run-as-root -n 2 python tools/train.py --config configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre> \u6709\u65f6\uff0c\u7528\u6237\u53ef\u80fd\u60f3\u8981\u6307\u5b9a\u8bbe\u5907id\u6765\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4f8b\u5982\uff0c<code>device:2</code>\u548c<code>device:3</code>\u3002</p> <p>\u5728GPU\u8bbe\u5907\u4e0a\uff0c\u5728\u8fd0\u884c\u4e0a\u9762\u7684<code>mpirun</code>\u547d\u4ee4\u4e4b\u524d\uff0c\u7528\u6237\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a <pre><code>export CUDA_VISIBLE_DEVICES=2,3\n</code></pre> \u5728Ascend\u8bbe\u5907\u4e0a\uff0c\u7528\u6237\u5e94\u8be5\u521b\u5efa\u4e00\u4e2a\u50cf\u8fd9\u6837\u7684<code>rank_table.json</code>\uff1a <pre><code>Copy{\n    \"version\": \"1.0\",\n    \"server_count\": \"1\",\n    \"server_list\": [\n        {\n            \"server_id\": \"10.155.111.140\",\n            \"device\": [\n                {\"device_id\": \"2\",\"device_ip\": \"192.3.27.6\",\"rank_id\": \"2\"},\n                {\"device_id\": \"3\",\"device_ip\": \"192.4.27.6\",\"rank_id\": \"3\"}],\n             \"host_nic_ip\": \"reserve\"\n        }\n    ],\n    \"status\": \"completed\"\n}\n</code></pre></p> <p>\u76ee\u6807\u8bbe\u5907\u7684<code>device_ip</code>\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c<code>cat /etc/hccn.conf</code>\u83b7\u53d6\u3002\u8f93\u51fa\u7ed3\u679c\u4e2d\u7684<code>address_x</code>\u5c31\u662f<code>ip</code>\u5730\u5740\u3002\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u6559\u7a0b\u4e2d\u627e\u5230\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#32","title":"3.2 \u8bc4\u4f30","text":"<p>\u4e3a\u4e86\u8bc4\u4f30\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>tools/eval.py</code>\u3002</p> <p>\u4ee5\u5355\u673a\u8bc4\u4f30\u4e3a\u4f8b\u3002\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c<code>system.distribute</code>\u5e94\u8be5\u4e3a<code>False</code>\uff1b<code>eval.ckpt_load_path</code>\u5e94\u8be5\u662f\u76ee\u6807checkpoint\u8def\u5f84\uff1b<code>eval.dataset_root</code>\uff0c<code>eval.data_dir</code>\u548c<code>eval.label_file</code>\u5e94\u8be5\u6307\u5b9a\u4e3a\u6b63\u786e\u7684\u6d4b\u8bd5\u96c6\u8def\u5f84\u3002\u7136\u540e\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5f00\u59cb\u6d4b\u8bd5\uff1a <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml\n</code></pre></p> <p>MindOCR\u8fd8\u652f\u6301\u5728\u547d\u4ee4\u884c\u4e2d\u6307\u5b9a\u53c2\u6570\uff0c\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u7684\u547d\u4ee4\uff1a <pre><code>python tools/eval.py -c=configs/det/dbnet/db_r50_icdar15.yaml \\\n            --opt eval.ckpt_load_path=\"/path/to/local_ckpt.ckpt\" \\\n                  eval.dataset_root=\"/path/to/val_set/root\" \\\n                  eval.data_dir=\"val_set/dir\"\\\n                  eval.label_file=\"val_set/label\"\n</code></pre></p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#33","title":"3.3 \u63a8\u7406","text":"<p>MindOCR\u63a8\u7406\u652f\u6301Ascend310/Ascend310P\u8bbe\u5907\uff0c\u652f\u6301MindSpore Lite\u548c ACL \u63a8\u7406\u540e\u7aef\u3002\u63a8\u7406\u6559\u7a0b\u7ed9\u51fa\u4e86\u5982\u4f55\u4f7f\u7528MindOCR\u8fdb\u884c\u63a8\u7406\u7684\u8be6\u7ec6\u6b65\u9aa4\uff0c\u4e3b\u8981\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u73af\u5883\u51c6\u5907\u3001\u6a21\u578b\u8f6c\u6362\u548c\u63a8\u7406\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#331","title":"3.3.1 \u73af\u5883\u51c6\u5907","text":"<p>\u8bf7\u53c2\u8003\u73af\u5883\u5b89\u88c5\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u6839\u636e\u6a21\u578b\u6ce8\u610f\u9009\u62e9ACL/Lite\u73af\u5883\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#332","title":"3.3.2 \u6a21\u578b\u8f6c\u6362","text":"<p>\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u7528\u6237\u9700\u8981\u4ece\u8bad\u7ec3\u5f97\u5230\u7684checkpoint\u6587\u4ef6\u5bfc\u51fa\u4e00\u4e2aMindIR\u6587\u4ef6\u3002MindSpore IR (MindIR)\u662f\u57fa\u4e8e\u56fe\u5f62\u8868\u793a\u7684\u51fd\u6570\u5f0fIR\u3002MindIR\u6587\u4ef6\u5b58\u50a8\u4e86\u63a8\u7406\u6240\u9700\u7684\u6a21\u578b\u7ed3\u6784\u548c\u6743\u91cd\u53c2\u6570\u3002</p> <p>\u6839\u636e\u8bad\u7ec3\u597d\u7684dbnet checkpoint\u6587\u4ef6\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u51faMindIR\uff1a <pre><code>python tools/export.py --model_name_or_config dbnet_resnet50 --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n# \u6216\u8005\npython tools/export.py --model_name_or_config configs/det/dbnet/db_r50_icdar15.yaml --data_shape 736 1280 --local_ckpt_path /path/to/local_ckpt.ckpt\n</code></pre></p> <p><code>data_shape</code>\u662fMindIR\u6587\u4ef6\u7684\u6a21\u578b\u8f93\u5165\u56fe\u7247\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u3002\u5f53\u7528\u6237\u4f7f\u7528\u5176\u4ed6\u7684\u6a21\u578b\u65f6\uff0c<code>data_shape</code>\u53ef\u80fd\u4f1a\u6539\u53d8\u3002</p> <p>\u8bf7\u53c2\u8003\u8f6c\u6362\u6559\u7a0b\u83b7\u53d6\u66f4\u591a\u5173\u4e8e\u6a21\u578b\u8f6c\u6362\u7684\u7ec6\u8282\u3002</p>"},{"location":"cn/tutorials/training_detection_custom_dataset/#333-python","title":"3.3.3 \u63a8\u7406 (Python)","text":"<p>\u7ecf\u8fc7\u6a21\u578b\u8f6c\u6362\u540e\uff0c \u7528\u6237\u80fd\u5f97\u5230<code>output.mindir</code>\u6587\u4ef6\u3002\u7528\u6237\u53ef\u4ee5\u8fdb\u5165\u5230<code>deploy/py_infer</code>\u76ee\u5f55\uff0c\u5e76\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u63a8\u7406\uff1a</p> <pre><code>python infer.py \\\n    --input_images_dir=/your_path_to/test_images \\\n    --device=Ascend \\\n    --device_id=0 \\\n    --det_model_path=your_path_to/output.mindir \\\n    --det_model_name_or_config=../../configs/det/dbnet/db_r50_icdar15.yaml \\\n    --backend=lite \\\n    --res_save_dir=results_dir\n</code></pre> <p>\u8bf7\u53c2\u8003\u63a8\u7406\u6559\u7a0b\u7684<code>4.1 \u547d\u4ee4\u793a\u4f8b</code>\u7ae0\u8282\u83b7\u53d6\u66f4\u591a\u4f8b\u5b50\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#mindocr","title":"MindOCR \u4e91\u4e0a\u8bad\u7ec3\u5feb\u901f\u5165\u95e8","text":"<p>\u672c\u6587\u4e3b\u8981\u4ecb\u7ecdMindOCR\u501f\u52a9OPENI\u542f\u667a\u5e73\u53f0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#_1","title":"\u8fc1\u79fb\u5916\u90e8\u9879\u76ee","text":"<p>\u70b9\u51fb\u542f\u667a\u5e73\u53f0\u4e3b\u9875\u9762\u53f3\u4e0a\u89d2\u7684\u52a0\u53f7\uff0c\u4ece\u4e0b\u62c9\u83dc\u5355\u4e2d\u9009\u62e9\u8fc1\u79fb\u5916\u90e8\u9879\u76ee\uff0c\u5c06MindOCR\u4ecegithub\u8fc1\u79fb\u81f3\u542f\u667a\u5e73\u53f0\u3002</p> <p>\u8f93\u5165MindOCR\u7684git url: https://github.com/mindspore-lab/mindocr.git \u5373\u53ef\u8fdb\u884c\u8fc1\u79fb\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#_2","title":"\u51c6\u5907\u6570\u636e\u96c6","text":"<p>\u53ef\u4ee5\u4e0a\u4f20\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u4e5f\u53ef\u4ee5\u5173\u8054\u5e73\u53f0\u5df2\u6709\u7684\u6570\u636e\u96c6\u3002</p> <p>\u4e0a\u4f20\u4e2a\u4eba\u6570\u636e\u96c6\u9700\u5c06\u53ef\u7528\u96c6\u7fa4\u9009\u62e9\u4e3aNPU.</p>"},{"location":"cn/tutorials/training_on_openi/#_3","title":"\u51c6\u5907\u9884\u8bad\u7ec3\u6a21\u578b(\u53ef\u9009)","text":"<p>\u5982\u9700\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u53ef\u5728\u6a21\u578b\u9009\u9879\u5361\u4e2d\u6dfb\u52a0\u3002</p> <p>\u5bfc\u5165\u672c\u5730\u6a21\u578b\u65f6\uff0c\u6a21\u578b\u6846\u67b6\u7eed\u4e3aMindSpore.</p>"},{"location":"cn/tutorials/training_on_openi/#_4","title":"\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1","text":"<p>\u5728\u4e91\u8111\u9009\u9879\u5361\u4e2d\u9009\u62e9\u8bad\u7ec3\u4efb\u52a1-&gt;\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1\u3002</p> <p>\u57fa\u672c\u4fe1\u606f\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u9009\u62e9\u4e3aAscend NPU.</p> <p>\u8bbe\u7f6e\u53c2\u6570\u5e76\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570\u3002</p> <ul> <li>\u5982\u9700\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u53ef\u5728\u9009\u62e9\u6a21\u578b\u4e2d\u9009\u62e9\u5df2\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u5e76\u5728\u8fd0\u884c\u53c2\u6570\u4e2d\u589e\u52a0ckpt_dir\u53c2\u6570\uff0c\u53c2\u6570\u503c\u4e3a/cache/*.ckpt\uff0c*\u4e3a\u5b9e\u9645\u7684\u6587\u4ef6\u540d</li> <li>AI\u5f15\u64ce\u4e2d\u9700\u9009\u62e9mindspore 1.9\u6216\u4ee5\u4e0a\u7684\u7248\u672c\uff0c\u542f\u52a8\u6587\u4ef6\u4e3a<code>tools/train.py</code></li> <li>\u8fd0\u884c\u53c2\u6570\u9700\u6dfb\u52a0<code>enable_modelarts</code>\uff0c\u503c\u4e3aTrue</li> <li>\u8fd0\u884c\u53c2\u6570\u4e2d\u7531<code>config</code>\u53c2\u6570\u6307\u5b9a\u5177\u4f53\u7684\u6a21\u578b\u7b97\u6cd5\uff0c\u53c2\u6570\u503c\u524d\u7f00\u4e3a/home/work/user-job-dir/\u8fd0\u884c\u7248\u672c\u53f7\uff0c\u65b0\u5efa\u8bad\u7ec3\u4efb\u52a1\u7684\u8fd0\u884c\u7248\u672c\u53f7\u901a\u5e38\u4e3aV0001</li> </ul>"},{"location":"cn/tutorials/training_on_openi/#_5","title":"\u4fee\u6539\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1","text":"<p>\u70b9\u51fb\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1\u7684\u4fee\u6539\u6309\u94ae\uff0c\u53ef\u4ee5\u57fa\u4e8e\u5df2\u6709\u8bad\u7ec3\u4efb\u52a1\u8fdb\u884c\u53c2\u6570\u4fee\u6539\u5e76\u8fd0\u884c\u65b0\u7684\u8bad\u7ec3\u4efb\u52a1\u3002</p> <p>\u6ce8\u610f\uff1a\u8fd0\u884c\u7248\u672c\u53f7=\u6240\u57fa\u4e8e\u7248\u672c\u53f7+1</p>"},{"location":"cn/tutorials/training_on_openi/#_6","title":"\u72b6\u6001\u67e5\u770b","text":"<p>\u70b9\u51fb\u76f8\u5e94\u7684\u4efb\u52a1\u540d\u79f0\uff0c\u5373\u53ef\u67e5\u770b\u914d\u7f6e\u4fe1\u606f\u3001\u65e5\u5fd7\u3001\u8d44\u6e90\u5360\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u7ed3\u679c\u4e0b\u8f7d\u3002</p>"},{"location":"cn/tutorials/training_on_openi/#reference","title":"Reference","text":"<p>[1] Modified from https://github.com/mindspore-lab/mindyolo/blob/master/tutorials/cloud/openi_CN.md</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8bad\u7ec3\u8bc6\u522b\u7f51\u7edc","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8fdb\u884c\u8bc6\u522b\u7f51\u7edc\u8bad\u7ec3\u7684\u6559\u5b66\uff0c\u5305\u62ec\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_2","title":"\u6570\u636e\u96c6\u51c6\u5907","text":"<p>\u76ee\u524dMindOCR\u8bc6\u522b\u7f51\u7edc\u652f\u6301\u4e24\u79cd\u8f93\u5165\u5f62\u5f0f\uff0c\u5206\u522b\u4e3a - <code>\u901a\u7528\u6570\u636e</code>\uff1a\u4f7f\u7528\u56fe\u50cf\u548c\u6587\u672c\u6587\u4ef6\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5RecDataset\u7c7b\u578b\u8bfb\u53d6\u3002 - <code>LMDB\u6570\u636e</code>: \u4f7f\u7528LMDB\u50a8\u5b58\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u4ee5LMDBDataset\u7c7b\u578b\u8bfb\u53d6\u3002</p> <p>\u4ee5\u4e0b\u6559\u5b66\u4ee5\u4f7f\u7528<code>\u901a\u7528\u6570\u636e</code>\u6587\u4ef6\u683c\u5f0f\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_3","title":"\u8bad\u7ec3\u96c6\u51c6\u5907","text":"<p>\u8bf7\u5c06\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u8bad\u7ec3\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002txt\u6587\u4ef6\u4f8b\u5b50\u5982\u4e0b</p> <p><pre><code># \u6587\u4ef6\u540d   # \u5bf9\u5e94\u6807\u7b7e\nword_421.png    \u83dc\u80b4\nword_1657.png   \u4f60\u597d\nword_1814.png   cathay\n</code></pre> \u6ce8\u610f\uff1a\u8bf7\u5c06\u56fe\u7247\u540d\u548c\u6807\u7b7e\u4ee5 \\tab \u4f5c\u4e3a\u5206\u9694\uff0c\u907f\u514d\u4f7f\u7528\u7a7a\u683c\u6216\u5176\u4ed6\u5206\u9694\u7b26\u3002</p> <p>\u6700\u7ec8\u8bad\u7ec3\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_training.txt\n    |- training\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_4","title":"\u9a8c\u8bc1\u96c6\u51c6\u5907","text":"<p>\u540c\u6837\uff0c\u8bf7\u5c06\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u7f6e\u5165\u540c\u4e00\u6587\u4ef6\u5939\uff0c\u5e76\u5728\u4e0a\u5c42\u8def\u5f84\u6307\u5b9a\u4e00\u4e2atxt\u6587\u4ef6\u7528\u6765\u6807\u6ce8\u6240\u6709\u9a8c\u8bc1\u56fe\u7247\u540d\u548c\u5bf9\u5e94\u6807\u7b7e\u3002\u6700\u7ec8\u9a8c\u8bc1\u96c6\u5b58\u653e\u4f1a\u662f\u4ee5\u4e0b\u5f62\u5f0f\uff1a</p> <pre><code>|-data\n    |- gt_validation.txt\n    |- validation\n        |- word_001.png\n        |- word_002.jpg\n        |- word_003.jpg\n        | ...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_5","title":"\u5b57\u5178\u51c6\u5907","text":"<p>\u4e3a\u8bad\u7ec3\u4e2d\u3001\u82f1\u6587\u7b49\u4e0d\u540c\u8bed\u79cd\u7684\u8bc6\u522b\u7f51\u7edc\uff0c\u7528\u6237\u9700\u914d\u7f6e\u5bf9\u5e94\u7684\u5b57\u5178\u3002\u53ea\u6709\u5b58\u5728\u4e8e\u5b57\u5178\u4e2d\u7684\u5b57\u7b26\u4f1a\u88ab\u6a21\u578b\u6b63\u786e\u9884\u6d4b\u3002MindOCR\u73b0\u63d0\u4f9b\u9ed8\u8ba4\u3001\u4e2d\u548c\u82f1\u4e09\u79cd\u5b57\u5178\uff0c\u5176\u4e2d - <code>\u9ed8\u8ba4\u5b57\u5178</code>: \u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\u3002\u5982\u7528\u6237\u4e0d\u914d\u7f6e\u5b57\u5178\uff0c\u8be5\u5b57\u5178\u4f1a\u88ab\u9ed8\u8ba4\u4f7f\u7528\u3002 - <code>\u82f1\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/en_dict.txt</code>\u3002 - <code>\u4e2d\u6587\u5b57\u5178</code>\uff1a\u5305\u62ec\u5e38\u7528\u4e2d\u6587\u5b57\u7b26\u3001\u5927\u5c0f\u5199\u82f1\u6587\u3001\u6570\u5b57\u548c\u6807\u70b9\u7b26\u53f7\uff0c\u5b58\u653e\u4e8e<code>mindocr/utils/dict/ch_dict.txt</code>\u3002</p> <p>\u76ee\u524dMindOCR\u6682\u672a\u63d0\u4f9b\u5176\u4ed6\u8bed\u79cd\u7684\u5b57\u5178\u914d\u7f6e\u3002\u8be5\u529f\u80fd\u5c06\u5728\u65b0\u7248\u672c\u4e2d\u63a8\u51fa\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_6","title":"\u914d\u7f6e\u6587\u4ef6\u51c6\u5907","text":"<p>\u9488\u5bf9\u4e0d\u540c\u7f51\u7edc\u7ed3\u6784\uff0c\u7528\u6237\u9700\u914d\u7f6e\u76f8\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u73b0\u5df2CRNN\uff08\u4ee5Resnet34\u4e3a\u9aa8\u5e72\u6a21\u578b\uff09\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_7","title":"\u914d\u7f6e\u82f1\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\n    dataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\n    data_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n    label_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\n  dataset:\n    type: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\n    dataset_root: dir/to/data/                                        # \u6570\u636e\u96c6\u6839\u76ee\u5f55\n    data_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n    label_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n  ...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u82f1\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/en_dict.txt\n...\n</code></pre> <p>\u7531\u4e8e\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\u7684\u5b57\u5178\u9ed8\u8ba4\u53ea\u5305\u542b\u5c0f\u5199\u82f1\u6587\u548c\u6570\u5b57\uff0c\u4e3a\u4f7f\u7528\u5b8c\u6574\u82f1\u6587\u5b57\u5178\uff0c\u7528\u6237\u9700\u8981\u4fee\u6539\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\u7684<code>common: num_classes</code>\u5c5e\u6027\uff1a</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 95                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 96                                        # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\n  use_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_8","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u82f1\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1\uff08\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_9","title":"\u914d\u7f6e\u4e2d\u6587\u6a21\u578b","text":"<p>\u8bf7\u9009\u62e9<code>configs/rec/crnn/crnn_resnet34_ch.yaml</code>\u505a\u4e3a\u521d\u59cb\u914d\u7f6e\u6587\u4ef6\uff0c\u540c\u6837\u4fee\u6539\u5f53\u4e2d\u7684<code>train.dataset</code>\u548c<code>eval.dataset</code>\u5185\u5bb9\u3002</p> <pre><code>...\ntrain:\n  ...\n  dataset:\n    type: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\n    dataset_root: dir/to/data/                                        # \u8bad\u7ec3\u6570\u636e\u96c6\u6839\u76ee\u5f55\n    data_dir: training/                                               # \u8bad\u7ec3\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n    label_file: gt_training.txt                                       # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n...\neval:\n  dataset:\n    type: RecDataset                                                  # \u6587\u4ef6\u8bfb\u53d6\u65b9\u5f0f\uff0c\u8fd9\u91cc\u7528\u901a\u7528\u6570\u636e\u65b9\u5f0f\u8bfb\u53d6\n    dataset_root: dir/to/data/                                        # \u9a8c\u8bc1\u6570\u636e\u96c6\u6839\u76ee\u5f55\n    data_dir: validation/                                             # \u9a8c\u8bc1\u6570\u636e\u96c6\u76ee\u5f55\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n    label_file: gt_validation.txt                                     # \u8bad\u7ec3\u6570\u636e\u96c6\u6807\u7b7e\u6446\u653e\u4f4d\u7f6e\uff0c\u5c06\u4e0e`dataset_root`\u62fc\u63a5\u5f62\u6210\u5b8c\u6574\u8def\u5f84\n  ...\n</code></pre> <p>\u5e76\u4fee\u6539\u5bf9\u5e94\u7684\u5b57\u5178\u4f4d\u7f6e\uff0c\u6307\u5411\u4e2d\u6587\u5b57\u5178\u8def\u5f84</p> <pre><code>...\ncommon:\n  character_dict_path: &amp;character_dict_path mindocr/utils/dict/ch_dict.txt\n...\n</code></pre> <p>\u5982\u7f51\u7edc\u9700\u8981\u8f93\u51fa\u7a7a\u683c\uff0c\u5219\u9700\u8981\u4fee\u6539<code>common.use_space_char</code>\u5c5e\u6027\u548c<code>common: num_classes</code>\u5c5e\u6027\u5982\u4e0b</p> <pre><code>...\ncommon:\n  num_classes: &amp;num_classes 6625                                      # \u6570\u5b57\u4e3a \u5b57\u5178\u5b57\u7b26\u6570\u91cf + \u7a7a\u683c + 1\n  use_space_char: &amp;use_space_char True                                # \u989d\u5916\u6dfb\u52a0\u7a7a\u683c\u8f93\u51fa\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_10","title":"\u914d\u7f6e\u81ea\u5b9a\u4e49\u4e2d\u6587\u5b57\u5178","text":"<p>\u7528\u6237\u53ef\u6839\u636e\u9700\u6c42\u6dfb\u52a0\u3001\u5220\u6539\u5305\u542b\u5728\u5b57\u5178\u5185\u7684\u5b57\u7b26\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\uff0c\u5b57\u7b26\u9700\u4ee5\u6362\u884c\u7b26<code>\\n</code>\u4f5c\u4e3a\u5206\u9694\uff0c\u5e76\u4e14\u907f\u514d\u76f8\u540c\u5b57\u7b26\u51fa\u73b0\u5728\u540c\u4e00\u5b57\u5178\u91cc\u3002\u53e6\u5916\u7528\u6237\u540c\u65f6\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>common: num_classes</code>\u5c5e\u6027\uff0c\u786e\u4fdd<code>common: num_classes</code>\u5c5e\u6027\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 1 (\u5728seq2seq\u6a21\u578b\u4e2d\u4e3a\u5b57\u5178\u5b57\u7b26\u6570\u91cf + 2)\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_11","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u5f53\u6240\u6709\u6570\u636e\u96c6\u548c\u914d\u7f6e\u6587\u4ef6\u51c6\u5907\u5b8c\u6210\uff0c\u7528\u6237\u53ef\u5f00\u59cb\u8bad\u7ec3\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u7684\u6a21\u578b\u3002\u7531\u4e8e\u5404\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u4e0d\u540c\uff0c\u7528\u6237\u53ef\u53c2\u8003\u5bf9\u5e94\u6a21\u578b\u4ecb\u7ecd\u6587\u6863\u4e2d\u7684**\u6a21\u578b\u8bad\u7ec3**\u548c**\u6a21\u578b\u8bc4\u4f30**\u7ae0\u8282\u3002 \u8fd9\u91cc\u4ec5\u4ee5CRNN\u4e3a\u4f8b\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_12","title":"\u51c6\u5907\u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u505a\u6a21\u578b\u505a\u4e3a\u8d77\u59cb\u8bad\u7ec3\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5f80\u5f80\u80fd\u63d0\u5347\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\u751a\u81f3\u7cbe\u5ea6\u3002\u4ee5\u4e2d\u6587\u6a21\u578b\u4e3a\u4f8b\uff0c\u6211\u4eec\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7f51\u5740\u4e3ahttps://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt, \u7528\u6237\u4ec5\u9700\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u6dfb\u52a0<code>model.pretrained</code>\u6dfb\u52a0\u5bf9\u5e94\u7f51\u5740\u5982\u4e0b</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: https://download.mindspore.cn/toolkits/mindocr/crnn/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>\u5982\u679c\u9047\u5230\u7f51\u7edc\u95ee\u9898\uff0c\u7528\u6237\u53ef\u5c1d\u8bd5\u9884\u5148\u628a\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u628a<code>model.pretained</code>\u6539\u4e3a\u672c\u5730\u5730\u5740\u5982\u4e0b</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  pretrained: /local_path_to_the_ckpt/crnn_resnet34_ch-7a342e3c.ckpt\n...\n</code></pre> <p>\u5982\u679c\u7528\u6237\u4e0d\u9700\u8981\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53ea\u9700\u628a<code>model.pretrained</code>\u5220\u9664\u5373\u53ef\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_13","title":"\u542f\u52a8\u8bad\u7ec3","text":""},{"location":"cn/tutorials/training_recognition_custom_dataset/#_14","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u5728\u5927\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u8bae\u7528\u6237\u4f7f\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u5bf9\u4e8e\u5728\u591a\u4e2a\u6607\u817e910\u8bbe\u5907\u6216\u7740GPU\u5361\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u5c06\u914d\u7f6e\u53c2\u6570<code>system.distribute</code>\u4fee\u6539\u4e3aTrue, \u4f8b\u5982\uff1a</p> <pre><code># \u57284\u4e2a GPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmpirun -n 4 python tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_15","title":"\u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u8981\u5728\u6ca1\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\uff0c\u8bf7\u5c06\u914d\u7f6e\u53c2\u6570<code>system.distribute</code>\u4fee\u6539\u4e3aFalse \u5e76\u8fd0\u884c\uff1a</p> <pre><code># CPU/GPU/Ascend \u8bbe\u5907\u4e0a\u7684\u5355\u5361\u8bad\u7ec3\npython tools/train.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>\u8bad\u7ec3\u7ed3\u679c\uff08\u5305\u62eccheckpoint\u3001\u6bcf\u4e2aepoch\u7684\u6027\u80fd\u548c\u66f2\u7ebf\u56fe\uff09\u5c06\u88ab\u4fdd\u5b58\u5728yaml\u914d\u7f6e\u6587\u4ef6\u7684<code>train.ckpt_save_dir</code>\u53c2\u6570\u914d\u7f6e\u7684\u76ee\u5f55\u4e0b\uff0c\u9ed8\u8ba4\u4e3a<code>./tmp_rec</code>\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_16","title":"\u65ad\u70b9\u7eed\u8bad","text":"<p>\u5982\u679c\u7528\u6237\u671f\u671b\u5728\u5f00\u59cb\u8bad\u7ec3\u65f6\u540c\u65f6\u52a0\u8f7d\u6a21\u578b\u7684\u4f18\u5316\u5668\uff0c\u5b66\u4e60\u7387\u7b49\u4fe1\u606f\uff0c\u5e76\u7ee7\u7eed\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u9762\u6dfb\u52a0<code>model.resume</code>\u4e3a\u5bf9\u5e94\u7684\u672c\u5730\u6a21\u578b\u5730\u5740\u5982\u4e0b\uff0c\u5e76\u542f\u52a8\u8bad\u7ec3</p> <pre><code>...\nmodel:\n  type: rec\n  transform: null\n  backbone:\n    name: rec_resnet34\n    pretrained: False\n  neck:\n    name: RNNEncoder\n    hidden_size: 64\n  head:\n    name: CTCHead\n    out_channels: *num_classes\n  resume: /local_path_to_the_ckpt/model.ckpt\n...\n</code></pre>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_17","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"<p>\u90e8\u5206\u6a21\u578b(\u5305\u62ecCRNN, RARE, SVTR)\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4ee5\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\u3002\u7528\u6237\u53ef\u5c1d\u8bd5\u628a\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>system.amp_level</code>\u8bbe\u4e3a<code>O2</code>\u542f\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u4f8b\u5b50\u5982\u4e0b</p> <pre><code>system:\n  mode: 0\n  distribute: True\n  amp_level: O2  # Mixed precision training\n  amp_level_infer: O2\n  seed: 42\n  log_interval: 100\n  val_while_train: True\n  drop_overflow_update: False\n  ckpt_max_keep: 5\n...\n</code></pre> <p>\u5c06<code>system.amp_level</code>\u6539\u4e3a<code>O0</code>\u5173\u95ed\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_18","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u82e5\u8981\u8bc4\u4f30\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u53ef\u4ee5\u4f7f\u7528<code>tools/eval.py</code>\u3002\u8bf7\u5728\u914d\u7f6e\u6587\u4ef6\u7684<code>eval</code>\u90e8\u5206\u5c06\u53c2\u6570<code>ckpt_load_path</code>\u8bbe\u7f6e\u4e3a\u6a21\u578bcheckpoint\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u8bbe\u7f6e<code>distribute</code>\u4e3a<code>False</code>\u5982\u4e0b</p> <pre><code>system:\n  distribute: False # During evaluation stage, set to False\n...\neval:\n  ckpt_load_path: /local_path_to_the_ckpt/model.ckpt\n</code></pre> <p>\u7136\u540e\u8fd0\u884c\uff1a</p> <pre><code>python tools/eval.py --config configs/rec/crnn/crnn_resnet34_ch.yaml\n</code></pre> <p>\u4f1a\u5f97\u51fa\u7c7b\u4f3c\u6a21\u578b\u7ed3\u679c\u5982\u4e0b</p> <pre><code>2023-06-16 03:41:20,237:INFO:Performance: {'acc': 0.821939, 'norm_edit_distance': 0.917264}\n</code></pre> <p>\u5176\u4e2d<code>acc</code>\u5bf9\u5e94\u7684\u6570\u5b57\u4e3a\u6a21\u578b\u7684\u7cbe\u786e\u5ea6\u3002</p>"},{"location":"cn/tutorials/training_recognition_custom_dataset/#_19","title":"\u6a21\u578b\u63a8\u7406","text":"<p>\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u63a8\u7406\u811a\u672c\u5feb\u901f\u5f97\u5230\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u679c\u3002\u8bf7\u5148\u5c06\u56fe\u7247\u653e\u81f3\u5728\u540c\u4e00\u6587\u4ef6\u5939\u5185\uff0c\u7136\u540e\u6267\u884c</p> <pre><code>python tools/infer/text/predict_rec.py --image_dir {dir_to_your_image_data} --rec_algorithm CRNN_CH --draw_img_save_dir inference_results\n</code></pre> <p>\u7ed3\u679c\u4f1a\u5b58\u653e\u4e8e<code>draw_img_save_dir/rec_results.txt</code>, \u4ee5\u4e0b\u662f\u90e8\u5206\u4f8b\u5b50</p> <p> </p> <p>  cert_id.png  </p> <p> </p> <p>  doc_cn3.png  </p> <p>\u5f97\u51fa\u63a8\u7406\u7ed3\u679c\u5982\u4e0b</p> <pre><code>cert_id.png \u516c\u6c11\u8eab\u4efd\u53f7\u780144052419\ndoc_cn3.png \u9a6c\u62c9\u677e\u9009\u624b\u4e0d\u4f1a\u4e3a\u77ed\u6682\u7684\u9886\u5148\u611f\u5230\u6ee1\u610f\uff0c\u800c\u662f\u6c38\u8fdc\u5728\u5954\u8dd1\u3002\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/","title":"Transformation\u6559\u7a0b","text":""},{"location":"cn/tutorials/transform_tutorial/#_1","title":"\u673a\u5236","text":"<ol> <li>\u6bcf\u4e2aTransformation\u90fd\u662f\u4e00\u4e2a\u5177\u6709\u53ef\u8c03\u7528\u51fd\u6570\u7684\u7c7b\u3002\u793a\u4f8b\u5982\u4e0b</li> </ol> <pre><code>class ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    required keys: image\n    modified keys: image\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data: dict):\n        img = data['image']\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n</code></pre> <ol> <li> <p>transformation\u7684\u8f93\u5165\u59cb\u7ec8\u662fdict\uff0c\u5176\u4e2d\u5305\u542bimg_path\u3001raw label\u7b49\u6570\u636e\u4fe1\u606f\u3002</p> </li> <li> <p>transformation api\u5e94\u8be5\u660e\u786e\u8f93\u5165\u4e2d\u6240\u9700\u7684key\u4ee5\u53ca\u8f93\u51fa\u6570\u636e\u4e2d\u4fee\u6539\u6216/\u548c\u6dfb\u52a0\u7684key\u3002</p> </li> </ol> <p>\u53ef\u7528\u7684transformation\u53ef\u4ee5\u5728<code>mindocr/data/transforms/*_transform.py</code>\u4e2d\u53d1\u73b0</p> <pre><code># import and check available transforms\n\nfrom mindocr.data.transforms import general_transforms, det_transforms, rec_transforms\n</code></pre> <pre><code>general_transforms.__all__\n</code></pre> <pre><code>['DecodeImage', 'NormalizeImage', 'ToCHWImage', 'PackLoaderInputs']\n</code></pre> <pre><code>det_transforms.__all__\n</code></pre> <pre><code>['DetLabelEncode',\n 'MakeBorderMap',\n 'MakeShrinkMap',\n 'EastRandomCropData',\n 'PSERandomCrop']\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#_2","title":"\u6587\u672c\u68c0\u6d4b","text":""},{"location":"cn/tutorials/transform_tutorial/#1","title":"1. \u52a0\u8f7d\u56fe\u50cf\u548c\u6ce8\u91ca","text":""},{"location":"cn/tutorials/transform_tutorial/#_3","title":"\u51c6\u5907","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n%reload_ext autoreload\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>import os\n\n# load the label file which has the info of image path and annotation.\n# This file is generated from the ic15 annotations using the converter script.\nlabel_fp = '/Users/Samit/Data/datasets/ic15/det/train/train_icdar2015_label.txt'\nroot_dir = '/Users/Samit/Data/datasets/ic15/det/train'\n\ndata_lines = []\nwith open(label_fp, 'r') as f:\n    for line in f:\n        data_lines.append(line)\n\n# just pick one image and its annotation\nidx = 3\nimg_path, annot = data_lines[idx].strip().split('\\t')\n\nimg_path = os.path.join(root_dir, img_path)\nprint('img_path', img_path)\nprint('raw annotation: ', annot)\n</code></pre> <pre><code>img_path /Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_612.jpg\nraw annotation:  [{\"transcription\": \"where\", \"points\": [[483, 197], [529, 174], [530, 197], [485, 221]]}, {\"transcription\": \"people\", \"points\": [[531, 168], [607, 136], [608, 166], [532, 198]]}, {\"transcription\": \"meet\", \"points\": [[613, 128], [691, 100], [691, 131], [613, 160]]}, {\"transcription\": \"###\", \"points\": [[695, 299], [888, 315], [931, 635], [737, 618]]}, {\"transcription\": \"###\", \"points\": [[709, 19], [876, 8], [880, 286], [713, 296]]}, {\"transcription\": \"###\", \"points\": [[530, 270], [660, 246], [661, 300], [532, 324]]}, {\"transcription\": \"###\", \"points\": [[113, 356], [181, 359], [180, 387], [112, 385]]}, {\"transcription\": \"###\", \"points\": [[281, 328], [369, 338], [366, 361], [279, 351]]}, {\"transcription\": \"###\", \"points\": [[66, 314], [183, 313], [183, 328], [68, 330]]}]\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#-decodeimage","title":"\u89e3\u7801\u56fe\u50cf  -  DecodeImage","text":"<pre><code>#img_path = '/Users/Samit/Data/datasets/ic15/det/train/ch4_training_images/img_1.jpg'\ndecode_image = general_transforms.DecodeImage(img_mode='RGB')\n\n# TODO: check the input keys and output keys for the trans. func.\n\ndata = {'img_path': img_path}\ndata  = decode_image(data)\nimg = data['image']\n\n# visualize\nfrom mindocr.utils.visualize import show_img, show_imgs\nshow_img(img)\n</code></pre> <pre><code>import time\n\nstart = time.time()\natt = 100\nfor i in range(att):\n    img  = decode_image(data)['image']\navg = (time.time() - start) / att\n\nprint('avg reading time: ', avg)\n</code></pre> <pre><code>avg reading time:  0.004545390605926514\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#-detlabelencode","title":"\u68c0\u6d4b\u6807\u7b7e\u7f16\u7801 - DetLabelEncode","text":"<pre><code>data['label'] = annot\n\ndecode_image = det_transforms.DetLabelEncode()\ndata = decode_image(data)\n\n#print(data['polys'])\nprint(data['texts'])\n\n# visualize\nfrom mindocr.utils.visualize import draw_boxes\n\nres = draw_boxes(data['image'], data['polys'])\nshow_img(res)\n</code></pre> <pre><code>['where', 'people', 'meet', '###', '###', '###', '###', '###', '###']\n</code></pre>"},{"location":"cn/tutorials/transform_tutorial/#2","title":"2. \u56fe\u50cf\u548c\u6ce8\u91ca\u5904\u7406/\u589e\u5f3a","text":""},{"location":"cn/tutorials/yaml_configuration/","title":"\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>\u73af\u5883\u53c2\u6570-system</li> <li>\u5171\u7528\u53c2\u6570-common</li> <li>\u6a21\u578b\u5b9a\u4e49\u53c2\u6570-model</li> <li>\u540e\u5904\u7406-postprocess</li> <li>\u8bc4\u4f30\u6307\u6807-metric</li> <li>\u635f\u5931\u51fd\u6570-loss</li> <li>\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u548c\u4f18\u5316\u5668(scheduler, optimizer, loss_scaler)</li> <li>\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565-scheduler</li> <li>\u4f18\u5316\u5668-optimizer</li> <li>\u635f\u5931\u7f29\u653e-loss_scaler</li> <li>\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b(train, eval)</li> <li>\u8bad\u7ec3\u6d41\u7a0b-train</li> <li>\u8bc4\u4f30\u6d41\u7a0b-eval</li> </ul> <p>\u672c\u6587\u6863\u4ee5 <code>configs/rec/crnn/crnn_icdar15.yaml</code> \u4e3a\u4f8b\uff0c\u8be6\u7ec6\u8bf4\u660e\u53c2\u6570\u7684\u7528\u9014\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#1-system","title":"1. \u73af\u5883\u53c2\u6570 (system)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u53ef\u9009\u503c \u5907\u6ce8 mode MindSpore\u8fd0\u884c\u6a21\u5f0f(\u9759\u6001\u56fe/\u52a8\u6001\u56fe) 0 0 / 1 0: \u8868\u793a\u5728GRAPH_MODE\u6a21\u5f0f\u4e2d\u8fd0\u884c; 1: PYNATIVE_MODE\u6a21\u5f0f distribute \u662f\u5426\u5f00\u542f\u5e76\u884c\u8bad\u7ec3 True True / False \\ device_id \u6307\u5b9a\u5355\u5361\u8bad\u7ec3\u65f6\u7684\u5361id 7 \u673a\u5668\u53ef\u7528\u7684\u5361\u7684id \u8be5\u53c2\u6570\u4ec5\u5728distribute=False\uff08\u5355\u5361\u8bad\u7ec3\uff09\u548c\u73af\u5883\u53d8\u91cfDEVICE_ID\u672a\u8bbe\u7f6e\u65f6\u751f\u6548\u3002\u5355\u5361\u8bad\u7ec3\u65f6\uff0c\u5982\u8be5\u53c2\u6570\u548c\u73af\u5883\u53d8\u91cfDEVICE_ID\u5747\u672a\u8bbe\u7f6e\uff0c\u5219\u9ed8\u8ba4\u4f7f\u75280\u5361\u3002 amp_level \u6df7\u5408\u7cbe\u5ea6\u6a21\u5f0f O0 O0/O1/O2/O3 'O0' - \u4e0d\u53d8\u5316\u3002 'O1' - \u5c06\u767d\u540d\u5355\u5185\u7684Cell\u548c\u8fd0\u7b97\u8f6c\u4e3afloat16\u7cbe\u5ea6\uff0c\u5176\u4f59\u90e8\u5206\u4fdd\u6301float32\u7cbe\u5ea6\u3002 'O2' - \u5c06\u9ed1\u540d\u5355\u5185\u7684Cell\u548c\u8fd0\u7b97\u4fdd\u6301float32\u7cbe\u5ea6\uff0c\u5176\u4f59\u90e8\u5206\u8f6c\u4e3afloat16\u7cbe\u5ea6\u3002 'O3' - \u5c06\u7f51\u7edc\u5168\u90e8\u8f6c\u4e3afloat16\u7cbe\u5ea6\u3002 \u6ce8\u610f\uff1aGPU\u5e73\u53f0\u4e0a\u7684\u6a21\u578b\u63a8\u7406\u6216\u8bc4\u4f30\u6682\u4e0d\u652f\u6301'O3'\u6a21\u5f0f\uff0c\u5982\u8bbe\u7f6e\u4e3a'O3'\u6a21\u5f0f\uff0c\u7a0b\u5e8f\u4f1a\u81ea\u52a8\u5c06\u5176\u8f6c\u4e3a'O2'\u6a21\u5f0f\u3002 seed \u968f\u673a\u79cd\u5b50 42 Integer \\ ckpt_save_policy \u6a21\u578b\u6743\u91cd\u4fdd\u5b58\u7b56\u7565 top_k \"top_k\" \u6216 \"latest_k\" \"top_k\"\u8868\u793a\u4fdd\u5b58\u524dk\u4e2a\u8bc4\u4f30\u6307\u6807\u5206\u6570\u6700\u9ad8\u7684checkpoint\uff1b\"latest_k\"\u8868\u793a\u4fdd\u5b58\u6700\u65b0\u7684k\u4e2acheckpoint\u3002 <code>k</code>\u7684\u6570\u503c\u901a\u8fc7<code>ckpt_max_keep</code>\u53c2\u6570\u5b9a\u4e49 ckpt_max_keep \u6700\u591a\u4fdd\u5b58\u7684checkpoint\u6570\u91cf 5 Integer \\ log_interval log\u8f93\u51fa\u95f4\u9694(\u5355\u4f4d:step) 100 Integer \\ val_while_train \u662f\u5426\u5f00\u542f\u8fb9\u8bad\u7ec3\u8fb9\u8bc4\u4f30 True True/False \u5982\u679c\u503c\u4e3aTrue\uff0c\u8bf7\u540c\u6b65\u914d\u7f6eeval\u6570\u636e\u96c6 val_start_epoch \u4ece\u7b2c\u51e0\u4e2aepoch\u5f00\u59cb\u8dd1\u8bc4\u4f30 1 Interger val_interval \u8bc4\u4f30\u95f4\u9694(\u5355\u4f4d: epoch) 1 Interger drop_overflow_update \u5f53loss/\u68af\u5ea6\u6ea2\u51fa\u65f6\uff0c\u662f\u5426\u653e\u5f03\u66f4\u65b0\u7f51\u7edc\u53c2\u6570 True True/False \u5982\u679c\u503c\u4e3aTrue\uff0c\u5219\u5f53\u51fa\u73b0\u6ea2\u51fa\u65f6\uff0c\u4e0d\u4f1a\u66f4\u65b0\u7f51\u7edc\u53c2\u6570"},{"location":"cn/tutorials/yaml_configuration/#2-common","title":"2. \u5171\u7528\u53c2\u6570 (common)","text":"<p>\u56e0\u4e3a\u540c\u4e00\u4e2a\u53c2\u6570\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u914d\u7f6e\u90e8\u5206\u90fd\u9700\u8981\u91cd\u590d\u5229\u7528\uff0c\u6240\u4ee5\u60a8\u53ef\u4ee5\u5728\u8fd9\u4e2a\u90e8\u5206\u81ea\u5b9a\u4e49\u4e00\u4e9b\u901a\u7528\u7684\u53c2\u6570\uff0c\u4ee5\u4fbf\u7ba1\u7406\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#3-model","title":"3. \u6a21\u578b\u5b9a\u4e49\u53c2\u6570 (model)","text":"<p>\u5728MindOCR\u4e2d\uff0c\u6a21\u578b\u7684\u7f51\u7edc\u67b6\u6784\u5212\u5206\u4e3a Transform, Backbone, Neck\u548cHead\u56db\u4e2a\u6a21\u5757\u3002\u8be6\u7ec6\u8bf7\u53c2\u9605\u6587\u6863\uff0c\u4ee5\u4e0b\u662f\u5404\u90e8\u5206\u7684\u914d\u7f6e\u8bf4\u660e\u4e0e\u4f8b\u5b50\u3002</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 type \u7f51\u7edc\u7c7b\u578b - \u76ee\u524d\u652f\u6301 rec/det; rec\u8868\u793a\u8bc6\u522b\u4efb\u52a1\uff0cdet\u8868\u793a\u68c0\u6d4b\u4efb\u52a1 pretrained \u6307\u5b9a\u9884\u8bad\u7ec3\u6743\u91cd\u8def\u5f84\u6216url null \u652f\u6301\u672c\u5730checkpoint\u6587\u4ef6\u8def\u5f84\u6216url transform: tranform\u6a21\u5757\u914d\u7f6e name \u6307\u5b9atransform\u7f51\u7edc\u7684\u540d\u5b57 - \u76ee\u524d\u652f\u6301 STN_ON \u53d8\u6362 backbone: \u9aa8\u5e72\u7f51\u7edc\u914d\u7f6e name \u6307\u5b9a\u9aa8\u5e72\u7f51\u7edc\u7c7b\u540d\u6216\u89c4\u683c\u51fd\u6570\u540d - \u76ee\u524d\u5df2\u5b9a\u4e49\u7684\u7c7b\u6709 rec_resnet34, rec_vgg7, SVTRNet and det_resnet18, det_resnet50, det_resnet152, det_mobilenet_v3\u3002\u4ea6\u53ef\u81ea\u5b9a\u4e49\u65b0\u7684\u7c7b\u522b\uff0c\u8bf7\u53c2\u7167\u6587\u6863\u6307\u793a\u5b9a\u4e49\u3002 pretrained \u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u9aa8\u5e72\u6743\u91cd False \u652f\u6301\u4f20\u5165bool\u7c7b\u578b\u6216str\u7c7b\u578b\uff0c\u82e5\u4e3aTrue\uff0c\u5219\u901a\u8fc7backbone py\u4ef6\u4e2d\u5b9a\u4e49\u7684url\u94fe\u63a5\u4e0b\u8f7d\u5e76\u52a0\u8f7d\u9ed8\u8ba4\u6743\u91cd\u3002\u82e5\u4f20\u5165str\uff0c\u53ef\u6307\u5b9a\u672c\u5730checkpoint\u8def\u5f84\u6216url\u8def\u5f84\u8fdb\u884c\u52a0\u8f7d\u3002 neck: \u914d\u7f6e\u7f51\u7edcNeck name Neck\u7c7b\u540d - \u76ee\u524d\u5df2\u5b9a\u4e49\u7684\u7c7b\u6709 RNNEncoder, DBFPN, EASTFPN \u548c PSEFPN. \u4ea6\u53ef\u81ea\u5b9a\u4e49\u65b0\u7684\u7c7b\u522b\uff0c\u8bf7\u53c2\u7167\u6587\u6863\u6307\u793a\u5b9a\u4e49\u3002 hidden_size RNN\u9690\u85cf\u5c42\u5355\u5143\u6570 - \\ head: \u8bbe\u7f6e\u7f51\u7edc\u9884\u6d4b\u5934 name Head\u7c7b\u540d - \u76ee\u524d\u652f\u6301CTCHead, AttentionHead, DBHead, EASTHead \u4ee5\u53ca PSEHead weight_init \u8bbe\u7f6e\u6743\u91cd\u521d\u59cb\u5316 'normal' \\ bias_init \u8bbe\u7f6e\u6743\u504f\u5dee\u521d\u59cb\u5316 'zeros' \\ out_channels \u8bbe\u7f6e\u5206\u7c7b\u6570 - \\ <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u7f51\u7edc\uff0cbackbone/neck/head\u6a21\u5757\u53ef\u914d\u7f6e\u53c2\u6570\u4f1a\u6709\u6240\u4e0d\u540c\uff0c\u5177\u4f53\u53ef\u914d\u7f6e\u53c2\u6570\u7531\u4e0a\u8868\u6a21\u5757\u7684<code>name</code>\u53c2\u6570\u6307\u5b9a\u7684\u7c7b\u7684__init__\u5165\u53c2\u6240\u51b3\u5b9a \uff08\u5982\u82e5\u6307\u5b9a\u4e0bneck\u6a21\u5757\u7684name\u4e3aDBFPN\uff0c\u7531\u4e8eDBFPN\u7c7b\u521d\u59cb\u5316\u5305\u62ecadaptive\u5165\u53c2\uff0c\u5219\u53ef\u5728yaml\u4e2dmodel.head\u5c42\u7ea7\u4e0b\u914d\u7f6eadaptive\u7b49\u53c2\u6570\u3002</p> <p>\u53c2\u8003\u4f8b\u5b50: DBNet, CRNN</p>"},{"location":"cn/tutorials/yaml_configuration/#4-postprocess","title":"4. \u540e\u5904\u7406 (postprocess)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/postprocess</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u540e\u5904\u7406\u7c7b\u540d - \u76ee\u524d\u652f\u6301 DBPostprocess, EASTPostprocess, PSEPostprocess, RecCTCLabelDecode \u548c RecAttnLabelDecode character_dict_path \u8bc6\u522b\u5b57\u5178\u8def\u5f84 None \u82e5\u503c\u4e3aNone, \u5219\u4f7f\u7528\u9ed8\u8ba4\u5b57\u5178[0-9a-z] use_space_char \u8bbe\u7f6e\u662f\u5426\u6dfb\u52a0\u7a7a\u683c\u5230\u5b57\u5178 False True/False <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u540e\u5904\u7406\u65b9\u6cd5\uff08\u7531name\u6307\u5b9a\uff09\uff0c\u53ef\u914d\u7f6e\u7684\u53c2\u6570\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u7531\u540e\u5904\u7406\u7c7b\u7684\u521d\u59cb\u5316\u65b9\u6cd5__init__\u7684\u5165\u53c2\u6240\u51b3\u5b9a\u3002</p> <p>\u53c2\u8003\u4f8b\u5b50: DBNet, PSENet</p>"},{"location":"cn/tutorials/yaml_configuration/#5-metric","title":"5. \u8bc4\u4f30\u6307\u6807 (metric)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/metrics</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u8bc4\u4f30\u6307\u6807\u7c7b\u540d - \u76ee\u524d\u652f\u6301 RecMetric, DetMetric main_indicator \u4e3b\u8981\u6307\u6807\uff0c\u7528\u4e8e\u6700\u4f18\u6a21\u578b\u7684\u6bd4\u8f83 hmean \u8bc6\u522b\u4efb\u52a1\u4f7f\u7528acc\uff0c\u68c0\u6d4b\u4efb\u52a1\u5efa\u8bae\u4f7f\u7528f-score character_dict_path \u8bc6\u522b\u5b57\u5178\u8def\u5f84 None \u82e5\u503c\u4e3aNone, \u5219\u4f7f\u7528\u9ed8\u8ba4\u5b57\u5178 \"0123456789abcdefghijklmnopqrstuvwxyz\" ignore_space \u662f\u5426\u8fc7\u6ee4\u7a7a\u683c True True/False print_flag \u662f\u5426\u6253\u5370log False \u5982\u8bbe\u7f6eTrue\uff0c\u5219\u8f93\u51fa\u9884\u6d4b\u7ed3\u679c\u548c\u6807\u51c6\u7b54\u6848\u7b49\u4fe1\u606f"},{"location":"cn/tutorials/yaml_configuration/#6-loss","title":"6. \u635f\u5931\u51fd\u6570 (loss)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/losses</p> \u5b57\u6bb5 \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 name \u635f\u5931\u51fd\u6570\u7c7b\u540d - \u76ee\u524d\u652f\u6301 DBLoss, CTCLoss, AttentionLoss, PSEDiceLoss, EASTLoss and CrossEntropySmooth pred_seq_len \u9884\u6d4b\u6587\u672c\u7684\u957f\u5ea6 26 \u7531\u7f51\u7edc\u67b6\u6784\u51b3\u5b9a max_label_len \u6700\u957f\u6807\u7b7e\u957f\u5ea6 25 \u6570\u503c\u5e94\u5c0f\u4e8e\u7f51\u7edc\u9884\u6d4b\u6587\u672c\u7684\u957f\u5ea6 batch_size \u5355\u5361\u6279\u91cf\u5927\u5c0f 32 \\ <p>\u6ce8\u610f\uff1a\u5bf9\u4e8e\u4e0d\u540c\u635f\u5931\u51fd\u6570\uff08\u7531name\u6307\u5b9a\uff09\uff0c\u53ef\u914d\u7f6e\u7684\u53c2\u6570\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u7531\u6240\u9009\u7684\u635f\u5931\u51fd\u6570\u7684\u5165\u53c2\u6240\u51b3\u5b9a\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#7-scheduler-optimizer-loss_scaler","title":"7. \u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u548c\u4f18\u5316\u5668 (scheduler, optimizer, loss_scaler)","text":""},{"location":"cn/tutorials/yaml_configuration/#scheduler","title":"\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565 (scheduler)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/scheduler</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 scheduler \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u540d\u5b57 'constant' \u76ee\u524d\u652f\u6301 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay' min_lr \u5b66\u4e60\u7387\u6700\u5c0f\u503c 1e-6 'cosine_decay'\u8c03\u6574\u5b66\u4e60\u7387\u7684\u4e0b\u9650 lr \u5b66\u4e60\u7387 0.01 num_epochs \u603b\u8bad\u7ec3epoch\u6570 200 \u6574\u4e2a\u8bad\u7ec3\u7684\u603bepoch\u6570 warmup_epochs \u8bad\u7ec3\u5b66\u4e60\u7387warmp\u9636\u6bb5\u7684epoch\u6570 3 \u5bf9\u4e8e'cosine_decay'\uff0c<code>warmup_epochs</code>\u8868\u793a\u5c06\u5b66\u4e60\u7387\u4ece0\u63d0\u5347\u5230<code>lr</code>\u7684\u65f6\u671f\u3002 decay_epochs \u8bad\u7ec3\u5b66\u4e60\u7387\u8870\u51cf\u9636\u6bb5epoch\u6570 10 \u5bf9\u4e8e'cosine_decay'\uff0c\u8868\u793a\u5728<code>decay_epochs</code>\u5185\u5c06 <code>lr</code> \u8870\u51cf\u5230 <code>min_lr</code>\u3002\u5bf9\u4e8e'step_decay'\uff0c\u8868\u793a\u6bcf\u7ecf\u8fc7<code>decay_epochs</code>\u8f6e\uff0c\u6309<code>decay_rate</code>\u56e0\u5b50\u5c06 <code>lr</code> \u8870\u51cf\u4e00\u6b21\u3002"},{"location":"cn/tutorials/yaml_configuration/#optimizer","title":"\u4f18\u5316\u5668 (optimizer)","text":"<p>\u4ee3\u7801\u4f4d\u7f6e\u8bf7\u770b\uff1a mindocr/optim</p> \u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 opt \u4f18\u5316\u5668\u540d 'adam' \u76ee\u524d\u652f\u6301'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'nadam', 'adan', 'rmsprop', 'adagrad', 'lamb'. filter_bias_and_bn \u8bbe\u7f6e\u662f\u5426\u6392\u9664bias\u548cbatch norm\u7684\u6743\u91cd\u9012\u51cf True \u5982\u679c\u4e3a True\uff0c\u5219\u6743\u91cd\u8870\u51cf\u5c06\u4e0d\u9002\u7528\u4e8e BN \u53c2\u6570\u548c Conv \u6216 Dense \u5c42\u4e2d\u7684\u504f\u5dee\u3002 momentum \u52a8\u91cf 0.9 \\ weight_decay \u6743\u91cd\u9012\u51cf\u7387 0 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cweight decay\u53ef\u4ee5\u662f\u4e00\u4e2a\u5e38\u91cf\u503c\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2aCell\u3002\u4ec5\u5f53\u5e94\u7528\u52a8\u6001\u6743\u91cd\u8870\u51cf\u65f6\uff0c\u5b83\u624d\u662f Cell\u3002\u52a8\u6001\u6743\u91cd\u8870\u51cf\u7c7b\u4f3c\u4e8e\u52a8\u6001\u5b66\u4e60\u7387\uff0c\u7528\u6237\u53ea\u9700\u8981\u4ee5\u5168\u5c40\u6b65\u957f\u4e3a\u8f93\u5165\u81ea\u5b9a\u4e49\u4e00\u4e2a\u6743\u91cd\u8870\u51cf\u65f6\u95f4\u8868\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4f18\u5316\u5668\u8c03\u7528WeightDecaySchedule\u5b9e\u4f8b\u83b7\u53d6\u5f53\u524d\u6b65\u957f\u7684\u6743\u91cd\u8870\u51cf\u503c\u3002 nesterov \u662f\u5426\u4f7f\u7528 Nesterov \u52a0\u901f\u68af\u5ea6 (NAG) \u7b97\u6cd5\u6765\u66f4\u65b0\u68af\u5ea6\u3002 False True/False"},{"location":"cn/tutorials/yaml_configuration/#loss_scaler","title":"\u635f\u5931\u7f29\u653e\u7cfb\u6570 (loss_scaler)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 type loss\u7f29\u653e\u65b9\u6cd5\u7c7b\u578b static \u76ee\u524d\u652f\u6301 static, dynamic\u3002\u5e38\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 loss_scale loss\u7f29\u653e\u7cfb\u6570 1.0 \\ scale_factor \u5f53\u4f7f\u7528dynamic loss scaler\u65f6\uff0c\u52a8\u6001\u8c03\u6574loss_scale\u7684\u7cfb\u6570 2.0 \u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4e2d\uff0c\u5f53\u53d1\u751f\u6ea2\u51fa\u65f6\uff0c\u635f\u5931\u7f29\u653e\u503c\u4f1a\u66f4\u65b0\u4e3a <code>loss_scale</code>/<code>scale_factor</code>\u3002 scale_window \u5f53\u4f7f\u7528dynamic loss scaler\u65f6\uff0c\u7ecf\u8fc7scale_window\u8bad\u7ec3\u6b65\u672a\u51fa\u73b0\u6ea2\u51fa\u65f6\uff0c\u5c06loss_scale\u653e\u5927scale_factor\u500d 1000 \u5982\u679c\u8fde\u7eed\u7684<code>scale_window</code>\u6b65\u6570\u6ca1\u6709\u6ea2\u51fa\uff0c\u635f\u5931\u5c06\u589e\u52a0<code>loss_scale</code>*<code>scale_factor</code>\u7f29\u653e"},{"location":"cn/tutorials/yaml_configuration/#8-train-eval","title":"8. \u8bad\u7ec3\u3001\u8bc4\u4f30\u6d41\u7a0b (train, eval)","text":"<p>\u8bad\u7ec3\u6d41\u7a0b\u7684\u914d\u7f6e\u653e\u5728 <code>train</code> \u5e95\u4e0b\uff0c\u8bc4\u4f30\u9636\u6bb5\u7684\u914d\u7f6e\u653e\u5728 <code>eval</code> \u5e95\u4e0b\u3002\u6ce8\u610f\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u82e5\u6253\u5f00\u8fb9\u8bad\u7ec3\u8fb9\u8bc4\u4f30\u6a21\u5f0f\uff0c\u5373val_while_train=True\u65f6\uff0c\u5219\u5728\u6bcf\u4e2aepoch\u8bad\u7ec3\u5b8c\u6bd5\u540e\u6309\u7167 <code>eval</code> \u5e95\u4e0b\u7684\u914d\u7f6e\u8fd0\u884c\u4e00\u6b21\u8bc4\u4f30\u3002\u5728\u975e\u8bad\u7ec3\u9636\u6bb5\uff0c\u53ea\u8fd0\u884c\u6a21\u578b\u8bc4\u4f30\u7684\u65f6\u5019\uff0c\u53ea\u8bfb\u53d6 <code>eval</code> \u914d\u7f6e\u3002</p>"},{"location":"cn/tutorials/yaml_configuration/#train","title":"\u8bad\u7ec3\u6d41\u7a0b (train)","text":"\u5b57\u6bb5 \u8bf4\u660e \u9ed8\u8ba4\u503c \u5907\u6ce8 ckpt_save_dir \u8bbe\u7f6e\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./tmp_rec \\ resume \u8bad\u7ec3\u4e2d\u65ad\u540e\u6062\u590d\u8bad\u7ec3\uff0c\u53ef\u8bbe\u5b9aTrue/False\uff0c\u6216\u6307\u5b9a\u9700\u8981\u52a0\u8f7d\u6062\u590d\u8bad\u7ec3\u7684ckpt\u8def\u5f84 False \u53ef\u6307\u5b9aTrue/False\u914d\u7f6e\u662f\u5426\u6062\u590d\u8bad\u7ec3\uff0c\u82e5True\uff0c\u5219\u52a0\u8f7dckpt_save_dir\u76ee\u5f55\u4e0b\u7684resume_train.ckpt\u7ee7\u7eed\u8bad\u7ec3\u3002\u4e5f\u53ef\u4ee5\u6307\u5b9ackpt\u6587\u4ef6\u8def\u5f84\u8fdb\u884c\u52a0\u8f7d\u6062\u590d\u8bad\u7ec3\u3002 dataset_sink_mode MindSpore\u6570\u636e\u4e0b\u6c89\u6a21\u5f0f - \u5982\u679c\u8bbe\u7f6eTrue\uff0c\u5219\u6570\u636e\u4e0b\u6c89\u81f3\u5904\u7406\u5668\uff0c\u81f3\u5c11\u5728\u6bcf\u4e2aepoch\u7ed3\u675f\u540e\u624d\u80fd\u8fd4\u56de\u6570\u636e gradient_accumulation_steps \u7d2f\u79ef\u68af\u5ea6\u7684\u6b65\u6570 1 \u6bcf\u4e00\u6b65\u4ee3\u8868\u4e00\u6b21\u6b63\u5411\u8ba1\u7b97\uff0c\u68af\u5ea6\u7d2f\u8ba1\u5b8c\u6210\u518d\u8fdb\u884c\u4e00\u6b21\u53cd\u5411\u4fee\u6b63 clip_grad \u662f\u5426\u88c1\u526a\u68af\u5ea6 False \u5982\u679c\u8bbe\u7f6eTrue\uff0c\u5219\u5c06\u68af\u5ea6\u88c1\u526a\u6210 <code>clip_norm</code> clip_norm \u8bbe\u7f6e\u88c1\u526a\u68af\u5ea6\u7684\u8303\u6570 1 \\ ema \u662f\u5426\u542f\u52a8EMA\u7b97\u6cd5 False \\ ema_decay EMA\u8870\u51cf\u7387 0.9999 \\ pred_cast_fp32 \u662f\u5426\u5c06logits\u7684\u6570\u636e\u7c7b\u578b\u5f3a\u5236\u8f6c\u6362\u4e3afp32 False \\ dataset \u6570\u636e\u96c6\u914d\u7f6e \u8be6\u7ec6\u8bf7\u53c2\u9605Data\u6587\u6863 type \u6570\u636e\u96c6\u7c7b\u578b - \u76ee\u524d\u652f\u6301 LMDBDataset, RecDataset \u548c DetDataset dataset_root \u6570\u636e\u96c6\u6240\u5728\u6839\u76ee\u5f55 None Optional data_dir \u6570\u636e\u96c6\u6240\u5728\u5b50\u76ee\u5f55 - \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e<code>dataset_root</code>\uff0c\u8bf7\u5c06\u6b64\u8bbe\u7f6e\u6210\u5b8c\u6574\u76ee\u5f55 label_file \u6570\u636e\u96c6\u7684\u6807\u7b7e\u6587\u4ef6\u8def\u5f84 - \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e<code>dataset_root</code>\uff0c\u8bf7\u5c06\u6b64\u8bbe\u7f6e\u6210\u5b8c\u6574\u8def\u5f84\uff0c\u5426\u5219\u53ea\u9700\u8bbe\u7f6e\u5b50\u8def\u5f84 sample_ratio \u6570\u636e\u96c6\u62bd\u6837\u6bd4\u7387 1.0 \u82e5\u6570\u503c&lt;1.0\uff0c\u5219\u968f\u673a\u9009\u53d6 shuffle \u662f\u5426\u6253\u4e71\u6570\u636e\u987a\u5e8f \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False transform_pipeline \u6570\u636e\u5904\u7406\u6d41\u7a0b None \u8be6\u60c5\u8bf7\u770b transforms output_columns \u6570\u636e\u52a0\u8f7d\uff08data loader\uff09\u6700\u7ec8\u9700\u8981\u8f93\u51fa\u7684\u6570\u636e\u5c5e\u6027\u540d\u79f0\u5217\u8868\uff08\u7ed9\u5230\u7f51\u7edc/loss\u8ba1\u7b97/\u540e\u5904\u7406) (\u7c7b\u578b\uff1a\u5217\u8868\uff09\uff0c\u5019\u9009\u7684\u6570\u636e\u5c5e\u6027\u540d\u79f0\u7531transform_pipeline\u6240\u51b3\u5b9a\u3002 None \u5982\u679c\u503c\u4e3aNone\uff0c\u5219\u8f93\u51fa\u6240\u6709\u5217\u3002\u4ee5crnn\u4e3a\u4f8b\uff0coutput_columns: ['image', 'text_seq'] net_input_column_index output_columns\u4e2d\uff0c\u5c5e\u4e8e\u7f51\u7edcconstruct\u51fd\u6570\u7684\u8f93\u5165\u9879\u7684\u7d22\u5f15 [0] \\ label_column_index \u5728train\u9636\u6bb5\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86output_columns\u4e2d\u7684label\u9879\uff0c\u7528\u4e8e\u8ba1\u7b97loss\u3002\u5728eval\u9636\u6bb5\uff0c\u8be5\u53c2\u6570\u6307\u5b9a\u4e86output_columns\u4e2d\u7684ground truth\u9879\uff0c\u7528\u4e8emetric\u8ba1\u7b97\u3002 [1] \\ loader \u6570\u636e\u52a0\u8f7d\u8bbe\u7f6e shuffle \u6bcf\u4e2aepoch\u662f\u5426\u6253\u4e71\u6570\u636e\u987a\u5e8f \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False batch_size \u5355\u5361\u7684\u6279\u91cf\u5927\u5c0f - \\ drop_remainder \u5f53\u6570\u636e\u603b\u6570\u4e0d\u80fd\u9664\u4ee5batch_size\u65f6\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u6279\u6570\u636e \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse True/False max_rowsize \u6307\u5b9a\u5728\u591a\u8fdb\u7a0b\u4e4b\u95f4\u590d\u5236\u6570\u636e\u65f6\uff0c\u5171\u4eab\u5185\u5b58\u5206\u914d\u7684\u6700\u5927\u7a7a\u95f4 64 \\ num_workers \u6307\u5b9a batch \u64cd\u4f5c\u7684\u5e76\u53d1\u8fdb\u7a0b\u6570/\u7ebf\u7a0b\u6570 n_cpus / n_devices - 2 \u8be5\u503c\u5e94\u5927\u4e8e\u6216\u7b49\u4e8e2 <p>\u53c2\u8003\u4f8b\u5b50: DBNet, CRNN</p>"},{"location":"cn/tutorials/yaml_configuration/#eval","title":"\u8bc4\u4f30\u6d41\u7a0b (eval)","text":"<p><code>eval</code> \u7684\u53c2\u6570\u4e0e <code>train</code> \u57fa\u672c\u4e00\u6837\uff0c\u53ea\u8865\u5145\u8bf4\u660e\u51e0\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff0c\u5176\u4f59\u7684\u8bf7\u53c2\u8003\u4e0a\u8ff0<code>train</code>\u7684\u53c2\u6570\u8bf4\u660e\u3002</p> \u5b57\u6bb5 \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 ckpt_load_path \u8bbe\u7f6e\u6a21\u578b\u52a0\u8f7d\u8def\u5f84 - \\ num_columns_of_labels \u8bbe\u7f6e\u6570\u636e\u96c6\u8f93\u51fa\u5217\u4e2d\u7684\u6807\u7b7e\u6570 None \u9ed8\u8ba4\u5047\u8bbe\u56fe\u50cf (data[1:]) \u4e4b\u540e\u7684\u5217\u662f\u6807\u7b7e\u3002\u5982\u679c\u503c\u4e0d\u4e3aNone\uff0c\u5373image(data[1:1+num_columns_of_labels])\u4e4b\u540e\u7684num_columns_of_labels\u5217\u662f\u6807\u7b7e\uff0c\u5176\u4f59\u5217\u662f\u9644\u52a0\u4fe1\u606f\uff0c\u5982image_path\u3002 drop_remainder \u5f53\u6570\u636e\u603b\u6570\u4e0d\u80fd\u9664\u4ee5batch_size\u65f6\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u6279\u6570\u636e \u5728\u8bad\u7ec3\u9636\u6bb5\u4e3aTrue\uff0c\u5426\u5219\u4e3aFalse \u5728\u505a\u6a21\u578b\u8bc4\u4f30\u65f6\u5efa\u8bae\u8bbe\u7f6e\u6210False\uff0c\u82e5\u4e0d\u80fd\u6574\u9664\uff0cmindocr\u4f1a\u81ea\u52a8\u9009\u62e9\u4e00\u4e2a\u6700\u5927\u53ef\u6574\u9664\u7684batch size"}]}